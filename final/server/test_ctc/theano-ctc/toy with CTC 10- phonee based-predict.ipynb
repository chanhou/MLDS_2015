{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This version\n",
    "\n",
    "Trying to implement a version about CTC with 3 hidden layer and combine with RNN\n",
    "\n",
    "***Summary***\n",
    "1. Try Deep-LSTM by lasagne\n",
    "- Try dropout layer\n",
    "- [CTC Problem] prefix search decoding? -> blank probability threshold set at 99.99% at paper\n",
    "- Try phoneme base instead of char base\n",
    "\n",
    "***Todo***\n",
    "1. ~~data normalize~~\n",
    "2. CTC correct?\n",
    "- Try phoneme but not character based\n",
    "1. ~~Using oop module~~\n",
    "- ~~Using CTC with log domain and recurrent relationship~~\n",
    "- ~~Add hidden layer before RNN~~\n",
    "- ~~Compile and update and learning~~\n",
    "- ~~func check_label_error~~\n",
    "- ~~data processing to char based~~\n",
    "    - ~~preprocess of data~~\n",
    "    - ~~remove_blank(predict)~~\n",
    "    - ~~remap_back( predict )~~\n",
    "    - ~~change to tri MFCC only~~\n",
    "- ~~Try CTC with character based data (data preprocess) (our data)~~\n",
    "- ~~Check CTC cost correctness~~\n",
    "- ~~Check CTC with y need to unmap or not~~\n",
    "- ~~Add Nestrove momentum~~\n",
    "- ~~Add minibatch~~\n",
    "- ~~Add bidirectional LSTM/RNN~~\n",
    "\n",
    "***Further***\n",
    "1. ~~Add Bidirectional LSTM/RNN~~\n",
    "2. ~~Make mini batch~~\n",
    "- Try phoneme based model\n",
    "\n",
    "***Problem need to understand about RNN***\n",
    "1. [Phd thesis Training RNN , Hessian free method](http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf)\n",
    "- [Does Theano do automatic unfolding for BPTT?](http://stackoverflow.com/questions/24431621/does-theano-do-automatic-unfolding-for-bptt)\n",
    "- [Github, General purpose Hessian-free optimization in Theano](https://github.com/boulanni/theano-hf)\n",
    "- [Theano scan](http://deeplearning.net/software/theano/library/scan.html#theano.scan)\n",
    "- []()\n",
    "\n",
    "***Problem need to understand about CTC***\n",
    "1. [CTC paper](ftp://ftp.idsia.ch/pub/juergen/icml2006.pdf)\n",
    "\n",
    "***Important References***\n",
    "1. [CTC cost use in our version](https://github.com/mohammadpz/CTC-Connectionist-Temporal-Classification/blob/5b4d4be19805d7795a4293b9c270ef7bf5fafbfc/ctc_cost.py)\n",
    "    - [Discussion](https://github.com/mohammadpz/CTC-Connectionist-Temporal-Classification/issues/1)\n",
    "- How to connect multiple recurrent layers,https://github.com/craffel/nntools/issues/9\n",
    "- https://github.com/craffel/nntools/issues/11\n",
    "- refactor recurrent, update examples, add tests, https://github.com/craffel/nntools/pull/27\n",
    "- example of deep lstm and penn tree,https://github.com/skaae/nntools/blob/combine/examples/lstm.py\n",
    "\n",
    "***Github***\n",
    "1. [Hessian theano](https://github.com/boulanni/theano-hf/blob/master/hf_examples.py)\n",
    "- [Vanilla RNN](https://github.com/mohammadpz/Recurrent-Neural-Networks)\n",
    "- [blocks example, reverse words](https://github.com/mila-udem/blocks-examples/tree/master/reverse_words)\n",
    "- [rnn-ctc, basics ctc](https://github.com/rakeshvar/rnn_ctc/blob/master/ctc.py)\n",
    "- [Lasagne nntools](https://github.com/craffel/nntools/tree/recurrent)\n",
    "- [Theanets, RNN tools](https://github.com/lmjohns3/theanets/blob/master/theanets/layers/recurrent.py)\n",
    "- [LSTM benchmarks example](https://github.com/craffel/lstm_benchmarks/blob/master/lasagne/experiment.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../../../nntools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980\n",
      "/home/pika/nntools/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.\n",
      "  warnings.warn(\"The uniform initializer no longer uses Glorot et al.'s \"\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "from theano_toolkit import utils as U\n",
    "from theano_toolkit import updates\n",
    "from theano.printing import Print\n",
    "from theano_toolkit.parameters import Parameters\n",
    "\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "# from lasagne.layers import RecurrentLayer, InputLayer, DenseLayer,\\\n",
    "#     NonlinearityLayer, ReshapeLayer, EmbeddingLayer\n",
    "# from lasagne.layers.recurrent import *\n",
    "\n",
    "from time import time\n",
    "\n",
    "import ctc_cost_2\n",
    "\n",
    "import cPickle\n",
    "import sys\n",
    "sys.setrecursionlimit(100000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data part\n",
    "\n",
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### DataProcessing \n",
    "\n",
    "'''\n",
    "[ INPUT  ] : model { wav, NFCC, ... }\n",
    "[ OUTPUT ] : (Training_data, Validation_data, Testing_Data)\n",
    "\n",
    "- Training_data : x, y { .astype(np.float32)\n",
    "- Validation_data : x, y { .astype(np.float32)\n",
    "- Testing_Data : ( x, id )\n",
    "\n",
    "-- x format : \n",
    "---- MFCC : [ 39-vector ] - \n",
    "---- FBank : [ 69-vector ]\n",
    "---- quadmfcc : [ 39*(1+1+1) - vector ] !!! Wron---- FBank : [ 69-vector ]\n",
    "---- quadmfcc : [ 39*(1+1+1) - vector ] !!! Wrong implementation\n",
    "g implementation\n",
    "---- hexamfcc : [ 39*(4+1+4) - vector ]\n",
    "---- hexaFbank : [ 69*(4+1+4) - vector ]\n",
    "\n",
    "-- y format :\n",
    "\n",
    "---- Now we use 39-phonemes for all : [ 0 0 0 ... 1 .... 0 0 0 ]  as a number of 39\n",
    "---- But answer we get from ./label is [ 48-vector]\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "input:\n",
    "    MFCC: 39,2-1-2\n",
    "output:\n",
    "    state label: 1943\n",
    "'''\n",
    "\n",
    "'''\n",
    "[ Input  ] :\n",
    "    - mfcc (39*(2+1+2))\n",
    "\n",
    "[ Output ] :\n",
    "    - maxi (1943-D)\n",
    "    \n",
    "'''\n",
    "\n",
    "from itertools import repeat\n",
    "import numpy as np\n",
    "import random\n",
    "# from sklearn import preprocessing\n",
    "\n",
    "# Parameter (note: switch /final/MLDS_Final_Data/ .. if u want\n",
    "MAPPING_FILE = '/home/pika/MLDS_2015/final/MLDS_HW1_RELEASE_v1/phones/48_39.map'\n",
    "# MFCC_TRAINING_ARK = '/home/pika/MLDS_2015/final/MLDS_Final_Data/mfcc/train_nor.ark'\n",
    "# MFCC_TRAINING_ARK = '/home/pika/MLDS_2015/final/MLDS_Final_Data/wav_mod/train.ark'\n",
    "\n",
    "MFCC_TEST_ARK = '/home/pika/MLDS_2015/final/MLDS_Final_Data/mfcc/test_nor.ark'\n",
    "\n",
    "\n",
    "# MAXI_TRAINING_LABEL = '/home/pika/MLDS_2015/final/MLDS_HW1_RELEASE_v1/state_label/train.lab'\n",
    "# NORMAL_TRAINING_LABEL = '/home/pika/MLDS_2015/final/MLDS_HW1_RELEASE_v1/label/train.lab'\n",
    "\n",
    "sentence_data = '/home/pika/MLDS_2015/final/MLDS_Final_Data/sentence/train_mod.set'\n",
    "\n",
    "\n",
    "def clean_up( y ):\n",
    "    \"\"\"\n",
    "    for final output clean up\n",
    "    B(a − ab−) = B(−aa − −abb) = aab\n",
    "    \"\"\"\n",
    "    answer = []\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if (i==0):\n",
    "            answer.append(y[i])\n",
    "        if y[i-1] != y[i]:\n",
    "            answer.append(y[i])\n",
    "    \n",
    "    return answer\n",
    "\n",
    "\n",
    "\n",
    "# Dataset \n",
    "def DNNDataset( inputMode ) :\n",
    "    trains = {}\n",
    "    valids = {}\n",
    "    test_data = {}\n",
    "    x_train = []\n",
    "    x_valid = []\n",
    "    y_train = []\n",
    "    y_valid = []\n",
    "    total_result = []\n",
    "    \n",
    "    slide_ration = 0.1\n",
    "    \n",
    "    sentense = {}\n",
    "    char_map = {}\n",
    "    char_unmap = {}\n",
    "    char_count = 0\n",
    "    \n",
    "    with open(sentence_data) as f:\n",
    "        for lines in f:\n",
    "            frames = lines.split(',',1)\n",
    "            frames[1] = frames[1].lower()\n",
    "            if len(frames[1])<2: # empty sentences\n",
    "                prob = frames[0]\n",
    "                print prob\n",
    "                continue\n",
    "            for cha in frames[1][:-2]: # skip \"\\n\" ending punctuation\n",
    "                if cha in ',.!?;:-\"':\n",
    "                    continue\n",
    "                if cha not in char_map:\n",
    "                    char_map[cha] = char_count\n",
    "                    char_unmap[char_count] = cha\n",
    "                    char_count += 1\n",
    "                    \n",
    "    char_map[\"-\"] = char_count # for blank, different with space\n",
    "    char_unmap[char_count] = \"-\"\n",
    "#     print char_map\n",
    "    \n",
    "    with open(sentence_data) as f:\n",
    "        for lines in f:\n",
    "            frames = lines.split(',',1)\n",
    "            frames[1] = frames[1].lower()\n",
    "            if len(frames[1])<2: # empty sentences\n",
    "                prob = frames[0]\n",
    "                continue\n",
    "            sentense[ frames[0] ] = []\n",
    "            for cha in frames[1][:-2]:\n",
    "                if cha in ',.!?;:\"':\n",
    "                    continue\n",
    "                if cha in '-':\n",
    "                    cha = \" \"\n",
    "#                 sentense[ frames[0] ].append( char_map['-'] ) # append blank before each label\n",
    "                sentense[ frames[0] ].append( char_map[cha] )\n",
    "#             sentense[ frames[0] ].append( char_map['-'] ) # append blank in the end\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # X(INPUT) ---------------------------------------\n",
    "    if inputMode == \"mfcc\":\n",
    "        \n",
    "        # speaker_window -(map)-> mfcc[39]\n",
    "        monoTrains = {}\n",
    "        with open(MFCC_TEST_ARK) as f:\n",
    "            for lines in f:\n",
    "                frames = lines.split(' ')\n",
    "                frame2float = [ float(x) for x in frames[1:] ]\n",
    "                monoTrains[frames[0]] = frame2float\n",
    "#                 break\n",
    "        # Initialize : L2 L1 S R1 R2\n",
    "        zeroFrame = list(repeat(0, 39))\n",
    "        prevframeL1 = [ float(x) for x in zeroFrame[:] ]\n",
    "#         prevframeL2 = [ float(x) for x in zeroFrame[:] ]\n",
    "        prevframeR1 = [ float(x) for x in zeroFrame[:] ]\n",
    "#         prevframeR2 = [ float(x) for x in zeroFrame[:] ]\n",
    "        blankframe  = [ float(x) for x in zeroFrame[:] ]\n",
    "        frameNameL1 = \"\"\n",
    "#         frameNameL2 = \"\"\n",
    "        frameNameR1 = \"\"\n",
    "#         frameNameR2 = \"\"\n",
    "        \n",
    "        # Speaker { windows { ... } }\n",
    "        # still is faem0_si1392, bug !\n",
    "        \n",
    "#         speaker_data = []\n",
    "#         for sample in monoTrains:\n",
    "#             frames_info = sample.split('_')\n",
    "#             if frames_info[0] not in speaker_data:\n",
    "#                 speaker_data.append(frames_info[0])\n",
    "\n",
    "#         speaker_data = np.array(speaker_data)\n",
    "#         speaker_data = np.random.permutation(speaker_data)\n",
    "# #         print speaker_data\n",
    "#         valid = len(speaker_data)*0.9\n",
    "#         speaker_train = speaker_data[ 0 : valid ]\n",
    "#         speaker_valid = speaker_data[ valid : ]\n",
    "        \n",
    "        for sample in monoTrains:\n",
    "            frames_info = sample.split('_')\n",
    "            speaker = frames_info[0] + \"_\" + frames_info[1]\n",
    "#             frameNameL2 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])-2))\n",
    "            frameNameL1 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])-1))\n",
    "            frameNameR1 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])+1))\n",
    "#             frameNameR2 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])+2))\n",
    "            \n",
    "#             if frames_info[0] in speaker_train:\n",
    "                \n",
    "            if trains.get(speaker) is None:\n",
    "                trains[speaker] = {}\n",
    "            if   frames_info[2] == \"1\":\n",
    "                trains[speaker][sample] = blankframe + monoTrains[sample] + monoTrains[frameNameR1]\n",
    "            elif monoTrains.get(frameNameR1) is None:\n",
    "                trains[speaker][sample] = monoTrains[frameNameL1] + monoTrains[sample] + blankframe\n",
    "            else:\n",
    "                trains[speaker][sample] = monoTrains[frameNameL1] + monoTrains[sample] + monoTrains[frameNameR1]  \n",
    "#                 trains[speaker][sample] = monoTrains[sample]\n",
    "\n",
    "        monoTrains.clear()\n",
    "        \n",
    "        phones_mapping = {}\n",
    "        result_mapping = {}\n",
    "        y_trains_phone = {}\n",
    "        with open(MAPPING_FILE) as f:\n",
    "            i = 0\n",
    "            for lines in f :\n",
    "                phones = lines.split('\\t')\n",
    "                phones_mapping[phones[0]] = i\n",
    "                i += 1\n",
    "                \n",
    "        with open(MAPPING_FILE) as f:    \n",
    "            for lines in f :\n",
    "                phones = lines.split('\\t')\n",
    "                erf = phones[1].split('\\n')\n",
    "                result_mapping[ phones_mapping[phones[0]] ] = erf[0]\n",
    "        \n",
    "                        \n",
    "        sssspppp = []\n",
    "        for sample in trains:\n",
    "#             print sample\n",
    "#             print len(trains[sample])\n",
    "            \n",
    "#             if sample in sentense:\n",
    "            temp = []\n",
    "            sssspppp.append(sample)\n",
    "            for i in range(len(trains[sample])):\n",
    "                temp.append( floatX( trains[sample][sample + \"_\" + str(i+1)] ))\n",
    "\n",
    "\n",
    "            x_train.append( floatX(temp ))\n",
    "\n",
    "#                 y_train.append( floatX(sentense[sample]) )\n",
    "\n",
    "        trains.clear()\n",
    "        \n",
    "    return char_map, char_unmap, phones_mapping, result_mapping, x_train, sssspppp\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def vectorized_result ( j , siz) :\n",
    "    e = np.zeros((siz, 1))\n",
    "    e[j] = 1.0\n",
    "    return np.reshape( e, siz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcth0_sx219\n"
     ]
    }
   ],
   "source": [
    "char_map, char_unmap, phones_mapping, result_mapping, x_train, speaker_sequence = DNNDataset(\"mfcc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mgrt0_si1450',\n",
       " 'mpdf0_sx372',\n",
       " 'fgjd0_sx279',\n",
       " 'mrws1_si1496',\n",
       " 'mrjm4_sx229',\n",
       " 'fpas0_si1272',\n",
       " 'mcmj0_sx374',\n",
       " 'mtas1_si2098',\n",
       " 'fdrw0_si653',\n",
       " 'mbwm0_si1934',\n",
       " 'mjdh0_sx274',\n",
       " 'faks0_si1573',\n",
       " 'mjsw0_si2270',\n",
       " 'fgjd0_si1179',\n",
       " 'fdrw0_sx383',\n",
       " 'mlnt0_si1902',\n",
       " 'mwjg0_sx224',\n",
       " 'mlll0_sx193',\n",
       " 'mrjm4_sx319',\n",
       " 'mjfc0_si2293',\n",
       " 'mjar0_si2247',\n",
       " 'mrjm4_sx139',\n",
       " 'majc0_sx205',\n",
       " 'fnlp0_sx408',\n",
       " 'mers0_sx389',\n",
       " 'mjdh0_si1354',\n",
       " 'mcmj0_sx104',\n",
       " 'mrtk0_si1723',\n",
       " 'mjar0_si728',\n",
       " 'mgrt0_si820',\n",
       " 'mwew0_sx371',\n",
       " 'mtdt0_sx274',\n",
       " 'mbwm0_si1304',\n",
       " 'mtaa0_sx205',\n",
       " 'fjmg0_sx191',\n",
       " 'fkms0_sx320',\n",
       " 'mdlf0_sx413',\n",
       " 'mrcs0_si1223',\n",
       " 'mklt0_si583',\n",
       " 'mrtk0_si1093',\n",
       " 'mrjr0_sx192',\n",
       " 'mwjg0_sx314',\n",
       " 'mers0_sx299',\n",
       " 'mjsw0_sx110',\n",
       " 'fmgd0_sx124',\n",
       " 'mcmj0_sx194',\n",
       " 'mdab0_sx409',\n",
       " 'mwew0_si731',\n",
       " 'mdlf0_si2213',\n",
       " 'fmld0_sx385',\n",
       " 'mwbt0_sx383',\n",
       " 'mrjr0_si1812',\n",
       " 'mmwh0_sx279',\n",
       " 'mtdt0_si1994',\n",
       " 'fmml0_sx230',\n",
       " 'frew0_si1280',\n",
       " 'mteb0_sx53',\n",
       " 'fpas0_sx404',\n",
       " 'mrcs0_sx143',\n",
       " 'fjlm0_sx233',\n",
       " 'mteb0_si1133',\n",
       " 'fpas0_sx224',\n",
       " 'fdhc0_sx209',\n",
       " 'mjsw0_sx200',\n",
       " 'mmdb1_sx365',\n",
       " 'mglb0_sx274',\n",
       " 'mgwt0_si909',\n",
       " 'mdvc0_sx306',\n",
       " 'fjem0_sx274',\n",
       " 'mwjg0_si494',\n",
       " 'mrws1_si1130',\n",
       " 'mpdf0_sx192',\n",
       " 'mroa0_sx227',\n",
       " 'fsem0_sx118',\n",
       " 'frew0_si1030',\n",
       " 'mwjg0_si1124',\n",
       " 'fmah0_sx389',\n",
       " 'mers0_si1019',\n",
       " 'mgwt0_sx189',\n",
       " 'fmld0_si2185',\n",
       " 'mdvc0_sx396',\n",
       " 'fpas0_sx314',\n",
       " 'mbns0_sx410',\n",
       " 'mtaa0_sx385',\n",
       " 'mdab0_si2299',\n",
       " 'fmah0_sx209',\n",
       " 'mrws1_si500',\n",
       " 'mbwm0_sx224',\n",
       " 'mtdt0_sx94',\n",
       " 'fpas0_sx134',\n",
       " 'mmdm2_sx282',\n",
       " 'fpkt0_sx188',\n",
       " 'fgjd0_si549',\n",
       " 'mnjm0_sx230',\n",
       " 'mpam0_si1819',\n",
       " 'mdls0_sx188',\n",
       " 'fpkt0_si2168',\n",
       " 'mreb0_sx295',\n",
       " 'mjar0_sx368',\n",
       " 'mroa0_sx317',\n",
       " 'mtaa0_sx295',\n",
       " 'mteb0_sx413',\n",
       " 'fnlp0_si678',\n",
       " 'mteb0_sx233',\n",
       " 'mbpm0_sx137',\n",
       " 'mjdh0_sx364',\n",
       " 'mtas1_si838',\n",
       " 'mbwm0_sx314',\n",
       " 'fnmr0_si2029',\n",
       " 'fjlm0_sx323',\n",
       " 'mrtk0_si1750',\n",
       " 'mpam0_si1961',\n",
       " 'fdhc0_sx389',\n",
       " 'mjsw0_sx380',\n",
       " 'fadg0_si649',\n",
       " 'fcal1_sx233',\n",
       " 'mgjf0_sx191',\n",
       " 'felc0_si756',\n",
       " 'mdvc0_sx126',\n",
       " 'mpam0_sx379',\n",
       " 'mbdg0_sx383',\n",
       " 'mcsh0_si2179',\n",
       " 'mlll0_si1993',\n",
       " 'fjem0_si634',\n",
       " 'mglb0_si2164',\n",
       " 'fdac1_sx304',\n",
       " 'mjmp0_si1535',\n",
       " 'mreb0_si745',\n",
       " 'fsem0_sx388',\n",
       " 'mmwh0_si1301',\n",
       " 'fmgd0_si2194',\n",
       " 'fmld0_si925',\n",
       " 'mjdh0_sx94',\n",
       " 'fjmg0_sx371',\n",
       " 'mdls0_sx368',\n",
       " 'fmml0_si1040',\n",
       " 'fdms0_sx408',\n",
       " 'mmjr0_si1648',\n",
       " 'mnjm0_si2210',\n",
       " 'fdac1_sx124',\n",
       " 'fjem0_sx94',\n",
       " 'mpam0_sx199',\n",
       " 'mdlf0_sx233',\n",
       " 'mpdf0_si2172',\n",
       " 'fedw0_si1084',\n",
       " 'mjln0_sx99',\n",
       " 'mtaa0_si1915',\n",
       " 'mjfc0_si1663',\n",
       " 'mwjg0_sx134',\n",
       " 'fedw0_sx274',\n",
       " 'mteb0_si2064',\n",
       " 'faks0_si943',\n",
       " 'fmml0_si1670',\n",
       " 'fdms0_sx318',\n",
       " 'fmgd0_si934',\n",
       " 'mbpm0_sx317',\n",
       " 'felc0_si1386',\n",
       " 'mrjm4_si2119',\n",
       " 'mmdm2_sx102',\n",
       " 'mmjr0_si2166',\n",
       " 'fedw0_si1653',\n",
       " 'mjmp0_sx185',\n",
       " 'mjln0_sx369',\n",
       " 'mklt0_sx403',\n",
       " 'mmdb1_sx95',\n",
       " 'mmdb1_sx275',\n",
       " 'mtaa0_sx115',\n",
       " 'mtls0_sx290',\n",
       " 'mwjg0_si1754',\n",
       " 'mcmj0_sx284',\n",
       " 'fdms0_sx48',\n",
       " 'mbns0_sx230',\n",
       " 'fmml0_si2300',\n",
       " 'fcmh0_si1454',\n",
       " 'mdls0_sx278',\n",
       " 'mpdf0_si912',\n",
       " 'fadg0_sx379',\n",
       " 'mklt0_si1843',\n",
       " 'fjsj0_si2114',\n",
       " 'mrjm4_si859',\n",
       " 'fnmr0_sx409',\n",
       " 'mmdb1_si995',\n",
       " 'mdvc0_si2196',\n",
       " 'mrtk0_sx283',\n",
       " 'mjmp0_sx95',\n",
       " 'mjfc0_sx313',\n",
       " 'mrcs0_sx413',\n",
       " 'mtas1_sx388',\n",
       " 'mgwt0_sx279',\n",
       " 'mgrt0_si2080',\n",
       " 'mbdg0_sx293',\n",
       " 'mklt0_sx133',\n",
       " 'fdhc0_sx119',\n",
       " 'fadg0_sx289',\n",
       " 'mroa0_sx137',\n",
       " 'mrjr0_sx282',\n",
       " 'fnlp0_si1308',\n",
       " 'fkms0_sx140',\n",
       " 'fdrw0_si1423',\n",
       " 'fdms0_sx138',\n",
       " 'mwew0_sx101',\n",
       " 'fkms0_sx50',\n",
       " 'fcmh0_sx284',\n",
       " 'mbns0_sx140',\n",
       " 'fjem0_sx184',\n",
       " 'mthc0_sx385',\n",
       " 'mjmp0_si905',\n",
       " 'fadg0_si1279',\n",
       " 'fmml0_sx320',\n",
       " 'mgrt0_sx280',\n",
       " 'mgjf0_sx101',\n",
       " 'fdms0_sx228',\n",
       " 'mgjf0_sx371',\n",
       " 'mnjm0_sx50',\n",
       " 'fdac1_si844',\n",
       " 'mbwm0_sx134',\n",
       " 'majc0_sx385',\n",
       " 'mcmj0_si464',\n",
       " 'mwew0_si1361',\n",
       " 'mcsh0_sx109',\n",
       " 'mreb0_si2005',\n",
       " 'mreb0_sx115',\n",
       " 'mtdt0_si994',\n",
       " 'mlll0_si1363',\n",
       " 'fjem0_si1264',\n",
       " 'felc0_sx216',\n",
       " 'mrcs0_sx323',\n",
       " 'fdrw0_sx293',\n",
       " 'fdrw0_sx113',\n",
       " 'mklt0_sx313',\n",
       " 'mtls0_si2000',\n",
       " 'mrtk0_sx103',\n",
       " 'mbpm0_sx227',\n",
       " 'faks0_sx403',\n",
       " 'fgjd0_sx99',\n",
       " 'fmml0_sx140',\n",
       " 'mbpm0_si947',\n",
       " 'mjfc0_sx133',\n",
       " 'mlll0_sx103',\n",
       " 'fkms0_sx410',\n",
       " 'mtdt0_sx184',\n",
       " 'felc0_sx126',\n",
       " 'mteb0_si503',\n",
       " 'fadg0_si1909',\n",
       " 'mcsh0_sx199',\n",
       " 'mjfc0_si1033',\n",
       " 'felc0_sx306',\n",
       " 'fnmr0_sx229',\n",
       " 'fjsj0_sx134',\n",
       " 'mpdf0_sx102',\n",
       " 'mgrt0_sx370',\n",
       " 'mmdm2_si1555',\n",
       " 'mwbt0_sx113',\n",
       " 'mklt0_si1213',\n",
       " 'mwew0_si1991',\n",
       " 'fpkt0_si908',\n",
       " 'fjlm0_si1043',\n",
       " 'felc0_sx396',\n",
       " 'frew0_sx290',\n",
       " 'fmld0_sx295',\n",
       " 'fjem0_sx364',\n",
       " 'fmld0_sx115',\n",
       " 'fcal1_sx413',\n",
       " 'mjfc0_sx223',\n",
       " 'mmwh0_sx189',\n",
       " 'fpkt0_si1538',\n",
       " 'majc0_sx295',\n",
       " 'mbpm0_sx407',\n",
       " 'mmwh0_si1089',\n",
       " 'mrcs0_si1853',\n",
       " 'mbwm0_sx404',\n",
       " 'fcmh0_si824',\n",
       " 'mdlf0_sx53',\n",
       " 'mbdg0_si1463',\n",
       " 'fdms0_si1848',\n",
       " 'fnmr0_si1399',\n",
       " 'fedw0_si1714',\n",
       " 'mnjm0_si950',\n",
       " 'mjdh0_si1984',\n",
       " 'fjlm0_sx53',\n",
       " 'fmld0_sx205',\n",
       " 'fcal1_sx53',\n",
       " 'mlll0_sx283',\n",
       " 'mpam0_sx289',\n",
       " 'mrws1_sx50',\n",
       " 'fmml0_sx50',\n",
       " 'mdls0_si998',\n",
       " 'mrjr0_sx102',\n",
       " 'fmah0_si659',\n",
       " 'fedw0_sx364',\n",
       " 'mmwh0_si459',\n",
       " 'mwbt0_si2183',\n",
       " 'mtls0_sx110',\n",
       " 'mjmp0_sx365',\n",
       " 'mwew0_sx281',\n",
       " 'mers0_sx119',\n",
       " 'mmjr0_si2278',\n",
       " 'fjmg0_si551',\n",
       " 'mreb0_sx385',\n",
       " 'mlnt0_sx372',\n",
       " 'mjln0_sx279',\n",
       " 'majc0_si1946',\n",
       " 'mjfc0_sx403',\n",
       " 'fsem0_si568',\n",
       " 'mgwt0_sx369',\n",
       " 'mbns0_sx50',\n",
       " 'fedw0_sx184',\n",
       " 'fmgd0_sx214',\n",
       " 'fjsj0_sx224',\n",
       " 'mklt0_sx223',\n",
       " 'fnmr0_sx319',\n",
       " 'mers0_sx209',\n",
       " 'mmjr0_sx388',\n",
       " 'mbns0_sx320',\n",
       " 'mmdm2_si1452',\n",
       " 'mnjm0_sx410',\n",
       " 'fcal1_si1403',\n",
       " 'mlnt0_sx102',\n",
       " 'fnlp0_si1938',\n",
       " 'faks0_sx223',\n",
       " 'fdhc0_sx299',\n",
       " 'fmah0_si1919',\n",
       " 'fdhc0_si1559',\n",
       " 'fkms0_si2120',\n",
       " 'mcsh0_si919',\n",
       " 'mjar0_sx188',\n",
       " 'fnlp0_sx228',\n",
       " 'mrws1_sx320',\n",
       " 'fmml0_sx410',\n",
       " 'mjsw0_sx290',\n",
       " 'fadg0_sx109',\n",
       " 'mgwt0_si2169',\n",
       " 'fdac1_sx214',\n",
       " 'fjlm0_si1673',\n",
       " 'faks0_sx313',\n",
       " 'fsem0_sx298',\n",
       " 'fadg0_sx199',\n",
       " 'mwbt0_si1553',\n",
       " 'fdac1_si1474',\n",
       " 'mlnt0_sx192',\n",
       " 'mreb0_sx205',\n",
       " 'mnjm0_si1580',\n",
       " 'fedw0_sx94',\n",
       " 'mgjf0_si776',\n",
       " 'majc0_si2095',\n",
       " 'mdab0_sx139',\n",
       " 'mteb0_sx323',\n",
       " 'mbns0_si590',\n",
       " 'mtas1_sx118',\n",
       " 'mmdm2_sx372',\n",
       " 'fdms0_si1218',\n",
       " 'mers0_si1649',\n",
       " 'mpam0_si1189',\n",
       " 'fcal1_sx323',\n",
       " 'fnmr0_sx49',\n",
       " 'mgjf0_si1901',\n",
       " 'mwbt0_sx293',\n",
       " 'mdab0_sx49',\n",
       " 'mjar0_si1988',\n",
       " 'fmld0_si822',\n",
       " 'mmjr0_sx208',\n",
       " 'mdlf0_si953',\n",
       " 'mthc0_sx115',\n",
       " 'mdls0_sx98',\n",
       " 'mbpm0_si1577',\n",
       " 'mrjm4_sx49',\n",
       " 'fpas0_si2204',\n",
       " 'mrcs0_sx233',\n",
       " 'mbpm0_si1584',\n",
       " 'mthc0_si1645',\n",
       " 'mrtk0_sx373',\n",
       " 'mwbt0_sx203',\n",
       " 'faks0_si2203',\n",
       " 'fjsj0_sx404',\n",
       " 'mroa0_si677',\n",
       " 'mcmj0_si602',\n",
       " 'fgjd0_sx369',\n",
       " 'mglb0_sx364',\n",
       " 'mthc0_si1015',\n",
       " 'mmjr0_sx298',\n",
       " 'fdac1_sx394',\n",
       " 'mmjr0_sx118',\n",
       " 'mtdt0_si2254',\n",
       " 'mpam0_sx109',\n",
       " 'fsem0_si1198',\n",
       " 'mjar0_sx98',\n",
       " 'mjln0_sx189',\n",
       " 'mmwh0_sx99',\n",
       " 'mtaa0_si1285',\n",
       " 'mglb0_si1534',\n",
       " 'mrws1_sx230',\n",
       " 'fjsj0_sx314',\n",
       " 'fmah0_sx299',\n",
       " 'mmdm2_sx192',\n",
       " 'frew0_sx110',\n",
       " 'mglb0_sx94',\n",
       " 'fcal1_si2033',\n",
       " 'mrws1_sx140',\n",
       " 'mdlf0_sx323',\n",
       " 'mlnt0_si1574',\n",
       " 'mrws1_sx410',\n",
       " 'fpkt0_sx278',\n",
       " 'mrjr0_sx372',\n",
       " 'frew0_si1910',\n",
       " 'mtdt0_sx364',\n",
       " 'mdab0_si1039',\n",
       " 'mers0_si497',\n",
       " 'mdab0_sx319',\n",
       " 'fjem0_si1894',\n",
       " 'fcal1_sx143',\n",
       " 'fnmr0_sx139',\n",
       " 'fnlp0_sx138',\n",
       " 'mtls0_sx380',\n",
       " 'mbpm0_sx47',\n",
       " 'mcsh0_si1549',\n",
       " 'fnlp0_sx318',\n",
       " 'mjsw0_si1010',\n",
       " 'fcmh0_sx104',\n",
       " 'mdab0_sx229',\n",
       " 'mrjm4_si1489',\n",
       " 'fdac1_si2104',\n",
       " 'mtas1_si1473',\n",
       " 'mtls0_sx200',\n",
       " 'fnmr0_si769',\n",
       " 'mjln0_si2079',\n",
       " 'mrjr0_si1182',\n",
       " 'mrcs0_sx53',\n",
       " 'mcsh0_sx379',\n",
       " 'mdvc0_sx216',\n",
       " 'mdlf0_sx143',\n",
       " 'mbns0_si1850',\n",
       " 'fcmh0_sx194',\n",
       " 'fjmg0_si1811',\n",
       " 'mjdh0_sx184',\n",
       " 'fdms0_si1502',\n",
       " 'mpdf0_sx282',\n",
       " 'majc0_si835',\n",
       " 'mlll0_si733',\n",
       " 'mwbt0_si923',\n",
       " 'mlll0_sx373',\n",
       " 'fgjd0_si818',\n",
       " 'mroa0_si1970',\n",
       " 'mbdg0_sx113',\n",
       " 'mmwh0_sx369',\n",
       " 'mjmp0_sx275',\n",
       " 'mdls0_si1628',\n",
       " 'mjdh0_si724',\n",
       " 'mgjf0_si641',\n",
       " 'mwjg0_sx404',\n",
       " 'fjmg0_sx281',\n",
       " 'mbdg0_si2093',\n",
       " 'mdlf0_si1583',\n",
       " 'mgwt0_si1539',\n",
       " 'mlnt0_si642',\n",
       " 'mbdg0_sx203',\n",
       " 'fmgd0_si1564',\n",
       " 'fjlm0_sx143',\n",
       " 'mglb0_si904',\n",
       " 'mbns0_si1220',\n",
       " 'fsem0_si1828',\n",
       " 'mcmj0_si1094',\n",
       " 'mdab0_si1669',\n",
       " 'mnjm0_sx320',\n",
       " 'faks0_sx133',\n",
       " 'mwew0_sx191',\n",
       " 'mtaa0_si596',\n",
       " 'fcmh0_si2084',\n",
       " 'fnlp0_sx48',\n",
       " 'fpkt0_sx368',\n",
       " 'fjlm0_si2303',\n",
       " 'mdls0_si2258',\n",
       " 'mcsh0_sx289',\n",
       " 'fcmh0_sx374',\n",
       " 'mrcs0_si593',\n",
       " 'mbdg0_si833',\n",
       " 'fjmg0_sx101',\n",
       " 'fpas0_si944',\n",
       " 'mjar0_sx278',\n",
       " 'fdhc0_si929',\n",
       " 'mglb0_sx184',\n",
       " 'mnjm0_sx140',\n",
       " 'fjsj0_si1484',\n",
       " 'mdvc0_si2174',\n",
       " 'mbwm0_si674',\n",
       " 'frew0_sx380',\n",
       " 'mjmp0_si1791',\n",
       " 'fkms0_sx230',\n",
       " 'mgrt0_sx190',\n",
       " 'mroa0_sx47',\n",
       " 'mthc0_si2275',\n",
       " 'mtas1_sx208',\n",
       " 'fjsj0_si854',\n",
       " 'fkms0_si1490',\n",
       " 'fkms0_si860',\n",
       " 'fdhc0_si2189',\n",
       " 'mteb0_sx143',\n",
       " 'mjsw0_si1640',\n",
       " 'frew0_sx200',\n",
       " 'mrtk0_sx193',\n",
       " 'mgjf0_sx281',\n",
       " 'fjlm0_sx413',\n",
       " 'mrjm4_sx409',\n",
       " 'mreb0_si1375',\n",
       " 'mgwt0_sx99',\n",
       " 'fgjd0_sx189',\n",
       " 'fcal1_si773',\n",
       " 'mjln0_si819',\n",
       " 'mtls0_si1370',\n",
       " 'mthc0_sx205',\n",
       " 'mpdf0_si1542',\n",
       " 'fdrw0_sx203',\n",
       " 'mroa0_sx407',\n",
       " 'mmdb1_sx185',\n",
       " 'fmgd0_sx394',\n",
       " 'mgrt0_sx100',\n",
       " 'mmdb1_si2255',\n",
       " 'fmah0_si1289',\n",
       " 'mtas1_sx298',\n",
       " 'fpkt0_sx98',\n",
       " 'mroa0_si1307',\n",
       " 'fdrw0_si1283',\n",
       " 'mtls0_si740',\n",
       " 'mmdm2_si2082',\n",
       " 'majc0_sx115',\n",
       " 'mthc0_sx295',\n",
       " 'mmdb1_si1625',\n",
       " 'felc0_si2016',\n",
       " 'mlnt0_sx282',\n",
       " 'fsem0_sx208',\n",
       " 'mrjr0_si2313',\n",
       " 'fmgd0_sx304',\n",
       " 'mdvc0_si936',\n",
       " 'fmah0_sx119',\n",
       " 'fjmg0_si1181',\n",
       " 'mjln0_si1449']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(speaker_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aa': 0, 'el': 14, 'ch': 8, 'ae': 1, 'eh': 13, 'cl': 9, 'ah': 2, 'ao': 3, 'ih': 22, 'en': 15, 'ey': 18, 'aw': 4, 'ay': 6, 'ax': 5, 'er': 17, 'vcl': 42, 'ng': 29, 'iy': 24, 'sh': 35, 'th': 38, 'sil': 36, 'zh': 46, 'w': 44, 'dh': 11, 'v': 43, 'ix': 23, 'y': 45, 'hh': 21, 'jh': 25, 'dx': 12, 'b': 7, 'd': 10, 'g': 20, 'f': 19, 'k': 26, 'm': 28, 'l': 27, 'n': 30, 'uh': 40, 'p': 33, 's': 37, 'r': 34, 't': 39, 'oy': 32, 'epi': 16, 'ow': 31, 'z': 47, 'uw': 41}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'aa',\n",
       " 1: 'ae',\n",
       " 2: 'ah',\n",
       " 3: 'aa',\n",
       " 4: 'aw',\n",
       " 5: 'ah',\n",
       " 6: 'ay',\n",
       " 7: 'b',\n",
       " 8: 'ch',\n",
       " 9: 'sil',\n",
       " 10: 'd',\n",
       " 11: 'dh',\n",
       " 12: 'dx',\n",
       " 13: 'eh',\n",
       " 14: 'l',\n",
       " 15: 'n',\n",
       " 16: 'sil',\n",
       " 17: 'er',\n",
       " 18: 'ey',\n",
       " 19: 'f',\n",
       " 20: 'g',\n",
       " 21: 'hh',\n",
       " 22: 'ih',\n",
       " 23: 'ih',\n",
       " 24: 'iy',\n",
       " 25: 'jh',\n",
       " 26: 'k',\n",
       " 27: 'l',\n",
       " 28: 'm',\n",
       " 29: 'ng',\n",
       " 30: 'n',\n",
       " 31: 'ow',\n",
       " 32: 'oy',\n",
       " 33: 'p',\n",
       " 34: 'r',\n",
       " 35: 'sh',\n",
       " 36: 'sil',\n",
       " 37: 's',\n",
       " 38: 'th',\n",
       " 39: 't',\n",
       " 40: 'uh',\n",
       " 41: 'uw',\n",
       " 42: 'sil',\n",
       " 43: 'v',\n",
       " 44: 'w',\n",
       " 45: 'y',\n",
       " 46: 'sh',\n",
       " 47: 'z'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# speaker_sequence\n",
    "print phones_mapping\n",
    "result_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'r', 1: 'e', 2: 's', 3: 'o', 4: 'l', 5: 'v', 6: 'd', 7: ' ', 8: 't', 9: 'h', 10: 'a', 11: 'n', 12: 'i', 13: 'y', 14: 'm', 15: 'b', 16: 'c', 17: 'g', 18: 'p', 19: 'f', 20: 'u', 21: 'w', 22: 'k', 23: \"'\", 24: 'q', 25: 'x', 26: 'z', 27: 'j', 28: '-'}\n",
      "29\n",
      "592\n",
      "619\n",
      "(117,)\n"
     ]
    }
   ],
   "source": [
    "# print y_train[0]\n",
    "# x_train[0][0].shape[0]\n",
    "print (char_unmap)\n",
    "print len(char_unmap)\n",
    "\n",
    "print len(x_train)\n",
    "print len(x_train[0])\n",
    "print x_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print x_train.shape\n",
    "# print x_train[0].shape\n",
    "# print x_train[0][1]\n",
    "len(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print x_test[0].shape\n",
    "# print x_test[0][1][0]\n",
    "len(phones_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data part\n",
    "\n",
    "## make batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_batches_X(X, length, batch_size=30):\n",
    "    '''\n",
    "    https://github.com/craffel/lstm_benchmarks/blob/master/lasagne/experiment.py\n",
    "    \n",
    "    Convert a list of matrices into batches of uniform length\n",
    "    :parameters:\n",
    "        - X : list of np.ndarray\n",
    "            List of matrices\n",
    "        - length : int\n",
    "            Desired sequence length.  Smaller sequences will be padded with 0s,\n",
    "            longer will be truncated.\n",
    "        - batch_size : int\n",
    "            Mini-batch size\n",
    "    :returns:\n",
    "        - X_batch : np.ndarray\n",
    "            Tensor of time series matrix batches,\n",
    "            shape=(n_batches, length, batch_size, n_features)\n",
    "        - X_mask : np.ndarray\n",
    "            shape=(n_batches, length, batch_size)\n",
    "            Mask denoting whether to include each time step of each time series\n",
    "            matrix\n",
    "    '''\n",
    "#     limit = len(X)//batch_size\n",
    "    n_batches = int(round(len(x_train)*1./batch_size))\n",
    "    \n",
    "    # Lasagne format\n",
    "    X_batch = np.zeros((n_batches, batch_size, length, X[0].shape[1]),\n",
    "                       dtype=theano.config.floatX)\n",
    "    # Else format\n",
    "#     X_batch = np.zeros((n_batches, length, batch_size, X[0].shape[1]),\n",
    "#                        dtype=theano.config.floatX)\n",
    "    \n",
    "#     X_mask = np.zeros(X_batch.shape, dtype=np.bool)\n",
    "    \n",
    "    X_mask = np.zeros((n_batches, length, batch_size ), dtype=theano.config.floatX)\n",
    "#     X_mask = np.zeros((n_batches,  batch_size, length ), dtype=theano.config.floatX)\n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for b in range(n_batches): \n",
    "        for n in range(batch_size): # go thorough batch size\n",
    "            \n",
    "            if ( len(x_train) > (b*batch_size + n )):\n",
    "#                 print b*batch_size + n\n",
    "                count += 1\n",
    "                \n",
    "                X_m = X[b*batch_size + n] # seq_length X feature dim\n",
    "\n",
    "                X_batch[b, n, :X_m.shape[0]] = X_m[:length]\n",
    "    \n",
    "                X_mask[b, :X_m.shape[0], n] = 1\n",
    "    print count\n",
    "            \n",
    "    return X_batch, X_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "535\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Find the longest sequence\n",
    "# length_x = (max([X.shape[0] for X in x_train]))\n",
    "\n",
    "# print length_x\n",
    "\n",
    "length_x = 777\n",
    "\n",
    "# Convert to batches of time series of uniform length\n",
    "# x_train_mask: seq_length X batch_size\n",
    "x_train__, x_train_mask__ = make_batches_X(x_train, length_x, batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 32, 777, 117)\n",
      "================mask================\n",
      "(19, 777, 32)\n"
     ]
    }
   ],
   "source": [
    "print x_train__.shape\n",
    "print \"================mask================\"\n",
    "print x_train_mask__.shape\n",
    "\n",
    "# print x_train_mask[0][400][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data part\n",
    "\n",
    "## decode for phone based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "credit by NLTK package of measure edit distance\n",
    "'''\n",
    "\n",
    "def _edit_dist_init(len1, len2):\n",
    "    lev = []\n",
    "    for i in range(len1):\n",
    "        lev.append([0] * len2)  # initialize 2D array to zero\n",
    "    for i in range(len1):\n",
    "        lev[i][0] = i           # column 0: 0,1,2,3,4,...\n",
    "    for j in range(len2):\n",
    "        lev[0][j] = j           # row 0: 0,1,2,3,4,...\n",
    "    return lev\n",
    "\n",
    "\n",
    "def _edit_dist_step(lev, i, j, s1, s2, transpositions=False):\n",
    "    c1 = s1[i - 1]\n",
    "    c2 = s2[j - 1]\n",
    "\n",
    "    # skipping a character in s1\n",
    "    a = lev[i - 1][j] + 1\n",
    "    # skipping a character in s2\n",
    "    b = lev[i][j - 1] + 1\n",
    "    # substitution\n",
    "    c = lev[i - 1][j - 1] + (c1 != c2)\n",
    "\n",
    "    # transposition\n",
    "    d = c + 1  # never picked by default\n",
    "    if transpositions and i > 1 and j > 1:\n",
    "        if s1[i - 2] == c2 and s2[j - 2] == c1:\n",
    "            d = lev[i - 2][j - 2] + 1\n",
    "\n",
    "    # pick the cheapest\n",
    "    lev[i][j] = min(a, b, c, d)\n",
    "\n",
    "\n",
    "def check_label_error( real , predict, transpositions=False):\n",
    "    ## length of real >= length of predict\n",
    "    \"\"\"\n",
    "    Calculate the Levenshtein edit-distance between two strings.\n",
    "    The edit distance is the number of characters that need to be\n",
    "    substituted, inserted, or deleted, to transform s1 into s2.  For\n",
    "    example, transforming \"rain\" to \"shine\" requires three steps,\n",
    "    consisting of two substitutions and one insertion:\n",
    "    \"rain\" -> \"sain\" -> \"shin\" -> \"shine\".  These operations could have\n",
    "    been done in other orders, but at least three steps are needed.\n",
    "\n",
    "    This also optionally allows transposition edits (e.g., \"ab\" -> \"ba\"),\n",
    "    though this is disabled by default.\n",
    "\n",
    "    :param s1, s2: The strings to be analysed\n",
    "    :param transpositions: Whether to allow transposition edits\n",
    "    :type s1: str\n",
    "    :type s2: str\n",
    "    :type transpositions: bool\n",
    "    :rtype int\n",
    "    \"\"\"\n",
    "    # set up a 2-D array\n",
    "    len1 = len(predict)\n",
    "    len2 = len(real)\n",
    "    lev = _edit_dist_init(len1 + 1, len2 + 1)\n",
    "\n",
    "    # iterate over the array\n",
    "    for i in range(len1):\n",
    "        for j in range(len2):\n",
    "            _edit_dist_step(lev, i + 1, j + 1, predict, real, transpositions=transpositions)\n",
    "            \n",
    "    return lev[len1][len2]*1.0/len2\n",
    "\n",
    "\n",
    "def clean_up_phone( y ):\n",
    "    \"\"\"\n",
    "    for final output clean up\n",
    "    B(a − ab−) = B(−aa − −abb) = aab\n",
    "    \"\"\"\n",
    "    answer = []\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        \n",
    "        if y[i] == len(phones_mapping): # blank\n",
    "                continue\n",
    "        if i != 0:\n",
    "            if y[i-1] != y[i]:\n",
    "                answer.append(y[i])\n",
    "        else:\n",
    "            answer.append(y[i])\n",
    "    \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def check_phone_err(y_actual, y_pred, y_mask_actual, y_mask_pred, batch_size):\n",
    "    \n",
    "    err = 0.\n",
    "    for i in xrange(batch_size):\n",
    "#         print i\n",
    "        mask_actual = np.swapaxes( y_mask_actual, 0, 1)[i]\n",
    "        ans_actual = np.swapaxes( y_actual, 0 , 1)[i]\n",
    "        mask_pred = np.swapaxes( y_mask_pred, 0, 1)[i]\n",
    "        ans_pred = y_pred[i]\n",
    "        \n",
    "        y_rrr = ans_actual[ np.nonzero(mask_actual) ]\n",
    "        y_ppp = ans_pred[ np.nonzero(mask_pred) ]\n",
    "        \n",
    "        y_ppp = clean_up_phone( y_ppp )\n",
    "        \n",
    "#         print \"Target \",y_rrr\n",
    "#         print \"\\t=>\", y_ppp\n",
    "        \n",
    "        err += check_label_error(y_rrr, y_ppp)\n",
    "        \n",
    "#         score += accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    return err\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class end_to_end():\n",
    "    \n",
    "    def __init__(self, input_shape, max_seq_length, hidden_layer, rnn_hidden_layer,\n",
    "                 batch, max_epochs, eval_size, output_num_units,\n",
    "                 patience, up_learning_rate, up_momentum, file_name):\n",
    "        \n",
    "        self.input_shape = input_shape # [batch, dim]\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.hidden_layer = hidden_layer # hidden [l1, l2, l3]\n",
    "        self.rnn_hidden_layer = rnn_hidden_layer # rnn hidden [l1]\n",
    "        self.output_num_units = output_num_units # [ # of class ]\n",
    "        \n",
    "        self.batch = batch\n",
    "        self.max_epochs = max_epochs\n",
    "        self.eval_size = eval_size\n",
    "        \n",
    "        self.up_learning_rate = up_learning_rate\n",
    "        self.up_momentum = up_momentum\n",
    "                \n",
    "        self.patience = patience\n",
    "        self.best_valid = np.inf\n",
    "        self.best_valid_epoch = 0\n",
    "        self.best_params = None\n",
    "        \n",
    "        self.train_history_ = []\n",
    "        \n",
    "        self.file_name = file_name\n",
    "        \n",
    "        self.drop_frac = 0.2\n",
    "#         X = T.matrix('X')\n",
    "#         Y = T.ivector('Y')     \n",
    "        \n",
    "        \"\"\"\n",
    "        input data type\n",
    "        y_hat : T x B x C+1\n",
    "        y : L x B\n",
    "        y_hat_mask : T x B\n",
    "        y_mask : L x B\n",
    "        \"\"\"\n",
    "        \n",
    "        # T x B x F\n",
    "        # B X T X F (Lasagne format)\n",
    "        x = T.tensor3('X', dtype=theano.config.floatX)\n",
    "        # L x B\n",
    "        y = T.matrix('y', dtype=theano.config.floatX)\n",
    "\n",
    "        # L x B\n",
    "        y_mask = T.matrix('y_mask', dtype=theano.config.floatX)\n",
    "        # T x B\n",
    "        y_hat_mask = T.matrix('y_hat_mask', dtype=theano.config.floatX)\n",
    "    \n",
    "        # Min/max sequence length\n",
    "#         MIN_LENGTH = 50\n",
    "        MAX_LENGTH = max_seq_length\n",
    "        # Number of units in the hidden (recurrent) layer\n",
    "        N_HIDDEN = hidden_layer[0]\n",
    "        # Number of training sequences in each batch\n",
    "        N_BATCH = batch\n",
    "        # Optimization learning rate\n",
    "        GRAD_CLIP = 100\n",
    "        # How often should we check the output?\n",
    "        EPOCH_SIZE = 100\n",
    "        # Number of epochs to train the net\n",
    "        NUM_EPOCHS = 10\n",
    "        \n",
    "        #===========================================================================================\n",
    "        # Bi RNN\n",
    "        #===========================================================================================\n",
    "\n",
    "        # Recurrent layers expect input of shape\n",
    "        # (batch size, max sequence length, number of features)\n",
    "        # T x B x F\n",
    "#         l_in = lasagne.layers.InputLayer(shape=( N_BATCH, MAX_LENGTH,  self.input_shape[1]))\n",
    "        \n",
    "        # We're using a bidirectional network, which means we will combine two\n",
    "        # RecurrentLayers, one with the backwards=True keyword argument.\n",
    "        # Setting a value for grad_clipping will clip the gradients in the layer\n",
    "#         l_forward = lasagne.layers.RecurrentLayer(l_in, N_HIDDEN, grad_clipping=GRAD_CLIP,\n",
    "#                                                   W_in_to_hid=lasagne.init.HeUniform(),\n",
    "#                                                   W_hid_to_hid=lasagne.init.HeUniform(),\n",
    "#                                                   nonlinearity=lasagne.nonlinearities.tanh)\n",
    "#         l_backward = lasagne.layers.RecurrentLayer(l_in, N_HIDDEN, grad_clipping=GRAD_CLIP,\n",
    "#                                                    W_in_to_hid=lasagne.init.HeUniform(),\n",
    "#                                                    W_hid_to_hid=lasagne.init.HeUniform(),\n",
    "#                                                    nonlinearity=lasagne.nonlinearities.tanh, \n",
    "#                                                    backwards=True)\n",
    "        # We'll use an elementwise sum to combine the forward/backward layers\n",
    "#         l_sum = lasagne.layers.ElemwiseSumLayer([l_forward, l_backward])\n",
    "        \n",
    "        # We need a reshape layer which combines the first (batch size) and second\n",
    "        # (number of timesteps) dimensions, otherwise the DenseLayer will treat the\n",
    "        # number of time steps as a feature dimension\n",
    "#         l_reshape = lasagne.layers.ReshapeLayer( l_sum, (N_BATCH*MAX_LENGTH, N_HIDDEN))\n",
    "        \n",
    "        #===========================================================================================\n",
    "        # dEEP lSTM\n",
    "        #===========================================================================================\n",
    "        \n",
    "        # Recurrent layers expect input of shape\n",
    "        # (batch size, max sequence length, number of features)\n",
    "        l_in = lasagne.layers.InputLayer(shape=(N_BATCH, MAX_LENGTH,  self.input_shape[1]))\n",
    "        \n",
    "        l_in_gau = lasagne.layers.GaussianNoiseLayer(l_in, sigma=0.5, )\n",
    "        \n",
    "        # LSTM layer 1\n",
    "        l_forward_1 = lasagne.layers.LSTMLayer(l_in_gau, num_units=hidden_layer[0], \n",
    "                                               learn_init=True, peepholes=True)\n",
    "        l_backward_1 = lasagne.layers.LSTMLayer(l_in, num_units=hidden_layer[0],\n",
    "                                                backwards=True, learn_init=True, peepholes=True)\n",
    "        l_recurrent_1 = ElemwiseSumLayer([l_forward_1, l_backward_1])\n",
    "  \n",
    "        # concatenate forward and backward LSTM layers\n",
    "#         l_fwd_reshape_1 = lasagne.layers.ReshapeLayer(l_forward_1, (N_BATCH*MAX_LENGTH, hidden_layer[0]))\n",
    "#         l_bck_reshape_1 = lasagne.layers.ReshapeLayer(l_backward_1, (N_BATCH*MAX_LENGTH, hidden_layer[0]))\n",
    "#         l_recurrent_1 = lasagne.layers.ConcatLayer([l_fwd_reshape_1, l_bck_reshape_1], axis=1)\n",
    "        \n",
    "#         if self.drop_frac > 0:\n",
    "#             l_drp_1 = lasagne.layers.DropoutLayer(l_recurrent_1, p = 0.2 )\n",
    "#         else:\n",
    "#             l_drp_1 = l_recurrent_1\n",
    "\n",
    "        # LSTM layer 2\n",
    "        l_forward_2 = lasagne.layers.LSTMLayer(l_recurrent_1, num_units=hidden_layer[1], \n",
    "                                               learn_init=True, peepholes=True)\n",
    "        l_backward_2 = lasagne.layers.LSTMLayer(l_recurrent_1, num_units=hidden_layer[1],\n",
    "                                                backwards=True, learn_init=True, peepholes=True)\n",
    "        l_recurrent_2 = ElemwiseSumLayer([l_forward_2, l_backward_2])\n",
    "        \n",
    "#         if self.drop_frac > 0:\n",
    "#             l_drp_2 = lasagne.layers.DropoutLayer(l_recurrent_2, p = 0.2 )\n",
    "#         else:\n",
    "#             l_drp_2 = l_recurrent_2\n",
    "        \n",
    "        # LSTM layer 3\n",
    "        l_forward_3 = lasagne.layers.LSTMLayer(l_recurrent_2, num_units=hidden_layer[2], \n",
    "                                               learn_init=True, peepholes=True)\n",
    "        l_backward_3 = lasagne.layers.LSTMLayer(l_recurrent_2, num_units=hidden_layer[2],\n",
    "                                                backwards=True, learn_init=True, peepholes=True)\n",
    "        l_recurrent_3 = ElemwiseSumLayer( [l_forward_3, l_backward_3] )\n",
    "        \n",
    "#         if self.drop_frac > 0:\n",
    "#             l_drp_3 = lasagne.layers.DropoutLayer(l_recurrent_3, p = 0.2 )\n",
    "#         else:\n",
    "#             l_drp_3 = l_recurrent_3\n",
    "\n",
    "        \n",
    "        # LSTM layer 4\n",
    "        l_forward_4 = lasagne.layers.LSTMLayer(l_recurrent_3, num_units=hidden_layer[3], \n",
    "                                               learn_init=True, peepholes=True)\n",
    "        l_backward_4 = lasagne.layers.LSTMLayer(l_recurrent_3, num_units=hidden_layer[3],\n",
    "                                                backwards=True, learn_init=True, peepholes=True)\n",
    "        l_recurrent_4 = ElemwiseSumLayer( [l_forward_4, l_backward_4] )\n",
    "\n",
    "#         if self.drop_frac > 0:\n",
    "#             l_drp_4 = lasagne.layers.DropoutLayer(l_recurrent_4, p = 0.2 )\n",
    "#         else:\n",
    "#             l_drp_4 = l_recurrent_4\n",
    "            \n",
    "        # LSTM layer 5\n",
    "#         l_forward_5 = lasagne.layers.LSTMLayer(l_drp_4, num_units=hidden_layer[4], \n",
    "#                                                learn_init=True, peepholes=True)\n",
    "#         l_backward_5 = lasagne.layers.LSTMLayer(l_drp_4, num_units=hidden_layer[4],\n",
    "#                                                 backwards=True, learn_init=True, peepholes=True)\n",
    "#         l_recurrent_5 = ElemwiseSumLayer( [l_forward_5, l_backward_5] )\n",
    "\n",
    "#         if self.drop_frac > 0:\n",
    "#             l_drp_5 = lasagne.layers.DropoutLayer(l_recurrent_5, p = 0.2 )\n",
    "#         else:\n",
    "#             l_drp_5 = l_recurrent_5\n",
    "\n",
    "\n",
    "        \n",
    "        l_reshape = lasagne.layers.ReshapeLayer(l_recurrent_4,\n",
    "                                               (N_BATCH*MAX_LENGTH, hidden_layer[3]))\n",
    "\n",
    "        \n",
    "        #===========================================================================================\n",
    "        # COMMON SETUP\n",
    "        #===========================================================================================\n",
    "        \n",
    "        # Our output layer is a simple dense connection\n",
    "        l_recurrent_out = lasagne.layers.DenseLayer( l_reshape, num_units= output_num_units[0] ,\n",
    "                                                    nonlinearity=lasagne.nonlinearities.identity)\n",
    "        \n",
    "        # Now, reshape the output back to the RNN format\n",
    "        l_out_shp = lasagne.layers.ReshapeLayer(l_recurrent_out, (N_BATCH, MAX_LENGTH, output_num_units[0] ))\n",
    "        \n",
    "        # dimshuffle to shape format (input_seq_len, batch_size, num_classes + 1)\n",
    "        l_out_shp_ctc = lasagne.layers.DimshuffleLayer(l_out_shp, (1, 0, 2))\n",
    "\n",
    "        l_out_softmax = lasagne.layers.NonlinearityLayer(\n",
    "                            l_recurrent_out, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "        l_out_softmax_shp = lasagne.layers.ReshapeLayer(\n",
    "                            l_out_softmax, (N_BATCH, MAX_LENGTH, output_num_units[0] ))\n",
    "        \n",
    "        # since we use gaussian noise in input, False means use noise, True means dont use noise\n",
    "        output_lin_ctc_train = lasagne.layers.get_output(l_out_shp_ctc, x, deterministic=False)\n",
    "        output_softmax_train = lasagne.layers.get_output(l_out_softmax_shp, x, deterministic=False)\n",
    "        \n",
    "        output_lin_ctc_val = lasagne.layers.get_output(l_out_shp_ctc, x, deterministic=True)\n",
    "        output_softmax_val = lasagne.layers.get_output(l_out_softmax_shp, x, deterministic=True)\n",
    "        \n",
    "        self.all_params = lasagne.layers.get_all_params(l_out_shp)\n",
    "\n",
    "        # the CTC cross entropy between y and linear output network\n",
    "        pseudo_cost = ctc_cost_2.pseudo_cost(\n",
    "            y, output_lin_ctc_train, y_mask, y_hat_mask,\n",
    "            skip_softmax=True)\n",
    "        \n",
    "        \n",
    "        pseudo_cost_grad = T.grad(pseudo_cost.mean(), self.all_params)\n",
    "        true_cost = ctc_cost_2.cost(y, output_softmax_train.dimshuffle(1, 0, 2), y_mask, y_hat_mask)\n",
    "        cost = T.mean(true_cost)\n",
    "        updates = lasagne.updates.rmsprop(pseudo_cost_grad, self.all_params, learning_rate=0.0001)\n",
    "\n",
    "        self.train = theano.function(\n",
    "            inputs = [x, y, y_hat_mask, y_mask],\n",
    "            outputs = [ pseudo_cost.mean(), cost, output_softmax_train ],\n",
    "#             outputs = [output_lin_ctc, output_softmax, cost],\n",
    "            updates=updates\n",
    "        )\n",
    "        \n",
    "        self.predict = theano.function( \n",
    "            inputs=[x], \n",
    "            outputs = [ output_softmax_val] \n",
    "        )\n",
    "\n",
    "        \n",
    "    # x_mask and y_mask is the same\n",
    "    # x_test_mask and y_test_mask is the same\n",
    "    def fit(self, x_train, y_train, x_test,  y_test , x_mask, y_mask, x_test_mask, y_test_mask ):\n",
    "        print \" \"\n",
    "        print \"start training!!!!\"\n",
    "        print \" \"\n",
    "        \n",
    "        epochs = 0\n",
    "        for i in range(self.max_epochs):\n",
    "            epochs +=1\n",
    "            t0 = time()\n",
    "            \n",
    "            cs = 0.\n",
    "            pseudo_cs = 0.\n",
    "            for index in range(len(x_train)):\n",
    "                pseudo, cost, output_softmax = self.train( x_train[index] , y_train[index],\n",
    "                                                          x_mask[index], y_mask[index])\n",
    "                cs += cost\n",
    "                pseudo_cs += pseudo\n",
    "                gg = index\n",
    "#                 print pseudo, cost\n",
    "                \n",
    "            cs /= len(x_train)\n",
    "            pseudo_cs /= len(x_train)\n",
    "            \n",
    "#             cPickle.dump( self , open(\"./\"+self.file_name+\".pkl\",\"wb\"))\n",
    "            if epochs <= 1:\n",
    "                previous_cs = \"-\"\n",
    "            else:\n",
    "                previous_cs = self.train_history_[-1]['cost']\n",
    "            print \"\\n===============================\"\n",
    "            print 'epoch {0} : pseudo= {1}, cost= {2}, previous_cost= {3}, train_time = {4} s'.format(epochs,\n",
    "                                                                                                      pseudo_cs, \n",
    "                                                                                                      cs,\n",
    "                                                                                                      previous_cs, \n",
    "                                                                                                      time() - t0)\n",
    "            \n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            t0 = time()\n",
    "            \n",
    "#             print np.argmax(output_softmax[:],axis=2)\n",
    "#             print decode_all( np.argmax(output_softmax[:],axis=2) , x_train_mask[gg],  batch_size , True)\n",
    "            \n",
    "            # save model first\n",
    "            # dont know the LER performance yet\n",
    "#             cPickle.dump( self , open(\"./\"+self.file_name+\".pkl\",\"wb\"))\n",
    "    \n",
    "\n",
    "            err = 0.\n",
    "            for index in range( len(x_test) ) :\n",
    "\n",
    "                prepre = self.predict( x_test[index])\n",
    "                \n",
    "                err += check_phone_err(y_test[index], np.argmax(prepre[0],axis=2), \n",
    "                                           y_test_mask[index], x_test_mask[index], batch_size)\n",
    "#                 label_error_rate += check_all( y_test[index], y_test_mask[index],\n",
    "#                                               np.argmax(prepre[0],axis=2) , x_test_mask[index],\n",
    "#                                               batch_size)\n",
    "                \n",
    "#                 if (( index + epochs ) % 4 == 0 ):\n",
    "#                     ## print actual\n",
    "#                     y_actual = decode_all_actual( y_test[index], y_test_mask[index], batch_size , True)\n",
    "\n",
    "#                     ## print pred\n",
    "#                     y_predict = decode_all_pred( np.argmax(prepre[0],axis=2) , x_test_mask[index],  batch_size , False)\n",
    "                \n",
    "            err /= len(x_test)\n",
    "            self.train_history_.append({\"epoch\":epochs, \"cost\": cs, \"err\":err})\n",
    "            \n",
    "            print '\\t\\t\\t val_= {0}, test_time  = {1} s'.format(err, time() - t0)\n",
    "            print \"===============================\\n\"\n",
    "            \n",
    "            sys.stdout.flush()\n",
    "            \n",
    "#             for a , b in zip (y_actual,y_predict):\n",
    "#                 print a \n",
    "#                 print \"====>>\",b\n",
    "            \n",
    "#             sys.stdout.flush()\n",
    "\n",
    "            for index in range( len(x_test) ) :\n",
    "\n",
    "                prepre = self.predict( x_test[index])\n",
    "                \n",
    "                err += check_phone_err(y_test[index], np.argmax(prepre[0],axis=2), \n",
    "                                           y_test_mask[index], x_test_mask[index], batch_size)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            should use cost do early stopping\n",
    "            \"\"\"\n",
    "            current_cs = self.train_history_[-1]['cost']\n",
    "            current_epoch = self.train_history_[-1]['epoch']\n",
    "            if current_cs < self.best_valid:\n",
    "                self.best_valid = current_cs\n",
    "                self.best_valid_epoch = current_epoch\n",
    "                self.best_params = [w.get_value() for w in self.all_params]\n",
    "                cPickle.dump( self , open(\"./\"+self.file_name+\".pkl\",\"wb\"))\n",
    "                \n",
    "            elif self.best_valid_epoch + self.patience <= current_epoch:\n",
    "                print \"\"\n",
    "                print \"Early stopping.\"\n",
    "                print self.best_valid_epoch,self.best_valid\n",
    "                print \"Best valid ler {:.6f} at epoch {}.\".format(self.best_valid, self.best_valid_epoch)              \n",
    "#                 for qq in range (len(self.all_params)):\n",
    "#                     self.all_params[qq].set_value( self.best_params[qq] )\n",
    "#                 break\n",
    "\n",
    "            \"\"\"\n",
    "            can not use label error rate do early stopping\n",
    "            \"\"\"\n",
    "#             current_ler = self.train_history_[-1]['LER']\n",
    "#             current_epoch = self.train_history_[-1]['epoch']\n",
    "#             if current_ler < self.best_valid:\n",
    "# #                 print \"********************* Now best ************************\"\n",
    "#                 sys.stdout.flush()\n",
    "#                 self.best_valid = current_ler\n",
    "#                 self.best_valid_epoch = current_epoch\n",
    "#                 self.best_params = [w.get_value() for w in self.all_params]\n",
    "# #                 cPickle.dump( self , open(\"./\"+self.file_name+\".pkl\",\"wb\"))\n",
    "                \n",
    "#             elif self.best_valid_epoch + self.patience <= current_epoch:\n",
    "#                 print \"\"\n",
    "#                 print \"Early stopping.\"\n",
    "#                 print self.best_valid_epoch,self.best_valid\n",
    "#                 print \"Best valid ler {:.6f} at epoch {}.\".format(self.best_valid, self.best_valid_epoch)\n",
    "#                 sys.stdout.flush()                \n",
    "#                 for qq in range (len(self.all_params)):\n",
    "#                     self.all_params[qq].set_value( self.best_params[qq] )\n",
    "#                 break\n",
    "\n",
    "\n",
    "#     def prediction(self, x, x_mask) :\n",
    "        \n",
    "#         abc =  self.predict(x, x_mask)\n",
    "        \n",
    "#         return np.argmax(abc[0], axis = 1 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pika/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:133: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n",
      "  from scan_perform.scan_perform import *\n"
     ]
    }
   ],
   "source": [
    "with open('model_phone_1.pkl', 'rb') as f:\n",
    "    net5_see_see = cPickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_train__, x_train_mask__\n",
    "\n",
    "# def check_phone_err(y_actual, y_pred, y_mask_actual, y_mask_pred, batch_size):\n",
    "    \n",
    "#     err = 0.\n",
    "#     for i in xrange(batch_size):\n",
    "# #         print i\n",
    "#         mask_actual = np.swapaxes( y_mask_actual, 0, 1)[i]\n",
    "#         ans_actual = np.swapaxes( y_actual, 0 , 1)[i]\n",
    "#         mask_pred = np.swapaxes( y_mask_pred, 0, 1)[i]\n",
    "#         ans_pred = y_pred[i]\n",
    "        \n",
    "#         y_rrr = ans_actual[ np.nonzero(mask_actual) ]\n",
    "#         y_ppp = ans_pred[ np.nonzero(mask_pred) ]\n",
    "        \n",
    "#         y_ppp = clean_up_phone( y_ppp )\n",
    "        \n",
    "        \n",
    "#         err += check_label_error(y_rrr, y_ppp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "535"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(speaker_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# for index in range( len(x_test) ) :\n",
    "\n",
    "#     prepre = self.predict( x_test[index])\n",
    "\n",
    "#     err += check_phone_err(y_test[index], np.argmax(prepre[0],axis=2), \n",
    "#                                y_test_mask[index], x_test_mask[index], batch_size)\n",
    "\n",
    "result = '/home/pika/MLDS_2015/final/WFST2/Lexicon_WFST/result/ctc_phone_result.txt'\n",
    "speaker = '/home/pika/MLDS_2015/final/WFST2/Lexicon_WFST/result/ctc_phone_speaker_sequence.txt'\n",
    "f = open(result, 'w+')\n",
    "sss = open(speaker, 'w+')\n",
    "\n",
    "# count = 0\n",
    "for name in speaker_sequence:\n",
    "#     print count\n",
    "    sss.write(name+\"\\n\")\n",
    "#     count += 1\n",
    "\n",
    "count = 0\n",
    "\n",
    "for index in range( len(x_train__)  ) : # num of batch\n",
    "\n",
    "    prepre = net5_see_see.predict( x_train__[index])\n",
    "\n",
    "    y_pred = np.argmax(prepre[0],axis=2)\n",
    "\n",
    "    for i in xrange(batch_size): # go through the batch, 1..32\n",
    "        \n",
    "        if (count < len(speaker_sequence) ):\n",
    "#             print count\n",
    "            mask_pred = np.swapaxes( x_train_mask__[index], 0, 1)[i]\n",
    "            y_ppp = y_pred[i]\n",
    "            y_ppp = y_ppp[ np.nonzero(mask_pred) ]\n",
    "            y_ppp = clean_up_phone( y_ppp )\n",
    "            \n",
    "            for value in range(len(y_ppp)):\n",
    "                if value+1 != len(y_ppp):\n",
    "                    f.write( str(  result_mapping[ y_ppp[value] ] ) + \" \") # map to phone\n",
    "                else:\n",
    "                    f.write(\"\\n\")\n",
    "                \n",
    "                    \n",
    "            count += 1\n",
    "    \n",
    "            \n",
    "            \n",
    "#         mask_actual = np.swapaxes( y_mask_actual, 0, 1)[i]\n",
    "#         ans_actual = np.swapaxes( y_actual, 0 , 1)[i]\n",
    "#         mask_pred = np.swapaxes( y_mask_pred, 0, 1)[i]\n",
    "#         ans_pred = y_pred[i]\n",
    "        \n",
    "#         y_rrr = ans_actual[ np.nonzero(mask_actual) ]\n",
    "#         y_ppp = ans_pred[ np.nonzero(mask_pred) ]\n",
    "        \n",
    "#         y_ppp = clean_up_phone( y_ppp )\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     err += check_phone_err(y_test[index], np.argmax(prepre[0],axis=2), \n",
    "#                                y_test_mask[index], x_test_mask[index], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prepre = net5_see_see.predict( x_train__[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 777, 49)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(prepre[0],axis=2)\n",
    "prepre[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(prepre[0],axis=2)\n",
    "mask_pred = np.swapaxes( x_train_mask__[0], 0,1)\n",
    "# y_ppp = y_pred[ np.nonzero(mask_pred) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 777)\n",
      "(32, 777)\n"
     ]
    }
   ],
   "source": [
    "print y_pred.shape\n",
    "print mask_pred.shape\n",
    "# np.nonzero(mask_pred)[0]\n",
    "# x_train_mask__[0].shape\n",
    "# y_ppp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = y_pred[0]\n",
    "mask_pred = mask_pred[0]\n",
    "# y_pred[ np.nonzero(mask_pred) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(619,)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[ np.nonzero(mask_pred) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36,\n",
       " 28,\n",
       " 24,\n",
       " 36,\n",
       " 11,\n",
       " 36,\n",
       " 36,\n",
       " 1,\n",
       " 27,\n",
       " 36,\n",
       " 44,\n",
       " 6,\n",
       " 36,\n",
       " 1,\n",
       " 19,\n",
       " 9,\n",
       " 36,\n",
       " 34,\n",
       " 36,\n",
       " 44,\n",
       " 27,\n",
       " 13,\n",
       " 36,\n",
       " 25,\n",
       " 36,\n",
       " 21,\n",
       " 23,\n",
       " 42,\n",
       " 10,\n",
       " 36,\n",
       " 35,\n",
       " 36,\n",
       " 44,\n",
       " 18,\n",
       " 12,\n",
       " 36,\n",
       " 5,\n",
       " 43,\n",
       " 36,\n",
       " 36,\n",
       " 39,\n",
       " 36,\n",
       " 11,\n",
       " 5,\n",
       " 9,\n",
       " 26,\n",
       " 36,\n",
       " 11,\n",
       " 23,\n",
       " 9,\n",
       " 26,\n",
       " 36,\n",
       " 11,\n",
       " 5,\n",
       " 9,\n",
       " 33,\n",
       " 36,\n",
       " 21]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_up_phone( y_ppp )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
