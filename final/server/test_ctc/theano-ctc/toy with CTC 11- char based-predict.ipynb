{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This version\n",
    "\n",
    "Trying to implement a version about CTC with 3 hidden layer and combine with RNN\n",
    "\n",
    "***Summary***\n",
    "1. Try Deep-LSTM by lasagne\n",
    "- Try dropout layer\n",
    "- [CTC Problem] prefix search decoding? -> blank probability threshold set at 99.99% at paper\n",
    "- Try phoneme base instead of char base\n",
    "\n",
    "***Todo***\n",
    "1. ~~data normalize~~\n",
    "2. CTC correct?\n",
    "- Try phoneme but not character based\n",
    "1. ~~Using oop module~~\n",
    "- ~~Using CTC with log domain and recurrent relationship~~\n",
    "- ~~Add hidden layer before RNN~~\n",
    "- ~~Compile and update and learning~~\n",
    "- ~~func check_label_error~~\n",
    "- ~~data processing to char based~~\n",
    "    - ~~preprocess of data~~\n",
    "    - ~~remove_blank(predict)~~\n",
    "    - ~~remap_back( predict )~~\n",
    "    - ~~change to tri MFCC only~~\n",
    "- ~~Try CTC with character based data (data preprocess) (our data)~~\n",
    "- ~~Check CTC cost correctness~~\n",
    "- ~~Check CTC with y need to unmap or not~~\n",
    "- ~~Add Nestrove momentum~~\n",
    "- ~~Add minibatch~~\n",
    "- ~~Add bidirectional LSTM/RNN~~\n",
    "\n",
    "***Further***\n",
    "1. ~~Add Bidirectional LSTM/RNN~~\n",
    "2. ~~Make mini batch~~\n",
    "- Try phoneme based model\n",
    "\n",
    "***Problem need to understand about RNN***\n",
    "1. [Phd thesis Training RNN , Hessian free method](http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf)\n",
    "- [Does Theano do automatic unfolding for BPTT?](http://stackoverflow.com/questions/24431621/does-theano-do-automatic-unfolding-for-bptt)\n",
    "- [Github, General purpose Hessian-free optimization in Theano](https://github.com/boulanni/theano-hf)\n",
    "- [Theano scan](http://deeplearning.net/software/theano/library/scan.html#theano.scan)\n",
    "- []()\n",
    "\n",
    "***Problem need to understand about CTC***\n",
    "1. [CTC paper](ftp://ftp.idsia.ch/pub/juergen/icml2006.pdf)\n",
    "\n",
    "***Important References***\n",
    "1. [CTC cost use in our version](https://github.com/mohammadpz/CTC-Connectionist-Temporal-Classification/blob/5b4d4be19805d7795a4293b9c270ef7bf5fafbfc/ctc_cost.py)\n",
    "    - [Discussion](https://github.com/mohammadpz/CTC-Connectionist-Temporal-Classification/issues/1)\n",
    "- How to connect multiple recurrent layers,https://github.com/craffel/nntools/issues/9\n",
    "- https://github.com/craffel/nntools/issues/11\n",
    "- refactor recurrent, update examples, add tests, https://github.com/craffel/nntools/pull/27\n",
    "- example of deep lstm and penn tree,https://github.com/skaae/nntools/blob/combine/examples/lstm.py\n",
    "\n",
    "***Github***\n",
    "1. [Hessian theano](https://github.com/boulanni/theano-hf/blob/master/hf_examples.py)\n",
    "- [Vanilla RNN](https://github.com/mohammadpz/Recurrent-Neural-Networks)\n",
    "- [blocks example, reverse words](https://github.com/mila-udem/blocks-examples/tree/master/reverse_words)\n",
    "- [rnn-ctc, basics ctc](https://github.com/rakeshvar/rnn_ctc/blob/master/ctc.py)\n",
    "- [Lasagne nntools](https://github.com/craffel/nntools/tree/recurrent)\n",
    "- [Theanets, RNN tools](https://github.com/lmjohns3/theanets/blob/master/theanets/layers/recurrent.py)\n",
    "- [LSTM benchmarks example](https://github.com/craffel/lstm_benchmarks/blob/master/lasagne/experiment.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../../../nntools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980\n",
      "/home/pika/nntools/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.\n",
      "  warnings.warn(\"The uniform initializer no longer uses Glorot et al.'s \"\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "from theano_toolkit import utils as U\n",
    "from theano_toolkit import updates\n",
    "from theano.printing import Print\n",
    "from theano_toolkit.parameters import Parameters\n",
    "\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "# from lasagne.layers import RecurrentLayer, InputLayer, DenseLayer,\\\n",
    "#     NonlinearityLayer, ReshapeLayer, EmbeddingLayer\n",
    "# from lasagne.layers.recurrent import *\n",
    "\n",
    "from time import time\n",
    "\n",
    "import ctc_cost_2\n",
    "\n",
    "import cPickle\n",
    "import sys\n",
    "sys.setrecursionlimit(100000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data part\n",
    "\n",
    "## decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "credit by NLTK package of measure edit distance\n",
    "'''\n",
    "\n",
    "def _edit_dist_init(len1, len2):\n",
    "    lev = []\n",
    "    for i in range(len1):\n",
    "        lev.append([0] * len2)  # initialize 2D array to zero\n",
    "    for i in range(len1):\n",
    "        lev[i][0] = i           # column 0: 0,1,2,3,4,...\n",
    "    for j in range(len2):\n",
    "        lev[0][j] = j           # row 0: 0,1,2,3,4,...\n",
    "    return lev\n",
    "\n",
    "\n",
    "def _edit_dist_step(lev, i, j, s1, s2, transpositions=False):\n",
    "    c1 = s1[i - 1]\n",
    "    c2 = s2[j - 1]\n",
    "\n",
    "    # skipping a character in s1\n",
    "    a = lev[i - 1][j] + 1\n",
    "    # skipping a character in s2\n",
    "    b = lev[i][j - 1] + 1\n",
    "    # substitution\n",
    "    c = lev[i - 1][j - 1] + (c1 != c2)\n",
    "\n",
    "    # transposition\n",
    "    d = c + 1  # never picked by default\n",
    "    if transpositions and i > 1 and j > 1:\n",
    "        if s1[i - 2] == c2 and s2[j - 2] == c1:\n",
    "            d = lev[i - 2][j - 2] + 1\n",
    "\n",
    "    # pick the cheapest\n",
    "    lev[i][j] = min(a, b, c, d)\n",
    "\n",
    "\n",
    "def check_label_error( real , predict, transpositions=False):\n",
    "    ## length of real >= length of predict\n",
    "    \"\"\"\n",
    "    Calculate the Levenshtein edit-distance between two strings.\n",
    "    The edit distance is the number of characters that need to be\n",
    "    substituted, inserted, or deleted, to transform s1 into s2.  For\n",
    "    example, transforming \"rain\" to \"shine\" requires three steps,\n",
    "    consisting of two substitutions and one insertion:\n",
    "    \"rain\" -> \"sain\" -> \"shin\" -> \"shine\".  These operations could have\n",
    "    been done in other orders, but at least three steps are needed.\n",
    "\n",
    "    This also optionally allows transposition edits (e.g., \"ab\" -> \"ba\"),\n",
    "    though this is disabled by default.\n",
    "\n",
    "    :param s1, s2: The strings to be analysed\n",
    "    :param transpositions: Whether to allow transposition edits\n",
    "    :type s1: str\n",
    "    :type s2: str\n",
    "    :type transpositions: bool\n",
    "    :rtype int\n",
    "    \"\"\"\n",
    "    # set up a 2-D array\n",
    "    len1 = len(predict)\n",
    "    len2 = len(real)\n",
    "    lev = _edit_dist_init(len1 + 1, len2 + 1)\n",
    "\n",
    "    # iterate over the array\n",
    "    for i in range(len1):\n",
    "        for j in range(len2):\n",
    "            _edit_dist_step(lev, i + 1, j + 1, predict, real, transpositions=transpositions)\n",
    "            \n",
    "    return lev[len1][len2]*1.0/len2\n",
    "\n",
    "\n",
    "def clean_up( y ):\n",
    "    \"\"\"\n",
    "    for final output clean up\n",
    "    B(a − ab−) = B(−aa − −abb) = aab\n",
    "    \"\"\"\n",
    "    answer = \"\"\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == '-':\n",
    "            continue\n",
    "        else:\n",
    "            if y[i-1] != y[i]:\n",
    "                answer += y[i]\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def remap_back( y , actual):\n",
    "    answer = \"\"\n",
    "    \n",
    "    for i in y:\n",
    "        answer += char_unmap[i]\n",
    "    \n",
    "    if not actual:\n",
    "        answer = clean_up(answer)\n",
    "        \n",
    "    return answer\n",
    "\n",
    "def decode_all_actual( y, y_mask ,batch_size , actual=False):\n",
    "    \"\"\"\n",
    "    y     : label_length X batch_size\n",
    "    y_mask: label_length X batch_size\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in xrange(batch_size):\n",
    "        mask = np.swapaxes( y_mask, 0, 1)[i]\n",
    "        ans = np.swapaxes( y, 0 , 1)[i]\n",
    "        result.append(remap_back( ans[np.nonzero(mask)] , actual ))\n",
    "    return result\n",
    "\n",
    "\n",
    "def decode_all_pred( y, y_mask ,batch_size , actual=False):\n",
    "    \"\"\"\n",
    "    y     : label_length X batch_size\n",
    "    y_mask: label_length X batch_size\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in xrange(batch_size):\n",
    "        mask = np.swapaxes( y_mask, 0, 1)[i]\n",
    "        ans = y[i]\n",
    "        result.append(remap_back( ans[np.nonzero(mask)] , actual ))\n",
    "    return result\n",
    "\n",
    "def check_all( y, y_mask, y_pred, y_pred_mask, batch_size ):\n",
    "    \n",
    "    actual = decode_all_actual( y, y_mask, batch_size , True )\n",
    "    predict = decode_all_pred( y_pred, y_pred_mask, batch_size , False )\n",
    "    \n",
    "    error = 0.\n",
    "    for a,b in zip (actual, predict):\n",
    "        error += check_label_error(a,b)\n",
    "    \n",
    "    return error/len(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def relu(X):\n",
    "#     # return T.maximum(X, 0.)\n",
    "# #     return lambda x: x * (x > 0)\n",
    "#     return T.minimum(X * (X > 0), 20)\n",
    "\n",
    "def relu(x):\n",
    "    return x * (T.lt(x, 1e-6) and T.gt(x, 20))\n",
    "\n",
    "def build_shared_zeros(shape):\n",
    "    \"\"\" Builds a theano shared variable filled with a zeros numpy array \"\"\"\n",
    "    return theano.shared(value=np.zeros(shape, dtype=theano.config.floatX),\n",
    "             borrow=True)\n",
    "\n",
    "def dropout(random_state, X, keep_prob=0.5):\n",
    "    if keep_prob > 0. and keep_prob < 1.:\n",
    "        seed = random_state.randint(2 ** 30)\n",
    "        srng = RandomStreams(seed)\n",
    "        mask = srng.binomial(n=1, p=keep_prob, size=X.shape,\n",
    "                             dtype=theano.config.floatX)\n",
    "        return X * mask\n",
    "    return X\n",
    "\n",
    "\n",
    "def fast_dropout(random_state, X):\n",
    "    seed = random_state.randint(2 ** 30)\n",
    "    srng = RandomStreams(seed)\n",
    "    mask = srng.normal(size=X.shape, avg=1., dtype=theano.config.floatX)\n",
    "    return X * mask\n",
    "\n",
    "\n",
    "# momentum method\n",
    "def momentum(loss, all_params, update_momentum=0.9, update_learning_rate=0.1, nesterov=True):\n",
    "    all_grads = theano.grad(loss, all_params)\n",
    "    updates = []\n",
    "\n",
    "    for param_i, grad_i in zip(all_params, all_grads):\n",
    "        mparam_i = theano.shared(np.zeros(param_i.get_value().shape,\n",
    "                                          dtype=theano.config.floatX),\n",
    "                                 broadcastable=param_i.broadcastable)\n",
    "        v = update_momentum * mparam_i - update_learning_rate * grad_i\n",
    "        updates.append((mparam_i, v))\n",
    "        \n",
    "        if nesterov:\n",
    "            new_p = param_i + update_momentum * v - update_learning_rate* grad_i\n",
    "        else:\n",
    "            new_p = param_i + v\n",
    "            \n",
    "        updates.append((param_i, new_p ))\n",
    "\n",
    "    return updates\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_rnn_combine( X, num_batch, W_input_hidden1, b_hidden1, \n",
    "                       W_hidden1_hidden2, b_hidden2, \n",
    "                       W_hidden2_hidden3, b_hidden3,\n",
    "                       W_hidden3_rnnhidden, \n",
    "                       W_rnnhidden_rnnhidden, b_rnnhidden,\n",
    "                       W_rnnhidden_output, i_rnnhidden, b_output ):\n",
    "\n",
    "    def step(input_curr,hidden_prev):\n",
    "        \n",
    "        # (batch X 117) dot (117 X hidden1)\n",
    "        hidden1 = relu( T.dot( input_curr , W_input_hidden1) + b_hidden1 )\n",
    "        # (batch X hidden1) dot ( hidden1 X hidden2 )\n",
    "        hidden2 = relu( T.dot( hidden1 , W_hidden1_hidden2) + b_hidden2 )\n",
    "        # (batch X hidden2) dot ( hidden2 X hidden3 )\n",
    "        hidden3 = relu( T.dot( hidden2 , W_hidden2_hidden3) + b_hidden3 )\n",
    "        \n",
    "        # (batch X hidden3) dot ( hidden3 X rnnhidden1 ) + \n",
    "        # (batch X rnnhidden1) dot ( rnnhidden1 X rnnhidden1 ) \n",
    "        hidden_rnn = relu( T.dot(hidden3, W_hidden3_rnnhidden ) + \n",
    "                      T.dot( hidden_prev, W_rnnhidden_rnnhidden ) +\n",
    "                      b_rnnhidden )\n",
    "        \n",
    "        # (batch X rnnhidden1) dot ( rnnhidden1 X ( num_class+1) ) \n",
    "        y_pred = T.dot( hidden_rnn, W_rnnhidden_output ) + b_output\n",
    "        \n",
    "        return hidden_rnn, y_pred\n",
    "        \n",
    "    # Iteration over the first dimension of a tensor which is TIME in our case\n",
    "    # recurrent_fn doesn't use y in the computations, so we do not need y0 (None)\n",
    "    # scan returns updates too which we do not need. (_)\n",
    "    [hidden, y_pred] , _ = theano.scan(\n",
    "        step,\n",
    "        sequences = X,\n",
    "        outputs_info = [i_rnnhidden, None],\n",
    "        truncate_gradient=5\n",
    "    )\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "class end_to_end():\n",
    "    \n",
    "    def __init__(self, input_shape, max_seq_length, hidden_layer, rnn_hidden_layer,\n",
    "                 batch, max_epochs, eval_size, output_num_units,\n",
    "                 patience, up_learning_rate, up_momentum, file_name):\n",
    "        \n",
    "        self.input_shape = input_shape # [batch, dim]\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.hidden_layer = hidden_layer # hidden [l1, l2, l3]\n",
    "        self.rnn_hidden_layer = rnn_hidden_layer # rnn hidden [l1]\n",
    "        self.output_num_units = output_num_units # [ # of class ]\n",
    "        \n",
    "        self.batch = batch\n",
    "        self.max_epochs = max_epochs\n",
    "        self.eval_size = eval_size\n",
    "        \n",
    "        self.up_learning_rate = up_learning_rate\n",
    "        self.up_momentum = up_momentum\n",
    "                \n",
    "        self.patience = patience\n",
    "        self.best_valid = np.inf\n",
    "        self.best_valid_epoch = 0\n",
    "        self.best_params = None\n",
    "        \n",
    "        self.train_history_ = []\n",
    "        self.epochs = 0\n",
    "        \n",
    "        self.file_name = file_name\n",
    "        \n",
    "        self.drop_frac = 0.2\n",
    "#         X = T.matrix('X')\n",
    "#         Y = T.ivector('Y')     \n",
    "        \n",
    "        \"\"\"\n",
    "        input data type\n",
    "        y_hat : T x B x C+1\n",
    "        y : L x B\n",
    "        y_hat_mask : T x B\n",
    "        y_mask : L x B\n",
    "        \"\"\"\n",
    "        \n",
    "        # T x B x F\n",
    "        # B X T X F (Lasagne format)\n",
    "        x = T.tensor3('X', dtype=theano.config.floatX)\n",
    "        # L x B\n",
    "        y = T.matrix('y', dtype=theano.config.floatX)\n",
    "\n",
    "        # L x B\n",
    "        y_mask = T.matrix('y_mask', dtype=theano.config.floatX)\n",
    "        # T x B\n",
    "        y_hat_mask = T.matrix('y_hat_mask', dtype=theano.config.floatX)\n",
    "    \n",
    "        # Min/max sequence length\n",
    "#         MIN_LENGTH = 50\n",
    "        MAX_LENGTH = max_seq_length\n",
    "        # Number of units in the hidden (recurrent) layer\n",
    "        N_HIDDEN = hidden_layer[0]\n",
    "        # Number of training sequences in each batch\n",
    "        N_BATCH = batch\n",
    "        # Optimization learning rate\n",
    "        GRAD_CLIP = 100\n",
    "        # How often should we check the output?\n",
    "        EPOCH_SIZE = 100\n",
    "        # Number of epochs to train the net\n",
    "        NUM_EPOCHS = 10\n",
    "        \n",
    "        #===========================================================================================\n",
    "        # Bi RNN\n",
    "        #===========================================================================================\n",
    "\n",
    "        # Recurrent layers expect input of shape\n",
    "        # (batch size, max sequence length, number of features)\n",
    "        # T x B x F\n",
    "#         l_in = lasagne.layers.InputLayer(shape=( N_BATCH, MAX_LENGTH,  self.input_shape[1]))\n",
    "        \n",
    "        # We're using a bidirectional network, which means we will combine two\n",
    "        # RecurrentLayers, one with the backwards=True keyword argument.\n",
    "        # Setting a value for grad_clipping will clip the gradients in the layer\n",
    "#         l_forward = lasagne.layers.RecurrentLayer(l_in, N_HIDDEN, grad_clipping=GRAD_CLIP,\n",
    "#                                                   W_in_to_hid=lasagne.init.HeUniform(),\n",
    "#                                                   W_hid_to_hid=lasagne.init.HeUniform(),\n",
    "#                                                   nonlinearity=lasagne.nonlinearities.tanh)\n",
    "#         l_backward = lasagne.layers.RecurrentLayer(l_in, N_HIDDEN, grad_clipping=GRAD_CLIP,\n",
    "#                                                    W_in_to_hid=lasagne.init.HeUniform(),\n",
    "#                                                    W_hid_to_hid=lasagne.init.HeUniform(),\n",
    "#                                                    nonlinearity=lasagne.nonlinearities.tanh, \n",
    "#                                                    backwards=True)\n",
    "        # We'll use an elementwise sum to combine the forward/backward layers\n",
    "#         l_sum = lasagne.layers.ElemwiseSumLayer([l_forward, l_backward])\n",
    "        \n",
    "        # We need a reshape layer which combines the first (batch size) and second\n",
    "        # (number of timesteps) dimensions, otherwise the DenseLayer will treat the\n",
    "        # number of time steps as a feature dimension\n",
    "#         l_reshape = lasagne.layers.ReshapeLayer( l_sum, (N_BATCH*MAX_LENGTH, N_HIDDEN))\n",
    "        \n",
    "        #===========================================================================================\n",
    "        # dEEP lSTM\n",
    "        #===========================================================================================\n",
    "        \n",
    "        # Recurrent layers expect input of shape\n",
    "        # (batch size, max sequence length, number of features)\n",
    "        l_in = lasagne.layers.InputLayer(shape=(N_BATCH, MAX_LENGTH,  self.input_shape[1]))\n",
    "        \n",
    "        l_in_gau = lasagne.layers.GaussianNoiseLayer(l_in, sigma=0.5 )\n",
    "        \n",
    "        # LSTM layer 1\n",
    "        l_forward_1 = lasagne.layers.LSTMLayer(l_in_gau, num_units=hidden_layer[0], \n",
    "                                               learn_init=True, peepholes=True)\n",
    "        l_backward_1 = lasagne.layers.LSTMLayer(l_in, num_units=hidden_layer[0],\n",
    "                                                backwards=True, learn_init=True, peepholes=True)\n",
    "        l_recurrent_1 = ElemwiseSumLayer([l_forward_1, l_backward_1])\n",
    "  \n",
    "        # concatenate forward and backward LSTM layers\n",
    "#         l_fwd_reshape_1 = lasagne.layers.ReshapeLayer(l_forward_1, (N_BATCH*MAX_LENGTH, hidden_layer[0]))\n",
    "#         l_bck_reshape_1 = lasagne.layers.ReshapeLayer(l_backward_1, (N_BATCH*MAX_LENGTH, hidden_layer[0]))\n",
    "#         l_recurrent_1 = lasagne.layers.ConcatLayer([l_fwd_reshape_1, l_bck_reshape_1], axis=1)\n",
    "        \n",
    "#         if self.drop_frac > 0:\n",
    "#             l_drp_1 = lasagne.layers.DropoutLayer(l_recurrent_1, p = 0.2 )\n",
    "#         else:\n",
    "#             l_drp_1 = l_recurrent_1\n",
    "\n",
    "        # LSTM layer 2\n",
    "        l_forward_2 = lasagne.layers.LSTMLayer(l_recurrent_1, num_units=hidden_layer[1], \n",
    "                                               learn_init=True, peepholes=True)\n",
    "        l_backward_2 = lasagne.layers.LSTMLayer(l_recurrent_1, num_units=hidden_layer[1],\n",
    "                                                backwards=True, learn_init=True, peepholes=True)\n",
    "        l_recurrent_2 = ElemwiseSumLayer([l_forward_2, l_backward_2])\n",
    "        \n",
    "#         if self.drop_frac > 0:\n",
    "#             l_drp_2 = lasagne.layers.DropoutLayer(l_recurrent_2, p = 0.2 )\n",
    "#         else:\n",
    "#             l_drp_2 = l_recurrent_2\n",
    "        \n",
    "        # LSTM layer 3\n",
    "        l_forward_3 = lasagne.layers.LSTMLayer(l_recurrent_2, num_units=hidden_layer[2], \n",
    "                                               learn_init=True, peepholes=True)\n",
    "        l_backward_3 = lasagne.layers.LSTMLayer(l_recurrent_2, num_units=hidden_layer[2],\n",
    "                                                backwards=True, learn_init=True, peepholes=True)\n",
    "        l_recurrent_3 = ElemwiseSumLayer( [l_forward_3, l_backward_3] )\n",
    "        \n",
    "#         if self.drop_frac > 0:\n",
    "#             l_drp_3 = lasagne.layers.DropoutLayer(l_recurrent_3, p = 0.2 )\n",
    "#         else:\n",
    "#             l_drp_3 = l_recurrent_3\n",
    "\n",
    "        \n",
    "        # LSTM layer 4\n",
    "        l_forward_4 = lasagne.layers.LSTMLayer(l_recurrent_3, num_units=hidden_layer[3], \n",
    "                                               learn_init=True, peepholes=True)\n",
    "        l_backward_4 = lasagne.layers.LSTMLayer(l_recurrent_3, num_units=hidden_layer[3],\n",
    "                                                backwards=True, learn_init=True, peepholes=True)\n",
    "        l_recurrent_4 = ElemwiseSumLayer( [l_forward_4, l_backward_4] )\n",
    "\n",
    "#         if self.drop_frac > 0:\n",
    "#             l_drp_4 = lasagne.layers.DropoutLayer(l_recurrent_4, p = 0.2 )\n",
    "#         else:\n",
    "#             l_drp_4 = l_recurrent_4\n",
    "            \n",
    "        # LSTM layer 5\n",
    "#         l_forward_5 = lasagne.layers.LSTMLayer(l_drp_4, num_units=hidden_layer[4], \n",
    "#                                                learn_init=True, peepholes=True)\n",
    "#         l_backward_5 = lasagne.layers.LSTMLayer(l_drp_4, num_units=hidden_layer[4],\n",
    "#                                                 backwards=True, learn_init=True, peepholes=True)\n",
    "#         l_recurrent_5 = ElemwiseSumLayer( [l_forward_5, l_backward_5] )\n",
    "\n",
    "#         if self.drop_frac > 0:\n",
    "#             l_drp_5 = lasagne.layers.DropoutLayer(l_recurrent_5, p = 0.2 )\n",
    "#         else:\n",
    "#             l_drp_5 = l_recurrent_5\n",
    "\n",
    "\n",
    "        \n",
    "        l_reshape = lasagne.layers.ReshapeLayer(l_recurrent_4,\n",
    "                                               (N_BATCH*MAX_LENGTH, hidden_layer[3]))\n",
    "\n",
    "        \n",
    "        #===========================================================================================\n",
    "        # COMMON SETUP\n",
    "        #===========================================================================================\n",
    "        \n",
    "        # Our output layer is a simple dense connection\n",
    "        l_recurrent_out = lasagne.layers.DenseLayer( l_reshape, num_units= output_num_units[0] ,\n",
    "                                                    nonlinearity=lasagne.nonlinearities.identity)\n",
    "        \n",
    "        # Now, reshape the output back to the RNN format\n",
    "        l_out_shp = lasagne.layers.ReshapeLayer(l_recurrent_out, (N_BATCH, MAX_LENGTH, output_num_units[0] ))\n",
    "        \n",
    "        # dimshuffle to shape format (input_seq_len, batch_size, num_classes + 1)\n",
    "        l_out_shp_ctc = lasagne.layers.DimshuffleLayer(l_out_shp, (1, 0, 2))\n",
    "\n",
    "        l_out_softmax = lasagne.layers.NonlinearityLayer(\n",
    "                            l_recurrent_out, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "        l_out_softmax_shp = lasagne.layers.ReshapeLayer(\n",
    "                            l_out_softmax, (N_BATCH, MAX_LENGTH, output_num_units[0] ))\n",
    "        \n",
    "        # since we use gaussian noise in input, False means use noise, True means dont use noise\n",
    "        output_lin_ctc_train = lasagne.layers.get_output(l_out_shp_ctc, x, deterministic=False)\n",
    "        output_softmax_train = lasagne.layers.get_output(l_out_softmax_shp, x, deterministic=False)\n",
    "        \n",
    "        output_lin_ctc_val = lasagne.layers.get_output(l_out_shp_ctc, x, deterministic=True)\n",
    "        output_softmax_val = lasagne.layers.get_output(l_out_softmax_shp, x, deterministic=True)\n",
    "        \n",
    "        self.all_params = lasagne.layers.get_all_params(l_out_shp)\n",
    "\n",
    "        # the CTC cross entropy between y and linear output network\n",
    "        pseudo_cost = ctc_cost_2.pseudo_cost(\n",
    "            y, output_lin_ctc_train, y_mask, y_hat_mask,\n",
    "            skip_softmax=True)\n",
    "        \n",
    "        \n",
    "        pseudo_cost_grad = T.grad(pseudo_cost.mean(), self.all_params)\n",
    "        true_cost = ctc_cost_2.cost(y, output_softmax_train.dimshuffle(1, 0, 2), y_mask, y_hat_mask)\n",
    "        cost = T.mean(true_cost)\n",
    "        updates = lasagne.updates.rmsprop(pseudo_cost_grad, self.all_params, learning_rate = self.up_learning_rate)\n",
    "\n",
    "        self.train = theano.function(\n",
    "            inputs = [x, y, y_hat_mask, y_mask],\n",
    "            outputs = [ pseudo_cost.mean(), cost, output_softmax_train ],\n",
    "#             outputs = [output_lin_ctc, output_softmax, cost],\n",
    "            updates=updates\n",
    "        )\n",
    "        \n",
    "        self.predict = theano.function( \n",
    "            inputs=[x], \n",
    "            outputs = [ output_softmax_val] \n",
    "        )\n",
    "\n",
    "        \n",
    "    # x_mask and y_mask is the same\n",
    "    # x_test_mask and y_test_mask is the same\n",
    "    def fit(self, x_train, y_train, x_test,  y_test , x_mask, y_mask, x_test_mask, y_test_mask ):\n",
    "        print \" \"\n",
    "        print \"start training!!!!\"\n",
    "        print \" \"\n",
    "        \n",
    "        for i in range(self.max_epochs):\n",
    "            self.epochs +=1\n",
    "            t0 = time()\n",
    "            \n",
    "            cs = 0.\n",
    "            pseudo_cs = 0.\n",
    "            for index in range(len(x_train)):\n",
    "                pseudo, cost, output_softmax = self.train( x_train[index] , y_train[index],\n",
    "                                                          x_mask[index], y_mask[index])\n",
    "                cs += cost\n",
    "                pseudo_cs += pseudo\n",
    "                gg = index\n",
    "#                 print pseudo, cost\n",
    "                \n",
    "            cs /= len(x_train)\n",
    "            pseudo_cs /= len(x_train)\n",
    "            \n",
    "#             cPickle.dump( self , open(\"./\"+self.file_name+\".pkl\",\"wb\"))\n",
    "            if self.epochs <= 1:\n",
    "                previous_cs = \"-\"\n",
    "            else:\n",
    "                previous_cs = self.train_history_[-1]['cost']\n",
    "            print \"\\n===============================\"\n",
    "            print 'epoch {0} : pseudo= {1}, cost= {2}, previous_cost= {3}, train_time = {4} s'.format(self.epochs,\n",
    "                                                                                                      pseudo_cs, \n",
    "                                                                                                      cs,\n",
    "                                                                                                      previous_cs, \n",
    "                                                                                                      time() - t0)\n",
    "            \n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            t0 = time()\n",
    "            \n",
    "#             print np.argmax(output_softmax[:],axis=2)\n",
    "#             print decode_all( np.argmax(output_softmax[:],axis=2) , x_train_mask[gg],  batch_size , True)\n",
    "            \n",
    "            # save model first\n",
    "            # dont know the LER performance yet\n",
    "#             cPickle.dump( self , open(\"./\"+self.file_name+\".pkl\",\"wb\"))\n",
    "    \n",
    "\n",
    "            label_error_rate = 0.\n",
    "            for index in range( len(x_test) ) :\n",
    "\n",
    "                prepre = self.predict( x_test[index])\n",
    "                \n",
    "                label_error_rate += check_all( y_test[index], y_test_mask[index],\n",
    "                                              np.argmax(prepre[0],axis=2) , x_test_mask[index],\n",
    "                                              batch_size)\n",
    "                \n",
    "                if (( index + self.epochs ) % 4 == 0 ):\n",
    "                    ## print actual\n",
    "                    y_actual = decode_all_actual( y_test[index], y_test_mask[index], batch_size , True)\n",
    "\n",
    "                    ## print pred\n",
    "                    y_predict = decode_all_pred( np.argmax(prepre[0],axis=2) , x_test_mask[index],  batch_size , False)\n",
    "                \n",
    "            label_error_rate /= len(x_test)\n",
    "            self.train_history_.append({\"epoch\":self.epochs, \"cost\": cs, \"LER\":label_error_rate})\n",
    "            \n",
    "            print '\\t\\t\\t val_= {0}, test_time  = {1} s'.format(label_error_rate, time() - t0)\n",
    "            print \"===============================\\n\"\n",
    "            \n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            for a , b in zip (y_actual,y_predict):\n",
    "                print \"Target== \",a \n",
    "                print \"\\tAns => \",b\n",
    "            \n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            \"\"\"\n",
    "            should use cost do early stopping\n",
    "            \"\"\"\n",
    "            current_cs = self.train_history_[-1]['cost']\n",
    "            current_epoch = self.train_history_[-1]['epoch']\n",
    "            if current_cs < self.best_valid:\n",
    "                self.best_valid = current_cs\n",
    "                self.best_valid_epoch = current_epoch\n",
    "                self.best_params = [w.get_value() for w in self.all_params]\n",
    "                cPickle.dump( self , open(\"./\"+self.file_name+\".pkl\",\"wb\"))\n",
    "                \n",
    "            elif self.best_valid_epoch + self.patience <= current_epoch:\n",
    "                print \"\"\n",
    "                print \"Early stopping.\"\n",
    "                print self.best_valid_epoch,self.best_valid\n",
    "                print \"Best valid ler {:.6f} at epoch {}.\".format(self.best_valid, self.best_valid_epoch)              \n",
    "#                 for qq in range (len(self.all_params)):\n",
    "#                     self.all_params[qq].set_value( self.best_params[qq] )\n",
    "#                 break\n",
    "\n",
    "            \"\"\"\n",
    "            can not use label error rate do early stopping\n",
    "            \"\"\"\n",
    "#             current_ler = self.train_history_[-1]['LER']\n",
    "#             current_epoch = self.train_history_[-1]['epoch']\n",
    "#             if current_ler < self.best_valid:\n",
    "# #                 print \"********************* Now best ************************\"\n",
    "#                 sys.stdout.flush()\n",
    "#                 self.best_valid = current_ler\n",
    "#                 self.best_valid_epoch = current_epoch\n",
    "#                 self.best_params = [w.get_value() for w in self.all_params]\n",
    "# #                 cPickle.dump( self , open(\"./\"+self.file_name+\".pkl\",\"wb\"))\n",
    "                \n",
    "#             elif self.best_valid_epoch + self.patience <= current_epoch:\n",
    "#                 print \"\"\n",
    "#                 print \"Early stopping.\"\n",
    "#                 print self.best_valid_epoch,self.best_valid\n",
    "#                 print \"Best valid ler {:.6f} at epoch {}.\".format(self.best_valid, self.best_valid_epoch)\n",
    "#                 sys.stdout.flush()                \n",
    "#                 for qq in range (len(self.all_params)):\n",
    "#                     self.all_params[qq].set_value( self.best_params[qq] )\n",
    "#                 break\n",
    "\n",
    "\n",
    "#     def prediction(self, x, x_mask) :\n",
    "        \n",
    "#         abc =  self.predict(x, x_mask)\n",
    "        \n",
    "#         return np.argmax(abc[0], axis = 1 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pika/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:133: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n",
      "  from scan_perform.scan_perform import *\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# del model1\n",
    "\n",
    "with open('model_11_char_1.pkl', 'rb') as f:\n",
    "    model1 = cPickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#=========================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data part\n",
    "\n",
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DataProcessing \n",
    "\n",
    "'''\n",
    "[ INPUT  ] : model { wav, NFCC, ... }\n",
    "[ OUTPUT ] : (Training_data, Validation_data, Testing_Data)\n",
    "\n",
    "- Training_data : x, y { .astype(np.float32)\n",
    "- Validation_data : x, y { .astype(np.float32)\n",
    "- Testing_Data : ( x, id )\n",
    "\n",
    "-- x format : \n",
    "---- MFCC : [ 39-vector ] - \n",
    "---- FBank : [ 69-vector ]\n",
    "---- quadmfcc : [ 39*(1+1+1) - vector ] !!! Wron---- FBank : [ 69-vector ]\n",
    "---- quadmfcc : [ 39*(1+1+1) - vector ] !!! Wrong implementation\n",
    "g implementation\n",
    "---- hexamfcc : [ 39*(4+1+4) - vector ]\n",
    "---- hexaFbank : [ 69*(4+1+4) - vector ]\n",
    "\n",
    "-- y format :\n",
    "\n",
    "---- Now we use 39-phonemes for all : [ 0 0 0 ... 1 .... 0 0 0 ]  as a number of 39\n",
    "---- But answer we get from ./label is [ 48-vector]\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "input:\n",
    "    MFCC: 39,2-1-2\n",
    "output:\n",
    "    state label: 1943\n",
    "'''\n",
    "\n",
    "'''\n",
    "[ Input  ] :\n",
    "    - mfcc (39*(2+1+2))\n",
    "\n",
    "[ Output ] :\n",
    "    - maxi (1943-D)\n",
    "    \n",
    "'''\n",
    "\n",
    "from itertools import repeat\n",
    "import numpy as np\n",
    "import random\n",
    "# from sklearn import preprocessing\n",
    "\n",
    "# Parameter (note: switch /final/MLDS_Final_Data/ .. if u want\n",
    "# MAPPING_FILE = '/home/pika/MLDS_2015/final/MLDS_HW1_RELEASE_v1/phones/48_39.map'\n",
    "# MFCC_TRAINING_ARK = '/home/pika/MLDS_2015/final/MLDS_Final_Data/mfcc/train_nor.ark'\n",
    "\n",
    "MFCC_TEST_ARK = '/home/pika/MLDS_2015/final/MLDS_Final_Data/mfcc/test_nor.ark'\n",
    "\n",
    "\n",
    "# MAXI_TRAINING_LABEL = '/home/pika/MLDS_2015/final/MLDS_HW1_RELEASE_v1/state_label/train.lab'\n",
    "# NORMAL_TRAINING_LABEL = '/home/pika/MLDS_2015/final/MLDS_HW1_RELEASE_v1/label/train.lab'\n",
    "\n",
    "sentence_data = '/home/pika/MLDS_2015/final/MLDS_Final_Data/sentence/train_mod.set'\n",
    "\n",
    "# MFCC_TRAINING_SPEAKER_ARK = '/home/pika/MLDS_2015/final/MLDS_Final_Data/mfcc/train_nor_speaker.ark'\n",
    "# MFCC_VALID_SPEAKER_ARK = '/home/pika/MLDS_2015/final/MLDS_Final_Data/mfcc/valid_nor_speaker.ark'\n",
    "\n",
    "\n",
    "\n",
    "# Dataset \n",
    "def DNNDataset( inputMode ) :\n",
    "    trains = {}\n",
    "    valids = {}\n",
    "    test_data = {}\n",
    "    x_train = []\n",
    "    x_valid = []\n",
    "    y_train = []\n",
    "    y_valid = []\n",
    "    total_result = []\n",
    "    \n",
    "    slide_ration = 0.1\n",
    "    \n",
    "    sentense = {}\n",
    "    char_map = {}\n",
    "    char_unmap = {}\n",
    "    char_count = 0\n",
    "    \n",
    "    with open(sentence_data) as f:\n",
    "        for lines in f:\n",
    "            frames = lines.split(',',1)\n",
    "            frames[1] = frames[1].lower()\n",
    "            if len(frames[1])<2: # empty sentences\n",
    "                prob = frames[0]\n",
    "#                 print prob\n",
    "                continue\n",
    "            for cha in frames[1][:-2]: # skip \"\\n\" ending punctuation\n",
    "                if cha in ',.!?;:-\"':\n",
    "                    continue\n",
    "                if cha not in char_map:\n",
    "                    char_map[cha] = char_count\n",
    "                    char_unmap[char_count] = cha\n",
    "                    char_count += 1\n",
    "                    \n",
    "    char_map[\"-\"] = char_count # for blank, different with space\n",
    "    char_unmap[char_count] = \"-\"\n",
    "#     print char_map\n",
    "    \n",
    "    with open(sentence_data) as f:\n",
    "        for lines in f:\n",
    "            frames = lines.split(',',1)\n",
    "            frames[1] = frames[1].lower()\n",
    "            if len(frames[1])<2: # empty sentences\n",
    "                prob = frames[0]\n",
    "                continue\n",
    "            sentense[ frames[0] ] = []\n",
    "            for cha in frames[1][:-2]:\n",
    "                if cha in ',.!?;:\"':\n",
    "                    continue\n",
    "                if cha in '-':\n",
    "                    cha = \" \"\n",
    "#                 sentense[ frames[0] ].append( char_map['-'] ) # append blank before each label\n",
    "                sentense[ frames[0] ].append( char_map[cha] )\n",
    "#             sentense[ frames[0] ].append( char_map['-'] ) # append blank in the end\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # X(INPUT) ---------------------------------------\n",
    "    if inputMode == \"mfcc\":\n",
    "        \n",
    "        # speaker_window -(map)-> mfcc[39]\n",
    "        monoTrains = {}\n",
    "        with open(MFCC_TEST_ARK) as f:\n",
    "            for lines in f:\n",
    "                frames = lines.split(' ')\n",
    "                frame2float = [ float(x) for x in frames[1:] ]\n",
    "                monoTrains[frames[0]] = frame2float\n",
    "#                 break\n",
    "        # Initialize : L2 L1 S R1 R2\n",
    "        zeroFrame = list(repeat(0, 39))\n",
    "        prevframeL1 = [ float(x) for x in zeroFrame[:] ]\n",
    "#         prevframeL2 = [ float(x) for x in zeroFrame[:] ]\n",
    "        prevframeR1 = [ float(x) for x in zeroFrame[:] ]\n",
    "#         prevframeR2 = [ float(x) for x in zeroFrame[:] ]\n",
    "        blankframe  = [ float(x) for x in zeroFrame[:] ]\n",
    "        frameNameL1 = \"\"\n",
    "#         frameNameL2 = \"\"\n",
    "        frameNameR1 = \"\"\n",
    "#         frameNameR2 = \"\"\n",
    "\n",
    "    \n",
    "        for sample in monoTrains:\n",
    "            frames_info = sample.split('_')\n",
    "            speaker = frames_info[0] + \"_\" + frames_info[1]\n",
    "#             frameNameL2 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])-2))\n",
    "            frameNameL1 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])-1))\n",
    "            frameNameR1 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])+1))\n",
    "#             frameNameR2 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])+2))\n",
    "            \n",
    "#             print speaker\n",
    "#             print 1\n",
    "#             break\n",
    "            if trains.get(speaker) is None:\n",
    "                trains[speaker] = {}\n",
    "            if   frames_info[2] == \"1\":\n",
    "                trains[speaker][sample] = blankframe + monoTrains[sample] + monoTrains[frameNameR1]\n",
    "            elif monoTrains.get(frameNameR1) is None:\n",
    "                trains[speaker][sample] = monoTrains[frameNameL1] + monoTrains[sample] + blankframe\n",
    "            else:\n",
    "                trains[speaker][sample] = monoTrains[frameNameL1] + monoTrains[sample] + monoTrains[frameNameR1]  \n",
    "\n",
    "        monoTrains.clear()\n",
    "#         print len(trains)\n",
    "        for sample in trains:\n",
    "            \n",
    "            temp = []\n",
    "#             print sample\n",
    "#             break\n",
    "#             print len(trains[sample])\n",
    "            for i in range(len(trains[sample])):\n",
    "#                 print sample + \"_\" + str(i+1)\n",
    "                temp.append( ( trains[sample][sample + \"_\" + str(i+1)]) )\n",
    "#                 break\n",
    "\n",
    "            x_train.append( floatX(temp ))\n",
    "\n",
    "    trains.clear()\n",
    "        \n",
    "    return char_map, char_unmap, x_train\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def vectorized_result ( j , siz) :\n",
    "    e = np.zeros((siz, 1))\n",
    "    e[j] = 1.0\n",
    "    return np.reshape( e, siz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "char_map, char_unmap, x_train = DNNDataset(\"mfcc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 619, 117)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_batches_X(X, length, batch_size=30):\n",
    "    '''\n",
    "    https://github.com/craffel/lstm_benchmarks/blob/master/lasagne/experiment.py\n",
    "    \n",
    "    Convert a list of matrices into batches of uniform length\n",
    "    :parameters:\n",
    "        - X : list of np.ndarray\n",
    "            List of matrices\n",
    "        - length : int\n",
    "            Desired sequence length.  Smaller sequences will be padded with 0s,\n",
    "            longer will be truncated.\n",
    "        - batch_size : int\n",
    "            Mini-batch size\n",
    "    :returns:\n",
    "        - X_batch : np.ndarray\n",
    "            Tensor of time series matrix batches,\n",
    "            shape=(n_batches, length, batch_size, n_features)\n",
    "        - X_mask : np.ndarray\n",
    "            shape=(n_batches, length, batch_size)\n",
    "            Mask denoting whether to include each time step of each time series\n",
    "            matrix\n",
    "    '''\n",
    "#     limit = len(X)//batch_size\n",
    "    n_batches = int(round(len(x_train)*1./batch_size))\n",
    "    \n",
    "    # Lasagne format\n",
    "    X_batch = np.zeros((n_batches, batch_size, length, X[0].shape[1]),\n",
    "                       dtype=theano.config.floatX)\n",
    "    # Else format\n",
    "#     X_batch = np.zeros((n_batches, length, batch_size, X[0].shape[1]),\n",
    "#                        dtype=theano.config.floatX)\n",
    "    \n",
    "#     X_mask = np.zeros(X_batch.shape, dtype=np.bool)\n",
    "    \n",
    "    X_mask = np.zeros((n_batches, length, batch_size ), dtype=theano.config.floatX)\n",
    "#     X_mask = np.zeros((n_batches,  batch_size, length ), dtype=theano.config.floatX)\n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for b in range(n_batches): \n",
    "        for n in range(batch_size): # go thorough batch size\n",
    "            \n",
    "            if ( len(x_train) > (b*batch_size + n )):\n",
    "#                 print b*batch_size + n\n",
    "                count += 1\n",
    "                \n",
    "                X_m = X[b*batch_size + n] # seq_length X feature dim\n",
    "\n",
    "                X_batch[b, n, :X_m.shape[0]] = X_m[:length]\n",
    "    \n",
    "                X_mask[b, :X_m.shape[0], n] = 1\n",
    "    print count\n",
    "            \n",
    "    return X_batch, X_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(round(len(x_train)/40.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592\n"
     ]
    }
   ],
   "source": [
    "batch_size = 40\n",
    "\n",
    "# Find the longest sequence\n",
    "# length_x = (max([X.shape[0] for X in x_train]))\n",
    "\n",
    "# print length_x\n",
    "\n",
    "length_x = 777\n",
    "\n",
    "# Convert to batches of time series of uniform length\n",
    "# x_train_mask: seq_length X batch_size\n",
    "x_train__, x_train_mask__ = make_batches_X(x_train, length_x, batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 40, 777, 117)\n",
      "================mask================\n",
      "(15, 777, 40)\n"
     ]
    }
   ],
   "source": [
    "print x_train__.shape\n",
    "print \"================mask================\"\n",
    "print x_train_mask__.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abc = model1.predict(x_train__[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mecstheretion proocsesin ho  med as re se eex tencu in istigation',\n",
       " 'het dgra mak centso te ato much study',\n",
       " 'a ba gar o wen arng o gaus',\n",
       " 'al is  tei oaet yo wr wai thastiprvation tis ne worthe',\n",
       " 'wi yorwery over sily ayums',\n",
       " 'autr vat buy ns frs a cale tomtlo sy',\n",
       " 'ary lokin forponn',\n",
       " 'lerterast r fisix',\n",
       " 'chncrains shofersh freven',\n",
       " 'he gi ma shored  hard la in otet her noingly',\n",
       " 'shuy is is i ryot we olid nolei',\n",
       " 'wit a low wl',\n",
       " 'o wi sou s bit ar oi hyu rest mo saws',\n",
       " 'is cotia stin toun  chadur den is beutfal ben swer wr in shabey',\n",
       " 'dogs ted sumting to wons  e go',\n",
       " 'the tun onm cap dis hice o curid a tun plnt wrthreors',\n",
       " 'the cop oa cranges shampoitno r onho rd',\n",
       " 'the posingclo e seg eret',\n",
       " 'ol ley wloyr lo noaners',\n",
       " 'o ct is urean done',\n",
       " 'whebt aei wit youws hes mng',\n",
       " 'ei be go  iely aml tri the foar',\n",
       " 'om boing al eng greins in ta larg bu',\n",
       " 'wetisenle er shm',\n",
       " 'tde prosor tn staplers a har an oar pom ban',\n",
       " 'the bnal o was prousn mee ti ate mner te surr',\n",
       " 'el haitaripers nalrxpec e too ba gofve men orders',\n",
       " 'dra the supla thet a ha ban te no tans',\n",
       " 'eta spinting smily an retistri nornkx isly',\n",
       " 'thet tg ram exes aly ater mah saty',\n",
       " 'a ifor tms e se dhis cleclumen trlier en hes oroinnest',\n",
       " 'within lar tore sucl sesto arthe strton funge o supsastums',\n",
       " 'histe dim fonor bager',\n",
       " 'he ne i ted dristor an fr no wr',\n",
       " 'poch he pls an i sur tr tal menas tar noman cowil',\n",
       " 'lestpertion  sils meg',\n",
       " 'ra knoment et te bust tfor bisnis lrgus',\n",
       " 'cloh w su dhbute losuris  mu s',\n",
       " 'i wis exbos t ha losti the ges thet',\n",
       " 'donth go cholley sgait dishos']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_all_pred( np.argmax(abc[0],axis=2) , x_train_mask__[0],  batch_size , False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
