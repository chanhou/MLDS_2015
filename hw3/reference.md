# Efficient GPU-based Training of Recurrent Neural Network Language Models Using Spliced Sentence Bunch
http://mi.eng.cam.ac.uk/~xc257/papers/RNNLMTrain_Interspeech2014.pdf

# SCALING RECURRENT NEURAL NETWORK LANGUAGE MODELS
http://arxiv.org/pdf/1502.00512.pdf

# theano-rnn
https://github.com/gwtaylor/theano-rnn

# Large Scale Recurrent Neural Network on GPU
http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6889433

# Theano: new features and speed improvements
http://arxiv.org/pdf/1211.5590v1.pdf

# Great result: 
Learning word embeddings efficiently with noise-contrastive estimation
http://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf

# RnnLM thesis:
http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf

# A fast and simple algorithm for training neural probabilistic language models
some preprocessing technique
https://www.cs.toronto.edu/~amnih/papers/ncelm.pdf

# BILLION WORD IMPUTATION, April 19, 2015
Using word2vec become model
http://cse.iitk.ac.in/users/cs365/2015/_submissions/mudgal/report.pdf

# Regularization and nonlinearities for neural language models: when are they needed?
Add regularization term and achieve 55% result
http://www.gatsby.ucl.ac.uk/~marius/papers/1301.5650v2.pdf

# Recurrent Neural Networks for Language Understanding
http://research.microsoft.com/en-us/um/people/gzweig/Pubs/Interspeech2013RNNLU.pdf