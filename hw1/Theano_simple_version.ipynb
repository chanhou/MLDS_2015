{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano version of simple DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import cross_validation as cv\n",
    "from sklearn import metrics\n",
    "from sklearn import grid_search as gs\n",
    "from sklearn.metrics import accuracy_score\n",
    "# from sklearn.base import BaseEstimator\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### DataProcessing \n",
    "\n",
    "'''\n",
    "[ INPUT  ] : model { wav, NFCC, ... }\n",
    "[ OUTPUT ] : (Training_data, Validation_data, Testing_Data)\n",
    "\n",
    "- Training_data : ( x, y, id )\n",
    "- Validation_data : ( x, y, id )\n",
    "- Testing_Data : ( x, id )\n",
    "\n",
    "-- x format : \n",
    "---- NFCC : [ 39-vector ]'\n",
    "\n",
    "-- y format :\n",
    "---- Now we use 39-phonemes for all : [ 0 0 0 ... 1 .... 0 0 0 ]  as a number of 39\n",
    "'''\n",
    "\n",
    "def Dataset ( model, dratio ) :\n",
    "    trains = {}\n",
    "    tests_data = {}\n",
    "    phones_mapping = {} # {48} to realNumber\n",
    "    result_mapping = {} # {48} to {39}\n",
    "    \n",
    "    training_inputs = []\n",
    "    training_result = []\n",
    "    if model == \"mfcc\":\n",
    "        # TRAINING X(INPUT)\n",
    "        with open('./MLDS_HW1_RELEASE_v1/mfcc/train.ark') as f:\n",
    "            for lines in f :\n",
    "                frames = lines.split(' ')\n",
    "                \n",
    "                # trains[frames[0]] = frames[1:]\n",
    "                trains[frames[0]] = frame2float\n",
    "\n",
    "        # MAPPING 48 to number ( we map 48 to 39 later )\n",
    "        with open('./MLDS_HW1_RELEASE_v1/phones/48_39.map') as f:    \n",
    "            i = 0\n",
    "            for lines in f :\n",
    "                phones = lines.split('\\t')\n",
    "                phones_mapping[phones[0]] = i\n",
    "                i += 1\n",
    "        \n",
    "        with open('./MLDS_HW1_RELEASE_v1/phones/48_39.map') as f:    \n",
    "            for lines in f :\n",
    "                phones = lines.split('\\t')\n",
    "                result_mapping[ phones_mapping[phones[0]] ] = phones[1]\n",
    "        \n",
    "        # TRAINING Y(OUTPUT)\n",
    "        with open('./MLDS_HW1_RELEASE_v1/label/train.lab') as f:\n",
    "            for lines in f :\n",
    "                labels = lines.split(',')\n",
    "                # trains[labels[0]].append(labels[1])\n",
    "                training_inputs.append( np.reshape(trains.get(labels[0]), (39, 1) ) )\n",
    "                training_result.append( vectorized_result(phones_mapping[labels[1].rstrip('\\n')] ) )\n",
    "        \n",
    "        # 10% for validation\n",
    "        dataSize_weUse = len(training_inputs) * dratio\n",
    "        trainingRationTEST = int( dataSize_weUse * 0.9)\n",
    "        trainingRationVARI = int( dataSize_weUse * 0.1)\n",
    "        trainingRationVARIend = trainingRationVARI + trainingRationTEST\n",
    "        print \"slide ratio : \" , trainingRationTEST\n",
    "        if dratio != 1:\n",
    "            training_data = zip(training_inputs[0:trainingRationTEST], training_result[0:trainingRationTEST])\n",
    "            validation_data = zip(training_inputs[trainingRationTEST+1:trainingRationVARIend], training_result[trainingRationTEST+1:trainingRationVARIend])\n",
    "        else:\n",
    "        \n",
    "            X_train, X_test, y_train, y_test = cv.train_test_split(training_inputs, training_result, test_size=0.1)\n",
    "            y_train = np.array(y_train)\n",
    "            y_train = ((y_train.flatten())).reshape(len(y_train.flatten())/48,48).astype(np.float32)\n",
    "            X_train = np.array(X_train)\n",
    "            X_train = ((X_train.flatten())).reshape(len(X_train.flatten())/39,39).astype(np.float32)\n",
    "            y_test = np.array(y_test)\n",
    "            y_test = ((y_test.flatten())).reshape(len(y_test.flatten())/48,48).astype(np.float32)\n",
    "            X_test = np.array(X_test)\n",
    "            X_test = ((X_test.flatten())).reshape(len(X_test.flatten())/39,39).astype(np.float32)\n",
    "            \n",
    "            \n",
    "#             training_data = zip(training_inputs[0:trainingRationTEST], training_result[0:trainingRationTEST])\n",
    "#             validation_data = zip(training_inputs[trainingRationTEST+1:], training_result[trainingRationTEST+1:])\n",
    "            \n",
    "#         print \"Size of Training Data\", len(training_data)\n",
    "#         print \"Size of Validation Data\", len(validation_data)\n",
    "        \n",
    "        # Testing data\n",
    "        with open('./MLDS_HW1_RELEASE_v1/mfcc/test.ark') as f:\n",
    "            # i = 0\n",
    "            for lines in f :\n",
    "                frames = lines.split(' ')\n",
    "                tests_data[frames[0]] = np.reshape([ float(x) for x in frames[1:] ], (39, 1) )\n",
    "                # if i < 20:\n",
    "                #     i += 1\n",
    "                # else:\n",
    "                #    break\n",
    "        \n",
    "#         tests_data = np.array(tests_data.values())\n",
    "#         tests_data = np.array(((tests_data.flatten()))).reshape(len(tests_data.flatten())/39,39).astype(np.float32)\n",
    "                \n",
    "    else:\n",
    "        print \"Not implement yet\"\n",
    "        \n",
    "#     return ( training_data, validation_data, tests_data, result_mapping )\n",
    "    return (X_train, X_test, y_train, y_test, tests_data, result_mapping)\n",
    "                                       \n",
    "def vectorized_result ( j ) :\n",
    "    e = np.zeros((48, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "srng = RandomStreams()\n",
    "\n",
    "# translate data to theano data type\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "# initialize weight by random\n",
    "def init_weights(shape):\n",
    "    return theano.shared(floatX(np.random.randn(*shape) * 0.01))\n",
    "\n",
    "# rectify\n",
    "def rectify(X):\n",
    "    return T.maximum(X, 0.)\n",
    "\n",
    "\n",
    "def sigmoid(X):\n",
    "    return 1.0/(1.0+ T.exp(-X))\n",
    "\n",
    "# softmax\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "# method provided by Hinton\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.9, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "# momentum method\n",
    "def momentum(loss, all_params, learning_rate, momentum=0.9):\n",
    "    all_grads = theano.grad(loss, all_params)\n",
    "    updates = []\n",
    "\n",
    "    for param_i, grad_i in zip(all_params, all_grads):\n",
    "        mparam_i = theano.shared(np.zeros(param_i.get_value().shape,\n",
    "                                          dtype=theano.config.floatX),\n",
    "                                 broadcastable=param_i.broadcastable)\n",
    "        v = momentum * mparam_i - learning_rate * grad_i\n",
    "        updates.append((mparam_i, v))\n",
    "        updates.append((param_i, param_i + v))\n",
    "\n",
    "    return updates\n",
    "\n",
    "def multinominal_cross_entropy(z, X):\n",
    "    \n",
    "    L = - T.sum(X * T.log(z) + (1 - X) * T.log(1 - z), axis=1)\n",
    "    loss = T.sum(L) / X.shape[0]\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# dropout\n",
    "def dropout(X, p=0.):\n",
    "    if p > 0:\n",
    "        retain_prob = 1 - p\n",
    "        X *= srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "        X /= retain_prob\n",
    "    return X\n",
    "\n",
    "# def model(X, w_h, w_o, p_drop_input, p_drop_hidden):\n",
    "#     X = dropout(X, p_drop_input)\n",
    "#     h = rectify(T.dot(X, w_h))\n",
    "\n",
    "#     h = dropout(h, p_drop_hidden)\n",
    "#     h2 = rectify(T.dot(h, w_h2))\n",
    "\n",
    "#     h2 = dropout(h2, p_drop_hidden)\n",
    "#     py_x = softmax(T.dot(h2, w_o))\n",
    "#     return h, h2, py_x\n",
    "\n",
    "def model(X, w_h, w_o):\n",
    "    h = sigmoid(T.dot(X,w_h))\n",
    "    py_x = softmax(T.dot(h,w_o))\n",
    "    \n",
    "    return py_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DNN():    \n",
    "    \n",
    "    def __init__(self, input_shape, hidden_layer, batch, \n",
    "                 max_epochs, eval_size, output_num_units, learning_rate):\n",
    "        self.input_shape = input_shape\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.batch = batch\n",
    "        self.max_epochs = max_epochs\n",
    "        self.eval_size = eval_size\n",
    "        self.output_num_units = output_num_units\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_history_ = []\n",
    "        \n",
    "        w_h = init_weights((self.input_shape[1], self.hidden_layer))\n",
    "        w_o = init_weights((self.hidden_layer, self.output_num_units))\n",
    "\n",
    "        X = T.dmatrix()\n",
    "        Y = T.dmatrix()\n",
    "\n",
    "        # Construct Theano expression graph\n",
    "        py_x = model(X, w_h, w_o)\n",
    "        y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "        # cost function need to modify\n",
    "        # cost = T.mean(T.nnet.categorical_crossentropy(py_x, Y))\n",
    "        cost = multinominal_cross_entropy(py_x, Y) \n",
    "        params = [w_h, w_o]\n",
    "        updates = momentum(cost, params, self.learning_rate)\n",
    "\n",
    "        # Compile expressions to functions\n",
    "        self.train = theano.function(inputs=[X, Y], outputs=[y_x, cost], \n",
    "                                updates=updates, allow_input_downcast=True, name = \"train\")\n",
    "        self.predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True, name = \"predict\")\n",
    "\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        X_train, X_test, y_train, y_test = cv.train_test_split(x, y, test_size= self.eval_size)\n",
    "        yy = np.array(map((lambda x: np.argmax(x)), y_test))\n",
    "        \n",
    "        if any([x.op.__class__.__name__ in ['Gemv', 'CGemv', 'Gemm', 'CGemm'] for x in\n",
    "        self.train.maker.fgraph.toposort()]):\n",
    "            print 'Used the cpu'\n",
    "        elif any([x.op.__class__.__name__ in ['GpuGemm', 'GpuGemv'] for x in\n",
    "                  self.train.maker.fgraph.toposort()]):\n",
    "            print 'Used the gpu'\n",
    "        else:\n",
    "            print 'ERROR, not able to tell if theano used the cpu or the gpu'\n",
    "            print self.train.maker.fgraph.toposort()\n",
    "            \n",
    "        print \" \"\n",
    "        print \"start training!!!!\"\n",
    "        print \" \"\n",
    "            \n",
    "        epochs = 0\n",
    "        for i in range(self.max_epochs):\n",
    "            epochs +=1\n",
    "            t0 = time()\n",
    "            \n",
    "            # To do: modify a function of mini batch and using random\n",
    "            for start, end in zip(range(0, len(X_train), self.batch), range(self.batch, len(X_train), self.batch)):\n",
    "                pred, err = self.train(X_train[start:end], y_train[start:end])\n",
    "            \n",
    "            score = accuracy_score(yy, self.predict(X_test))\n",
    "            self.train_history_.append({\"epoch\":epochs, \"err\": err, \"score\":score})\n",
    "            \n",
    "            print 'epoch {0} : err = {1}, score = {2}, time ={3} s'.format(epochs, err, score, time() - t0)\n",
    "\n",
    "    def prediction(self, x):\n",
    "        return self.predict(x)\n",
    "\n",
    "    def outputCSV(self, wfilename, test_data, mapd): # read dictionary for id \n",
    "        test_results = []\n",
    "                \n",
    "        for xid, xdata in test_data.iteritems():\n",
    "            test_results.append( (xid, self.predict(xdata.T)) ) \n",
    "            \n",
    "        f = open(wfilename, 'w+')\n",
    "        f.write(\"Id,Prediction\\n\")\n",
    "        for xid, y in test_results:\n",
    "            f.write(\"{0},{1}\".format(xid, mapd[y[0]]))\n",
    "            # f.write(\"{0},{1},{2}\".format(xid, mapd[y], y))\n",
    "        f.close()\n",
    "        print \"MISSION COMPLETE\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slide ratio :  1012340\n"
     ]
    }
   ],
   "source": [
    "### Data Processing \n",
    "\n",
    "data_ratio = 1 # the input we use ( to put more efford on improve parameter)\n",
    "\n",
    "X_train, X_test, y_train, y_test, test_data, result_mapping = Dataset(\"mfcc\", data_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1012340, 48)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(y_train)).shape\n",
    "# y_train = np.array(y_train)\n",
    "# ((y_train.flatten())).reshape(len(y_train.flatten())/48,48).astype(np.float32)\n",
    "# y_train[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1012340, 39)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(X_train)).shape\n",
    "# X_train = np.array(X_train)\n",
    "# X_train[0].shape\n",
    "# (X_train.flatten()).reshape(1012340,39)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(test_data)).shape\n",
    "# len(np.array(test_data.values()[0]))\n",
    "# np.array(test_data.values()).flatten()\n",
    "\n",
    "# test_data = np.array(test_data.values())\n",
    "# np.array(((test_data.flatten()))).reshape(len(test_data.flatten())/39,39).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net1 = DNN(input_shape=(128,39), hidden_layer=128, batch=128, \n",
    "                 max_epochs=25, eval_size=0.1, output_num_units=48, learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR, not able to tell if theano used the cpu or the gpu\n",
      "[HostFromGpu(<CudaNdarrayType(float32, matrix)>), GpuDimShuffle{1,0}(<CudaNdarrayType(float32, matrix)>), HostFromGpu(<CudaNdarrayType(float32, matrix)>), InplaceDimShuffle{1,0}(<TensorType(float64, matrix)>), Elemwise{sub,no_inplace}(TensorConstant{(1, 1) of 1.0}, <TensorType(float64, matrix)>), Shape_i{0}(<TensorType(float64, matrix)>), HostFromGpu(GpuDimShuffle{1,0}.0), dot(<TensorType(float64, matrix)>, HostFromGpu.0), InplaceDimShuffle{x}(Shape_i{0}.0), sigmoid(dot.0), Elemwise{inv,no_inplace}(InplaceDimShuffle{x}.0), dot(sigmoid.0, HostFromGpu.0), InplaceDimShuffle{1,0}(sigmoid.0), InplaceDimShuffle{0,x}(Elemwise{inv,no_inplace}.0), Reduce{maximum}{1}(dot.0), InplaceDimShuffle{0,x}(max), Elemwise{sub,no_inplace}(dot.0, InplaceDimShuffle{0,x}.0), Softmax(Elemwise{sub,no_inplace}.0), Elemwise{Exp}[(0, 0)](Elemwise{sub,no_inplace}.0), Sum{axis=[1], acc_dtype=float64}(Elemwise{Exp}[(0, 0)].0), InplaceDimShuffle{0,x}(Sum{axis=[1], acc_dtype=float64}.0), Elemwise{true_div,no_inplace}(Elemwise{Exp}[(0, 0)].0, InplaceDimShuffle{0,x}.0), Elemwise{sub,no_inplace}(TensorConstant{(1, 1) of 1.0}, Elemwise{true_div,no_inplace}.0), MaxAndArgmax(Elemwise{true_div,no_inplace}.0, TensorConstant{1}), Elemwise{Composite{((i0 * log(i1)) + (i2 * log(i3)))}}(<TensorType(float64, matrix)>, Elemwise{true_div,no_inplace}.0, Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0), Elemwise{Composite{((((i0 * i1 * i2) / i3) + ((i1 * i4) / i5)) * i6)}}[(0, 3)](TensorConstant{(1, 1) of -1.0}, InplaceDimShuffle{0,x}.0, <TensorType(float64, matrix)>, Elemwise{true_div,no_inplace}.0, Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0, Softmax.0), Sum{axis=[1], acc_dtype=float64}(Elemwise{Composite{((i0 * log(i1)) + (i2 * log(i3)))}}.0), Sum{axis=[1], acc_dtype=float64}(Elemwise{Composite{((((i0 * i1 * i2) / i3) + ((i1 * i4) / i5)) * i6)}}[(0, 3)].0), Sum{acc_dtype=float64}(Sum{axis=[1], acc_dtype=float64}.0), InplaceDimShuffle{0,x}(Sum{axis=[1], acc_dtype=float64}.0), Elemwise{Composite{((-i0) / i1)}}[(0, 0)](Sum{acc_dtype=float64}.0, Shape_i{0}.0), Elemwise{Composite{((-i0) / i1)}}[(0, 0)](InplaceDimShuffle{0,x}.0, InplaceDimShuffle{0,x}.0), Elemwise{Mul}[(0, 1)](Elemwise{Composite{((-i0) / i1)}}[(0, 0)].0, Elemwise{Exp}[(0, 0)].0), Elemwise{add,no_inplace}(Elemwise{Composite{((((i0 * i1 * i2) / i3) + ((i1 * i4) / i5)) * i6)}}[(0, 3)].0, Elemwise{Mul}[(0, 1)].0), Sum{axis=[1], acc_dtype=float64}(Elemwise{add,no_inplace}.0), InplaceDimShuffle{0,x}(Sum{axis=[1], acc_dtype=float64}.0), Elemwise{Composite{((i0 * EQ(i1, i2) * i3) + i4 + i5)}}[(0, 2)](TensorConstant{(1, 1) of -1.0}, InplaceDimShuffle{0,x}.0, dot.0, InplaceDimShuffle{0,x}.0, Elemwise{Composite{((((i0 * i1 * i2) / i3) + ((i1 * i4) / i5)) * i6)}}[(0, 3)].0, Elemwise{Mul}[(0, 1)].0), Dot22(InplaceDimShuffle{1,0}.0, Elemwise{Composite{((i0 * EQ(i1, i2) * i3) + i4 + i5)}}[(0, 2)].0), dot(Elemwise{Composite{((i0 * EQ(i1, i2) * i3) + i4 + i5)}}[(0, 2)].0, HostFromGpu.0), Elemwise{Cast{float32}}(Dot22.0), Elemwise{Composite{(scalar_sigmoid((-i0)) * i1 * i2)}}[(0, 0)](dot.0, sigmoid.0, dot.0), GpuFromHost(Elemwise{Cast{float32}}.0), Dot22(InplaceDimShuffle{1,0}.0, Elemwise{Composite{(scalar_sigmoid((-i0)) * i1 * i2)}}[(0, 0)].0), GpuElemwise{Composite{((i0 * i1) - (i2 * i3))}}[(0, 1)](CudaNdarrayConstant{[[ 0.89999998]]}, <CudaNdarrayType(float32, matrix)>, CudaNdarrayConstant{[[ 0.001]]}, GpuFromHost.0), Elemwise{Cast{float32}}(Dot22.0), GpuElemwise{Add}[(0, 0)](<CudaNdarrayType(float32, matrix)>, GpuElemwise{Composite{((i0 * i1) - (i2 * i3))}}[(0, 1)].0), GpuFromHost(Elemwise{Cast{float32}}.0), GpuElemwise{Composite{((i0 * i1) - (i2 * i3))}}[(0, 1)](CudaNdarrayConstant{[[ 0.89999998]]}, <CudaNdarrayType(float32, matrix)>, CudaNdarrayConstant{[[ 0.001]]}, GpuFromHost.0), GpuElemwise{Add}[(0, 0)](<CudaNdarrayType(float32, matrix)>, GpuElemwise{Composite{((i0 * i1) - (i2 * i3))}}[(0, 1)].0)]\n",
      " \n",
      "start training!!!!\n",
      " \n",
      "epoch 1 : err = 2.97678580266, score = 0.390570361736, time =15.822273016 s\n",
      "epoch 2 : err = 2.75452518876, score = 0.437293794575, time =15.8453149796 s\n",
      "epoch 3 : err = 2.62780697493, score = 0.461188928621, time =15.7619819641 s\n",
      "epoch 4 : err = 2.53099165268, score = 0.477221091728, time =15.7559859753 s\n",
      "epoch 5 : err = 2.45498814478, score = 0.487958590987, time =15.7508671284 s\n",
      "epoch 6 : err = 2.39436952609, score = 0.495485706383, time =15.7806639671 s\n",
      "epoch 7 : err = 2.34122433474, score = 0.502874528321, time =15.7367100716 s\n",
      "epoch 8 : err = 2.2946611221, score = 0.508307485627, time =15.7395498753 s\n",
      "epoch 9 : err = 2.25525800342, score = 0.513088488057, time =15.7439579964 s\n",
      "epoch 10 : err = 2.22269257182, score = 0.516644605567, time =15.7479910851 s\n",
      "epoch 11 : err = 2.19534664331, score = 0.520062429619, time =15.7413849831 s\n",
      "epoch 12 : err = 2.17182580261, score = 0.523183910544, time =15.7483639717 s\n",
      "epoch 13 : err = 2.15127112447, score = 0.52581148626, time =15.7373008728 s\n",
      "epoch 14 : err = 2.13322581118, score = 0.528182231266, time =15.7382249832 s\n",
      "epoch 15 : err = 2.11662405521, score = 0.530355414189, time =15.7376999855 s\n",
      "epoch 16 : err = 2.10160563001, score = 0.532301400715, time =15.7290139198 s\n",
      "epoch 17 : err = 2.08834851908, score = 0.534444949325, time =15.738394022 s\n",
      "epoch 18 : err = 2.0765760095, score = 0.536390935852, time =15.7387218475 s\n",
      "epoch 19 : err = 2.06595679078, score = 0.537763992335, time =15.7307481766 s\n",
      "epoch 20 : err = 2.05625427908, score = 0.539285220381, time =15.7325801849 s\n",
      "epoch 21 : err = 2.04729457199, score = 0.540421202363, time =15.7299249172 s\n",
      "epoch 22 : err = 2.0389378151, score = 0.541784380742, time =15.7205209732 s\n",
      "epoch 23 : err = 2.03108321366, score = 0.543187071537, time =15.7168149948 s\n",
      "epoch 24 : err = 2.02364524428, score = 0.544352687832, time =15.7190179825 s\n",
      "epoch 25 : err = 2.01653189563, score = 0.54517257048, time =15.7167978287 s\n",
      "CPU times: user 9min 14s, sys: 16min 28s, total: 25min 43s\n",
      "Wall time: 6min 33s\n"
     ]
    }
   ],
   "source": [
    "%time net1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1, 'err': array(2.9767858026605394), 'score': 0.3905703617361756},\n",
       " {'epoch': 2, 'err': array(2.75452518875809), 'score': 0.43729379457494516},\n",
       " {'epoch': 3, 'err': array(2.627806974933892), 'score': 0.46118892862081912},\n",
       " {'epoch': 4, 'err': array(2.5309916526771397), 'score': 0.47722109172807553},\n",
       " {'epoch': 5, 'err': array(2.454988144782991), 'score': 0.48795859098721772},\n",
       " {'epoch': 6, 'err': array(2.3943695260869777), 'score': 0.49548570638323092},\n",
       " {'epoch': 7, 'err': array(2.341224334737912), 'score': 0.50287452832052471},\n",
       " {'epoch': 8, 'err': array(2.294661122103386), 'score': 0.50830748562735839},\n",
       " {'epoch': 9, 'err': array(2.2552580034164778), 'score': 0.51308848805737206},\n",
       " {'epoch': 10, 'err': array(2.2226925718169546), 'score': 0.51664460556729952},\n",
       " {'epoch': 11, 'err': array(2.1953466433118676), 'score': 0.52006242961850757},\n",
       " {'epoch': 12, 'err': array(2.171825802607993), 'score': 0.52318391054388846},\n",
       " {'epoch': 13, 'err': array(2.151271124465296), 'score': 0.52581148625955709},\n",
       " {'epoch': 14, 'err': array(2.133225811183054), 'score': 0.52818223126617536},\n",
       " {'epoch': 15, 'err': array(2.1166240552079705), 'score': 0.53035541418890886},\n",
       " {'epoch': 16, 'err': array(2.1016056300064956), 'score': 0.53230140071517473},\n",
       " {'epoch': 17, 'err': array(2.088348519080315), 'score': 0.53444494932532549},\n",
       " {'epoch': 18, 'err': array(2.0765760094985595), 'score': 0.53639093585159137},\n",
       " {'epoch': 19, 'err': array(2.0659567907833534), 'score': 0.53776399233459116},\n",
       " {'epoch': 20, 'err': array(2.056254279077868), 'score': 0.53928522038050453},\n",
       " {'epoch': 21, 'err': array(2.047294571985215), 'score': 0.54042120236284252},\n",
       " {'epoch': 22, 'err': array(2.0389378150970034), 'score': 0.54178438074164803},\n",
       " {'epoch': 23, 'err': array(2.0310832136552577), 'score': 0.54318707153723056},\n",
       " {'epoch': 24, 'err': array(2.0236452442811372), 'score': 0.54435268783215129},\n",
       " {'epoch': 25, 'err': array(2.0165318956287295), 'score': 0.54517257048027346}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1.train_history_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "import cPickle as pickle\n",
    "with open('first_net.pickle', 'wb') as f:\n",
    "    pickle.dump(net1, f, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "\n",
    "with open('./first_net.pickle', 'rb') as f:\n",
    "    net = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.39      0.49      0.43      2462\n",
      "          1       0.43      0.65      0.52      2828\n",
      "          2       0.28      0.26      0.27      1654\n",
      "          3       0.42      0.38      0.40      2204\n",
      "          4       0.29      0.19      0.23      1085\n",
      "          5       0.38      0.22      0.28      1868\n",
      "          6       0.49      0.49      0.49      2640\n",
      "          7       0.54      0.45      0.49      1090\n",
      "          8       0.47      0.19      0.27       807\n",
      "          9       0.62      0.70      0.66      6688\n",
      "         10       0.46      0.23      0.31      1207\n",
      "         11       0.36      0.34      0.35      1415\n",
      "         12       0.53      0.36      0.43       857\n",
      "         13       0.35      0.28      0.31      2573\n",
      "         14       0.44      0.22      0.29       995\n",
      "         15       0.41      0.01      0.02       619\n",
      "         16       0.41      0.19      0.26       375\n",
      "         17       0.62      0.63      0.62      3966\n",
      "         18       0.51      0.47      0.49      2625\n",
      "         19       0.61      0.67      0.64      2370\n",
      "         20       0.50      0.16      0.24       664\n",
      "         21       0.41      0.43      0.42      1407\n",
      "         22       0.35      0.28      0.31      2943\n",
      "         23       0.36      0.38      0.37      3626\n",
      "         24       0.58      0.74      0.65      4507\n",
      "         25       0.47      0.30      0.36       837\n",
      "         26       0.59      0.61      0.60      2571\n",
      "         27       0.44      0.59      0.50      3522\n",
      "         28       0.58      0.48      0.53      2758\n",
      "         29       0.48      0.19      0.27       951\n",
      "         30       0.48      0.65      0.56      4374\n",
      "         31       0.41      0.35      0.38      1869\n",
      "         32       0.31      0.02      0.04       467\n",
      "         33       0.49      0.48      0.48      1770\n",
      "         34       0.52      0.58      0.54      3001\n",
      "         35       0.55      0.66      0.60      1617\n",
      "         36       0.80      0.91      0.85     11469\n",
      "         37       0.64      0.85      0.73      7199\n",
      "         38       0.42      0.01      0.02       737\n",
      "         39       0.50      0.51      0.51      2503\n",
      "         40       0.00      0.00      0.00       323\n",
      "         41       0.46      0.36      0.41      1779\n",
      "         42       0.60      0.56      0.58      3607\n",
      "         43       0.50      0.27      0.35      1320\n",
      "         44       0.59      0.54      0.56      1870\n",
      "         45       0.47      0.27      0.34       770\n",
      "         46       1.00      0.01      0.01       149\n",
      "         47       0.60      0.41      0.49      3545\n",
      "\n",
      "avg / total       0.53      0.54      0.53    112483\n",
      "\n",
      "0.544953459634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pika/anaconda/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = net1.prediction(X_test)\n",
    "\n",
    "yyy = np.array(map((lambda x: np.argmax(x)), y_test))\n",
    "\n",
    "print metrics.classification_report((yyy), (y_pred))\n",
    "print accuracy_score(yyy, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write predict file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSION COMPLETE\n"
     ]
    }
   ],
   "source": [
    "net1.outputCSV(\"net1_predict.csv\", test_data, result_mapping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
