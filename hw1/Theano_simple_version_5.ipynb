{"nbformat_minor": 0, "cells": [{"source": "# Theano version of simple DNN version 5\n\n***What this version done ***\n1. Add layer dynamically to 6 layers\n2. Solve dropout bug -- predict problem, prob. problem\n3. momentum dynamically increase from 0.5 to 0.9 for first 20 epochs, and using 0.9 till end (or to 0.999)\n4. Add max norm to each hidden layer\n\n***Todo***\n2. ~~Add other method -- nesterov_momentum, relu, tanh~~\n3. debug auto adjust since we just linearly divide by max_epoch for every epoch\n4. [Add keyboardinterrupt exception then save the model](http://stackoverflow.com/questions/21120947/catching-keyboardinterrupt-in-python-during-program-shutdown)\n5. Add unsupervised learning method http://www.csri.utoronto.ca/~hinton/absps/googlerectified.pdf\n6. Change data to filter-bank\n7. ~~fix X.dot(w)+b~~\n8. ~~Check dropout method problem~~\n9. Add maxout network\n- ~~Add regularization term to every hidden layer~~\n- Add regularization to cost function\n-------\n\n***Tune variable***\n1. Change hidden layer size\n2. Change dropout ratio\n3. Change activation function\n4. [Weight initializate](http://deeplearning.net/tutorial/mlp.html#weight-initialization)\n5. > We used probability of retention p = 0.8 in the input layers and 0.5 in the hidden layers.\nMax-norm constraint with c = 4 was used in all the layers. A momentum of 0.95 with a\nhigh learning rate of 0.1 was used. The learning rate was decayed as $\\epsilon_0 \\frac{1}{(1 + t/T)}$, from [Dropout: A Simple Way to Prevent Neural Networks from\nOverfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n\n***Knowledge need to fill the gap***\n1. Numpy broadcast http://wiki.scipy.org/EricsBroadcastingDoc\n2. Rewrite deep equation\n\n***Question***\n1. Why dont need a bias term b in the equation or even initialize?\n2. Once apply dropout, the result is strange\n3. Softmax weight divide by 2 ??\n\n***Github***\n- https://gist.github.com/SnippyHolloW/8a0f820261926e2f41cc\n- https://github.com/dnouri/nolearn/blob/master/nolearn/lasagne.py\n- https://github.com/benanne/Lasagne/blob/master/lasagne/nonlinearities.py\n- https://github.com/nitishsrivastava/deepnet/tree/master/deepnet\n\n***Paper***\n1. [Improving neural networks by preventing\nco-adaptation of feature detectors](http://arxiv.org/pdf/1207.0580.pdf)\n2. [Dropout: A Simple Way to Prevent Neural Networks from\nOverfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n3. [Maxout Network](http://arxiv.org/pdf/1302.4389v4.pdf)\n\n***References***\n0. [Ipython Markdown](http://nbviewer.ipython.org/github/twistedhardware/mltutorial/blob/master/notebooks/IPython-Tutorial/2%20-%20Markdown%20%26%20LATEX.ipynb)\n- Dropout, Hinton http://arxiv.org/pdf/1207.0580.pdf\n- Dropout, Pratical guide http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf\n- Dropout, MaxOutnetwork http://arxiv.org/pdf/1302.4389v4.pdf\n- [Several Deep](http://snippyhollow.github.io/blog/2014/08/09/so-you-wanna-try-deep-learning/) -- Dropout, you will see improvement only on large-enough networks (> 1000 units / layer, > 3-4 layers **[example](https://gist.github.com/SnippyHolloW/8a0f820261926e2f41cc)**\n- Using cudamat implementation deep: expert github https://github.com/nitishsrivastava/deepnet\n2. Tutorial http://neuralnetworksanddeeplearning.com/chap3.html\n3. http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6296526&searchWithin%3Dp_Authors%3A.QT.Hinton%2C+G..QT.%26searchWithin%3Dp_Author_Ids%3A37270925500\n4. http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5704567&searchWithin%3Dp_Authors%3A.QT.Hinton%2C+G..QT.%26searchWithin%3Dp_Author_Ids%3A37270925500%26sortType%3Ddesc_p_Publication_Year\n5. Kaldi, acostic preprocess http://kaldi.sourceforge.net/", "cell_type": "markdown", "metadata": {}}, {"execution_count": 16, "cell_type": "code", "source": "import theano\nfrom theano import tensor as T\nfrom theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\nimport numpy as np\n\nfrom sklearn import cross_validation as cv\nfrom sklearn import metrics\nfrom sklearn import grid_search as gs\nfrom sklearn.metrics import accuracy_score\n# from sklearn.base import BaseEstimator\n\nfrom time import time", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "## Data preprocess", "cell_type": "markdown", "metadata": {}}, {"execution_count": 17, "cell_type": "code", "source": "### DataProcessing \n\n'''\n[ INPUT  ] : model { wav, NFCC, ... }\n[ OUTPUT ] : (Training_data, Validation_data, Testing_Data)\n\n- Training_data : ( x, y, id )\n- Validation_data : ( x, y, id )\n- Testing_Data : ( x, id )\n\n-- x format : \n---- NFCC : [ 39-vector ]'\n\n-- y format :\n---- Now we use 39-phonemes for all : [ 0 0 0 ... 1 .... 0 0 0 ]  as a number of 39\n'''\n\ndef Dataset ( model, dratio ) :\n    trains = {}\n    tests_data = {}\n    phones_mapping = {} # {48} to realNumber\n    result_mapping = {} # {48} to {39}\n    \n    training_inputs = []\n    training_result = []\n    if model == \"mfcc\":\n        # TRAINING X(INPUT)\n        with open('./MLDS_HW1_RELEASE_v1/mfcc/train.ark') as f:\n            for lines in f :\n                frames = lines.split(' ')\n                frame2float = [ float(x) for x in frames[1:] ]\n                # trains[frames[0]] = frames[1:]\n                trains[frames[0]] = frame2float\n\n        # MAPPING 48 to number ( we map 48 to 39 later )\n        with open('./MLDS_HW1_RELEASE_v1/phones/48_39.map') as f:    \n            i = 0\n            for lines in f :\n                phones = lines.split('\\t')\n                phones_mapping[phones[0]] = i\n                i += 1\n        \n        with open('./MLDS_HW1_RELEASE_v1/phones/48_39.map') as f:    \n            for lines in f :\n                phones = lines.split('\\t')\n                result_mapping[ phones_mapping[phones[0]] ] = phones[1]\n        \n        # TRAINING Y(OUTPUT)\n        with open('./MLDS_HW1_RELEASE_v1/label/train.lab') as f:\n            for lines in f :\n                labels = lines.split(',')\n                # trains[labels[0]].append(labels[1])\n                training_inputs.append( np.reshape(trains.get(labels[0]), (39, 1) ) )\n                training_result.append( vectorized_result(phones_mapping[labels[1].rstrip('\\n')] ) )\n        \n        # 10% for validation\n        dataSize_weUse = len(training_inputs) * dratio\n        trainingRationTEST = int( dataSize_weUse * 0.9)\n        trainingRationVARI = int( dataSize_weUse * 0.1)\n        trainingRationVARIend = trainingRationVARI + trainingRationTEST\n        print \"slide ratio : \" , trainingRationTEST\n        if dratio != 1:\n            training_data = zip(training_inputs[0:trainingRationTEST], training_result[0:trainingRationTEST])\n            validation_data = zip(training_inputs[trainingRationTEST+1:trainingRationVARIend], training_result[trainingRationTEST+1:trainingRationVARIend])\n        else:\n        \n            X_train, X_test, y_train, y_test = cv.train_test_split(training_inputs, training_result, test_size=0.1)\n            y_train = np.array(y_train)\n            y_train = ((y_train.flatten())).reshape(len(y_train.flatten())/48,48).astype(np.float32)\n            X_train = np.array(X_train)\n            X_train = ((X_train.flatten())).reshape(len(X_train.flatten())/39,39).astype(np.float32)\n            y_test = np.array(y_test)\n            y_test = ((y_test.flatten())).reshape(len(y_test.flatten())/48,48).astype(np.float32)\n            X_test = np.array(X_test)\n            X_test = ((X_test.flatten())).reshape(len(X_test.flatten())/39,39).astype(np.float32)\n            \n            \n#             training_data = zip(training_inputs[0:trainingRationTEST], training_result[0:trainingRationTEST])\n#             validation_data = zip(training_inputs[trainingRationTEST+1:], training_result[trainingRationTEST+1:])\n            \n#         print \"Size of Training Data\", len(training_data)\n#         print \"Size of Validation Data\", len(validation_data)\n        \n        # Testing data\n        with open('./MLDS_HW1_RELEASE_v1/mfcc/test.ark') as f:\n            # i = 0\n            for lines in f :\n                frames = lines.split(' ')\n                tests_data[frames[0]] = np.reshape([ float(x) for x in frames[1:] ], (39, 1) )\n                # if i < 20:\n                #     i += 1\n                # else:\n                #    break\n        \n#         tests_data = np.array(tests_data.values())\n#         tests_data = np.array(((tests_data.flatten()))).reshape(len(tests_data.flatten())/39,39).astype(np.float32)\n                \n    else:\n        print \"Not implement yet\"\n        \n#     return ( training_data, validation_data, tests_data, result_mapping )\n    return (X_train, X_test, y_train, y_test, tests_data, result_mapping)\n                                       \ndef vectorized_result ( j ) :\n    e = np.zeros((48, 1))\n    e[j] = 1.0\n    return e", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "## Model part\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 65, "cell_type": "code", "source": "srng = RandomStreams()\n\n# translate data to theano data type\ndef floatX(X):\n    return np.asarray(X, dtype=theano.config.floatX)\n\n# initialize weight by random\ndef init_weights(shape):\n    return theano.shared(floatX(np.random.randn(*shape) * 0.01))\n\ndef tanh(X):\n    return (1 + T.tanh(X / 2)) / 2\n\n# rectified linear unit\ndef relu(X):\n    # return T.maximum(X, 0.)\n    return (X + abs(X)) / 2.\n\ndef sigmoid(X):\n    return 1.0/(1.0+ T.exp(-X))\n\n# softmax\ndef softmax(X):\n    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n\ndef build_shared_zeros(shape):\n    \"\"\" Builds a theano shared variable filled with a zeros numpy array \"\"\"\n    return theano.shared(value=np.zeros(shape, dtype=theano.config.floatX),\n             borrow=True)\n\n# method provided by Hinton\ndef RMSprop(cost, params, lr=0.001, rho=0.9, epsilon=1e-6):\n    grads = T.grad(cost=cost, wrt=params)\n    updates = []\n    for p, g in zip(params, grads):\n        acc = theano.shared(p.get_value() * 0.)\n        acc_new = rho * acc + (1 - rho) * g ** 2\n        gradient_scaling = T.sqrt(acc_new + epsilon)\n        g = g / gradient_scaling\n        updates.append((acc, acc_new))\n        updates.append((p, p - lr * g))\n    return updates\n\n# momentum method\ndef momentum(loss, all_params, update_momentum, update_learning_rate, max_norm):\n    all_grads = theano.grad(loss, all_params)\n    updates = []\n\n    for param_i, grad_i in zip(all_params, all_grads):\n        mparam_i = theano.shared(np.zeros(param_i.get_value().shape,\n                                          dtype=theano.config.floatX),\n                                 broadcastable=param_i.broadcastable)\n        v = update_momentum * mparam_i - update_learning_rate * grad_i\n        updates.append((mparam_i, v))\n        \n        if max_norm != None:\n            W = param_i + v\n            col_norms = W.norm(2, axis=0)\n            desired_norms = T.clip(col_norms, 0, max_norm)\n            updates.append( ( param_i , (W * (desired_norms / (1e-6 + col_norms))) ) )\n        else:\n            updates.append((param_i, param_i + v))\n\n    return updates\n\ndef multinominal_cross_entropy(z, X):\n    \n    L = - T.sum(X * T.log(z) + (1 - X) * T.log(1 - z), axis=1)\n    loss = T.sum(L) / X.shape[0]\n    \n    return loss\n\ndef negative_log_likelihood_mean(z, X):\n    return -T.mean(T.log(z)[T.arange(X.shape[0]), X])\n\ndef negative_log_likelihood_sum(z, X):\n    return -T.sum(T.log(z)[T.arange(X.shape[0]), X])\n\n\n# dropout\n# https://github.com/mdenil/dropout/issues/6\ndef dropout(X, p=0.):\n    if p > 0. and p < 1.:\n        retain_prob = 1 - p\n        X *= srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n    return X\n\n\ndef model(X, para, p_drop_input, p_drop_hidden, relu_bool, train):\n    ## assume 6 hidden  layers only\n    ## Softmax weight divide by 2??\n    for i in range (len(para)/2):\n        \n        if i is 0:\n            ######## Input layer\n            if train:\n                X = dropout(X, p_drop_input)\n            else:\n                para[i*2].set_value(para[i*2].get_value()*(1-p_drop_hidden[i-1]))\n                \n            if relu_bool:\n                h = relu(T.dot(X, para[i*2]) +para[i+1])\n            else:\n                h = sigmoid(T.dot(X, para[i*2]) + para[i+1])\n\n        elif i is 1:\n            ######## layer 1\n            if train:\n                h = dropout(h, p_drop_hidden[i-1])\n            else:\n                # h *= (1-p_drop_hidden[i-1])\n                para[i*2].set_value(para[i*2].get_value()*(1-p_drop_hidden[i-1]))\n            \n            if (i*2+2) is len(para): ## weight at least have 4, w_h_1, b_1, w_o, b_o\n                py_xx = softmax(T.dot(h, para[i*2])+ para[i*2+1])\n                break\n            \n            if relu_bool:\n                h2 = relu(T.dot(h, para[i*2] )+ para[i*2+1])\n            else: \n                h2 = sigmoid(T.dot(h, para[i*2] ) +para[i*2+1])\n        \n        elif i is 2:\n            ######## layer 2\n            if train:\n                h2 = dropout(h2, p_drop_hidden[i-1])\n            else:\n                # h2 *= (1-p_drop_hidden[i-1])\n                para[i*2].set_value(para[i*2].get_value()*(1-p_drop_hidden[i-1]))\n            \n            if (i*2+2) is len(para): ## weight at least have 6, w_h_1, b_1, w_o, b_o\n                py_xx = softmax(T.dot(h2, para[i*2])+ para[i*2+1])\n                break\n            \n            if relu_bool:\n                h3 = relu(T.dot(h2, para[i*2] )+ para[i*2+1])\n            else:\n                h3 = sigmoid(T.dot(h2, para[i*2] ) +para[i*2+1])\n                \n            \n        elif i is 3:\n            ######## layer 3\n            if train:\n                h3 = dropout(h3, p_drop_hidden[i-1])\n            else:\n                # h3 *= (1-p_drop_hidden[i-1])\n                para[i*2].set_value(para[i*2].get_value()*(1-p_drop_hidden[i-1]))\n            \n            if (i*2+2) is len(para): ## weight at least have 8, w_h_1, b_1, w_o, b_o\n                py_xx = softmax(T.dot(h3, para[i*2])+ para[i*2+1])\n                break\n            \n            if relu_bool:\n                h4 = relu(T.dot(h3, para[i*2] )+ para[i*2+1])\n            else:\n                h4 = sigmoid(T.dot(h3, para[i*2] ) +para[i*2+1])\n                \n                \n        elif i is 4:\n            ######## layer 4\n            if train:\n                h4 = dropout(h4, p_drop_hidden[i-1])\n            else:\n                # h4 *= (1-p_drop_hidden[i-1])\n                para[i*2].set_value(para[i*2].get_value()*(1-p_drop_hidden[i-1]))\n            \n            if (i*2+2) is len(para): ## weight at least have 10, w_h_1, b_1, w_o, b_o\n                py_xx = softmax(T.dot(h4, para[i*2])+ para[i*2+1])\n                break\n            \n            if relu_bool:\n                h5 = relu(T.dot(h4, para[i*2] )+ para[i*2+1])\n            else:\n                h5 = sigmoid(T.dot(h4, para[i*2] ) +para[i*2+1])\n                \n                \n        elif i is 5:\n            ######## layer 5\n            if train:\n                h5 = dropout(h5, p_drop_hidden[i-1])\n            else:\n                # h5 *= (1-p_drop_hidden[i-1])\n                para[i*2].set_value(para[i*2].get_value()*(1-p_drop_hidden[i-1]))\n            \n            if (i*2+2) is len(para): ## weight at least have 12, w_h_1, b_1, w_o, b_o\n                py_xx = softmax(T.dot(h5, para[i*2])+ para[i*2+1])\n                break\n            \n            if relu_bool:\n                h6 = relu(T.dot(h5, para[i*2] )+ para[i*2+1])\n            else:\n                h6 = sigmoid(T.dot(h5, para[i*2] ) +para[i*2+1])\n                \n        elif i is 6:\n            ######## layer 6\n            if train:\n                h6 = dropout(h6, p_drop_hidden[i-1])\n            else:\n                # h6 *= (1-p_drop_hidden[i-1])\n                para[i*2].set_value(para[i*2].get_value()*(1-p_drop_hidden[i-1]))\n            \n            if (i*2+2) is len(para): ## weight at least have 12, w_h_1, b_1, w_o, b_o\n                py_xx = softmax(T.dot(h6, para[i*2])+ para[i*2+1])\n                break\n            \n            if relu_bool:\n                h7 = relu(T.dot(h6, para[i*2] )+ para[i*2+1])\n            else:\n                h7 = sigmoid(T.dot(h6, para[i*2] ) +para[i*2+1])\n                \n    return py_xx\n    \n    \n\n# def model(X, w_h_1, b_1, \n#           w_h_2, b_2, \n#           w_h_3, b_3, \n#           w_h_4, b_4, \n#           w_h_5, b_5, \n#           w_h_6, b_6, \n#           w_o, b_o, \n#           p_drop_input, \n#           p_drop_hidden_1, p_drop_hidden_2, p_drop_hidden_3,\n#           p_drop_hidden_4, p_drop_hidden_5, p_drop_hidden_6,\n#           relu, train):\n    \n#     if train:\n#         X = dropout(X, p_drop_input)\n    \n#     ######## layer 1\n#     if !relu:\n#         h = sigmoid(T.dot(X, w_h_1) +b_1)\n#     else:\n#         h = relu(T.dot(X, w_h_1) +b_1)\n\n#     if train:\n#         h = dropout(h, p_drop_hidden_1)\n#     else:\n#         h *= (1-p_drop_hidden_1)\n    \n#     ######## layer 2\n#     if !relu:\n#         h2 = sigmoid(T.dot(h, w_h_2 ) +b_2)\n#     else:\n#         h2 = relu(T.dot(h, w_h_2 )+b_2)\n    \n#     if train:\n#         h2 = dropout(h2, p_drop_hidden_2)\n#     else:\n#         h2 *= (1-p_drop_hidden_2)\n    \n#     ######## layer 3\n#     if !relu:\n#         h3 = sigmoid(T.dot(h2, w_h_3) +b_3)\n#     else:\n#         h3 = relu(T.dot(h2, w_h_3)+b_3)\n    \n#     if train:\n#         h3 = dropout(h3, p_drop_hidden_3)\n#     else:\n#         h3 *= (1-p_drop_hidden_3)\n    \n#     ######## layer 4\n        \n#     if !relu:\n#         h4 = sigmoid(T.dot(h3, w_h_4) +b_4)\n#     else:\n#         h4 = relu(T.dot(h3, w_h_4)+b_4)\n\n#     if train:\n#         h4 = dropout(h4, p_drop_hidden_4)\n#     else:\n#         h4 *= (1-p_drop_hidden_4)\n    \n#     ######## layer 5\n#     if !relu:\n#         h5 = sigmoid(T.dot(h4, w_h_5) +b_5)\n#     else:\n#         h5 = relu(T.dot(h4, w_h_5)+b_5)\n\n#     if train:\n#         h5 = dropout(h5, p_drop_hidden_5)\n#     else:\n#         h5 *= (1-p_drop_hidden_5)\n        \n#     ######## layer 6\n#     if !relu:\n#         h6 = sigmoid(T.dot(h5, w_h_6) +b_6)\n#     else:\n#         h6 = relu(T.dot(h5, w_h_6)+b_6)\n    \n#     if train:\n#         h6 = dropout(h6, p_drop_hidden_6)\n#     else:\n#         h6 *= (1-p_drop_hidden_6)\n\n#     ######## output layer\n#     # when dropout, softmax weight need to devide by 2 ?\n#     py_x = softmax(T.dot(h6, w_o) +b_o) \n    \n#     return py_x\n\n\n# def model(X, w_h, w_o):\n#     h = sigmoid(T.dot(X,w_h))\n#     py_x = softmax(T.dot(h,w_o))\n    \n#     return py_x", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 66, "cell_type": "code", "source": "class DNN():    \n    \n    def __init__(self, input_shape, activation, hidden_layer,\n                 batch, max_epochs, eval_size, output_num_units,\n                 drop_input, drop_hidden,\n                 patience, up_learning_rate, up_momentum, max_norm):\n        \n        self.input_shape = input_shape\n        self.hidden_layer = hidden_layer\n#         self.hidden_layer_1 = hidden_layer_1\n#         self.hidden_layer_2 = hidden_layer_2\n#         self.hidden_layer_3 = hidden_layer_3\n        \n        if activation == \"relu\":\n            self.activation = True\n        else:\n            self.activation = False\n        \n        self.batch = batch\n        self.max_epochs = max_epochs\n        self.eval_size = eval_size\n        self.output_num_units = output_num_units\n        \n        self.up_learning_rate = up_learning_rate\n        self.up_momentum = up_momentum\n        \n        self.drop_input = drop_input\n        self.drop_hidden = drop_hidden\n#         self.drop_hidden_1 = drop_hidden_1\n#         self.drop_hidden_2 = drop_hidden_2\n#         self.drop_hidden_3 = drop_hidden_3\n        \n        self.patience = patience\n        self.best_valid = -np.inf\n        self.best_valid_epoch = 0\n        self.best_weights = None\n        \n        self.max_norm = max_norm\n        \n        self.train_history_ = []\n\n        self.params = []\n        for i in range(len(hidden_layer)):\n            if i is 0:\n                w_h_1 = init_weights((self.input_shape[1], self.hidden_layer[i]))\n                b_1 = build_shared_zeros(self.hidden_layer[i])\n                self.params.append(w_h_1)\n                self.params.append(b_1)\n            elif i is 1:\n                w_h_2 = init_weights((self.hidden_layer[i-1], self.hidden_layer[i]))\n                b_2 = build_shared_zeros(self.hidden_layer[i])\n                self.params.append(w_h_2)\n                self.params.append(b_2)\n            elif i is 2:\n                w_h_3 = init_weights((self.hidden_layer[i-1], self.hidden_layer[i]))\n                b_3 = build_shared_zeros(self.hidden_layer[i])\n                self.params.append(w_h_3)\n                self.params.append(b_3)\n            elif i is 3:\n                w_h_4 = init_weights((self.hidden_layer[i-1], self.hidden_layer[i]))\n                b_4 = build_shared_zeros(self.hidden_layer[i])\n                self.params.append(w_h_4)\n                self.params.append(b_4)\n            elif i is 4:\n                w_h_5 = init_weights((self.hidden_layer[i-1], self.hidden_layer[i]))\n                b_5 = build_shared_zeros(self.hidden_layer[i])\n                self.params.append(w_h_5)\n                self.params.append(b_5)\n            elif i is 5:\n                w_h_6 = init_weights((self.hidden_layer[i-1], self.hidden_layer[i]))\n                b_6 = build_shared_zeros(self.hidden_layer[i])\n                self.params.append(w_h_6)\n                self.params.append(b_6)\n            elif i is 6:\n                w_h_7 = init_weights((self.hidden_layer[i-1], self.hidden_layer[i]))\n                b_7 = build_shared_zeros(self.hidden_layer[i])\n                self.params.append(w_h_7)\n                self.params.append(b_7)\n                \n            if (i+1) is len(hidden_layer):\n                w_o = init_weights((self.hidden_layer[i], self.output_num_units))\n                b_o = build_shared_zeros(self.output_num_units)\n                self.params.append(w_o)\n                self.params.append(b_o)\n                break\n\n            \n#         w_h_1 = init_weights((self.input_shape[1], self.hidden_layer_1))\n#         w_h_2 = init_weights((self.hidden_layer_1, self.hidden_layer_2))\n#         w_h_3 = init_weights((self.hidden_layer_2, self.hidden_layer_3))\n#         w_h_4 = init_weights((self.hidden_layer_3, self.hidden_layer_4))\n#         w_h_5 = init_weights((self.hidden_layer_4, self.hidden_layer_5))\n#         w_h_6 = init_weights((self.hidden_layer_5, self.hidden_layer_6))\n#         w_o = init_weights((self.hidden_layer_6, self.output_num_units))\n        \n#         b_1 = build_shared_zeros(self.hidden_layer_1)\n#         b_2 = build_shared_zeros(self.hidden_layer_2)\n#         b_3 = build_shared_zeros(self.hidden_layer_3)\n#         b_4 = build_shared_zeros(self.hidden_layer_4)\n#         b_5 = build_shared_zeros(self.hidden_layer_5)\n#         b_6 = build_shared_zeros(self.hidden_layer_6)\n#         b_o = build_shared_zeros(self.output_num_units)\n        \n        self.update_learning_rate= theano.shared(floatX( up_learning_rate['start'] ))\n        self.lr = np.linspace(up_learning_rate['start'], up_learning_rate['stop'], self.max_epochs)\n        # self.lr = np.linspace(up_learning_rate['start'], up_learning_rate['stop'], 20)\n        \n        self.update_momentum= theano.shared(floatX( up_momentum['start'] ))\n        # self.mm = np.linspace(up_momentum['start'], up_momentum['stop'], self.max_epochs)\n        self.mm = np.linspace(up_momentum['start'], up_momentum['stop'], up_momentum['epoch'])\n    \n        X = T.dmatrix()\n        Y = T.dmatrix()\n\n        # Construct Theano expression graph\n#         py_x_drop = model(X, \n#                      w_h_1,b_1,\n#                      w_h_2,b_2,\n#                      w_h_3,b_3, \n#                      w_h_4,b_4, \n#                      w_h_5,b_5, \n#                      w_h_6,b_6, \n#                      w_o, b_o,\n#                      self.drop_input, \n#                      self.drop_hidden_1, self.drop_hidden_2, self.drop_hidden_3, \n#                      self.drop_hidden_4, self.drop_hidden_5, self.drop_hidden_6,\n#                      self.activation, True)\n        \n#         py_x = model(X, \n#                      w_h_1,b_1,\n#                      w_h_2,b_2,\n#                      w_h_3,b_3, \n#                      w_h_4,b_4, \n#                      w_h_5,b_5, \n#                      w_h_6,b_6, \n#                      w_o, b_o,\n#                      self.drop_input, \n#                      self.drop_hidden_1, self.drop_hidden_2, self.drop_hidden_3, \n#                      self.drop_hidden_4, self.drop_hidden_5, self.drop_hidden_6,\n#                      self.activation, False)\n        \n        py_x_drop = model(X, self.params, \n                     self.drop_input, self.drop_hidden, self.activation, True)\n    \n        py_x = model(X, self.params, \n                     self.drop_input, self.drop_hidden, self.activation, False)\n\n        y_x = T.argmax(py_x, axis=1)\n\n        # cost = T.mean(T.nnet.categorical_crossentropy(py_x, Y))\n        cost = multinominal_cross_entropy(py_x_drop, Y) \n        \n#         self.params = [ w_h_1, b_1, \n#                        w_h_2, b_2, \n#                        w_h_3, b_3, \n#                        w_h_4, b_4,\n#                        w_h_5, b_5,\n#                        w_h_6, b_6,\n#                        w_o, b_o]\n        \n        updates = momentum(cost, self.params, self.update_momentum, self.update_learning_rate, self.max_norm)\n\n        # Compile expressions to functions\n        self.train = theano.function(inputs=[X, Y], outputs=[cost], \n                                updates=updates, allow_input_downcast=True, name = \"train\")\n        self.predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True, name = \"predict\")\n\n        \n    def fit(self, x, y):\n        X_train, X_test, y_train, y_test = cv.train_test_split(x, y, test_size= self.eval_size)\n        yy = np.array(map((lambda x: np.argmax(x)), y_test))\n        \n        if any([x.op.__class__.__name__ in ['Gemv', 'CGemv', 'Gemm', 'CGemm'] for x in\n        self.train.maker.fgraph.toposort()]):\n            print 'Used the cpu'\n        elif any([x.op.__class__.__name__ in ['GpuGemm', 'GpuGemv'] for x in\n                  self.train.maker.fgraph.toposort()]):\n            print 'Used the gpu'\n        else:\n            print 'ERROR, not able to tell if theano used the cpu or the gpu'\n            print self.train.maker.fgraph.toposort()\n            \n        print \" \"\n        print \"start training!!!!\"\n        print \" \"\n            \n        epochs = 0\n        for i in range(self.max_epochs):\n            epochs +=1\n            t0 = time()\n            \n            # To do: modify a function of mini batch and using random\n            for start, end in zip(range(0, len(X_train), self.batch), range(self.batch, len(X_train), self.batch)):\n                err = self.train(X_train[start:end], y_train[start:end])\n            \n            score = accuracy_score(yy, self.predict(X_test))\n            self.train_history_.append({\"epoch\":epochs, \"err\": err, \"score\":score})\n            \n            print 'epoch {0} : err = {1}, score = {2}, time ={3} s'.format(epochs, err, score, time() - t0)\n            \n            if np.isnan(err):\n                break\n            \n            if epochs != 1:\n                new_lr = floatX(self.lr[epochs - 1])\n                self.update_learning_rate.set_value(new_lr)\n                if epochs <= self.up_momentum['epoch']:\n                    new_mm = floatX(self.mm[epochs - 1])\n                    self.update_momentum.set_value(new_mm)\n                    print \"momentum update: \", new_mm\n                \n            current_score = self.train_history_[-1]['score']\n            current_epoch = self.train_history_[-1]['epoch']\n            if current_score > self.best_valid:\n                self.best_valid = current_score\n                self.best_valid_epoch = current_epoch\n                self.best_weights = [w.get_value() for w in self.params]\n            elif self.best_valid_epoch + self.patience <= current_epoch:\n                print \"\"\n                print \"Early stopping.\"\n                print self.best_valid_epoch,self.best_valid\n                print \"Best valid score {:.6f} at epoch {}.\".format(self.best_valid, self.best_valid_epoch)\n                \n                for qq in range (len(self.params)):\n                    self.params[qq].set_value( self.best_weights[qq] )\n                break\n\n\n    def prediction(self, x):\n        return self.predict(x)\n\n    def outputCSV(self, wfilename, test_data, mapd): # read dictionary for id \n        test_results = []\n                \n        for xid, xdata in test_data.iteritems():\n            test_results.append( (xid, self.predict(xdata.T)) ) \n            \n        f = open(wfilename, 'w+')\n        f.write(\"Id,Prediction\\n\")\n        for xid, y in test_results:\n            f.write(\"{0},{1}\".format(xid, mapd[y[0]]))\n        f.close()\n        print \"MISSION COMPLETE\"\n    \n    ", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "# Start training!", "cell_type": "markdown", "metadata": {}}, {"source": " ## Data reading", "cell_type": "markdown", "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": "### Data Processing \n\ndata_ratio = 1 # the input we use ( to put more efford on improve parameter)\n\nX_train, X_test, y_train, y_test, test_data, result_mapping = Dataset(\"mfcc\", data_ratio)\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "slide ratio :  1012340\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 23, "cell_type": "code", "source": "(np.array(y_train)).shape\n# y_train = np.array(y_train)\n# ((y_train.flatten())).reshape(len(y_train.flatten())/48,48).astype(np.float32)\n# y_train[0].flatten()", "outputs": [{"execution_count": 23, "output_type": "execute_result", "data": {"text/plain": "(1012340, 48)"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "(np.array(X_train)).shape\n# X_train = np.array(X_train)\n# X_train[0].shape\n# (X_train.flatten()).reshape(1012340,39)[0]", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "(np.array(test_data)).shape\n# len(np.array(test_data.values()[0]))\n# np.array(test_data.values()).flatten()\n\n# test_data = np.array(test_data.values())\n# np.array(((test_data.flatten()))).reshape(len(test_data.flatten())/39,39).shape\n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "## Train model!", "cell_type": "markdown", "metadata": {}}, {"execution_count": 69, "cell_type": "code", "source": "net5 = DNN(\n    input_shape=(128,39), \n    hidden_layer=[300, 300, 300, 300], # maximum size to 6 layer only\n    drop_input=0, # usually is 0.2\n    drop_hidden=[0.2,0.2,0.2,0.2], # maximum size to 6 layer only, usually use 0.5 \n    activation = \"relu\", # relu, sigmoid\n    batch=128, \n    max_epochs=600, \n    eval_size=0.1, \n    output_num_units=48, \n    up_learning_rate = {'start':0.1, 'stop':0.0001}, \n    up_momentum = {'start':0.8, 'stop':0.9, 'epoch':40}, # only 40 epochs, and after 40 epochs, use 0.9\n    patience=100,\n    max_norm = 4 # need to tune, maybe start from 3 or 4\n)\n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 12, "cell_type": "code", "source": "# net5 = DNN(\n#     input_shape=(128,39), \n#     hidden_layer_1=256, \n#     hidden_layer_2=256, \n#     hidden_layer_3=256, \n#     hidden_layer_4=256, \n#     hidden_layer_5=256, \n#     hidden_layer_6=256, \n#     activation = \"sigmoid\",\n#     drop_input=0.2, \n#     drop_hidden_1=0.5,\n#     drop_hidden_2=0.5,\n#     drop_hidden_3=0.5,\n#     drop_hidden_4=0.5,\n#     drop_hidden_5=0.5,\n#     drop_hidden_6=0.5,\n#     batch=128, \n#     max_epochs=1000, \n#     eval_size=0.1, \n#     output_num_units=48, \n#     up_learning_rate = {'start':0.03, 'stop':0.0001}, \n#     up_momentum = {'start':0.9, 'stop':0.9},\n#     patience=100\n# )", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "%time net5.fit(X_train, y_train)", "outputs": [], "metadata": {"scrolled": false, "collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "net5.train_history_", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "## Save model!!", "cell_type": "markdown", "metadata": {}}, {"execution_count": 52, "cell_type": "code", "source": "import sys\nsys.setrecursionlimit(10000)\n\nimport cPickle as pickle\nwith open('net5.pickle', 'wb') as f:\n    pickle.dump(net5, f, -1)\n", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"source": "## Load model", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import cPickle as pickle\n\nwith open('./net5.pickle', 'rb') as f:\n    net5 = pickle.load(f)\n", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"source": "## validation result", "cell_type": "markdown", "metadata": {}}, {"execution_count": 53, "cell_type": "code", "source": "y_pred = net5.prediction(X_test)\n\nyyy = np.array(map((lambda x: np.argmax(x)), y_test))\n\nprint metrics.classification_report((yyy), (y_pred))\nprint accuracy_score(yyy, y_pred)\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "             precision    recall  f1-score   support\n\n          0       0.51      0.58      0.54      2472\n          1       0.56      0.63      0.59      2808\n          2       0.35      0.35      0.35      1751\n          3       0.57      0.49      0.53      2249\n          4       0.47      0.38      0.42      1097\n          5       0.39      0.29      0.34      1873\n          6       0.62      0.59      0.61      2767\n          7       0.64      0.54      0.58       986\n          8       0.47      0.42      0.44       742\n          9       0.69      0.70      0.69      6786\n         10       0.49      0.36      0.41      1169\n         11       0.55      0.41      0.47      1408\n         12       0.51      0.60      0.55       846\n         13       0.44      0.40      0.42      2678\n         14       0.47      0.38      0.42       972\n         15       0.34      0.16      0.21       557\n         16       0.54      0.26      0.35       411\n         17       0.70      0.66      0.68      3821\n         18       0.52      0.67      0.58      2686\n         19       0.68      0.70      0.69      2362\n         20       0.60      0.43      0.50       633\n         21       0.58      0.56      0.57      1418\n         22       0.44      0.27      0.34      2879\n         23       0.41      0.44      0.43      3575\n         24       0.67      0.71      0.69      4457\n         25       0.59      0.44      0.50       815\n         26       0.68      0.74      0.71      2537\n         27       0.59      0.60      0.60      3387\n         28       0.60      0.72      0.65      2840\n         29       0.52      0.42      0.46      1028\n         30       0.60      0.65      0.62      4471\n         31       0.48      0.55      0.51      1951\n         32       0.47      0.27      0.34       464\n         33       0.59      0.63      0.61      1760\n         34       0.62      0.64      0.63      2990\n         35       0.65      0.71      0.67      1687\n         36       0.80      0.92      0.86     11714\n         37       0.69      0.80      0.74      7121\n         38       0.30      0.10      0.15       657\n         39       0.58      0.57      0.57      2409\n         40       0.27      0.09      0.13       338\n         41       0.55      0.54      0.54      1781\n         42       0.61      0.60      0.61      3509\n         43       0.57      0.46      0.51      1323\n         44       0.66      0.72      0.69      1768\n         45       0.54      0.49      0.51       764\n         46       0.51      0.29      0.37       142\n         47       0.61      0.48      0.54      3624\n\navg / total       0.60      0.61      0.60    112483\n\n0.612563676289\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "# write predict file", "cell_type": "markdown", "metadata": {}}, {"execution_count": 54, "cell_type": "code", "source": "net5.outputCSV(\"net5_predict.csv\", test_data, result_mapping)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "MISSION COMPLETE\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.9", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}