{"nbformat_minor": 0, "cells": [{"source": "# Theano version of simple DNN version 7\n\n***What this version done ***\n1. Adjust dropout\n2. Play MFCC and FBank\n3. Data collect\n\n***Todo***\n- Add [RBM](http://deeplearning.net/tutorial/rbm.html)\n- RBM http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/\n- RBM https://class.coursera.org/neuralnets-2012-001/lecture\n- RBM https://github.com/lmjohns3/py-rbm/blob/master/lmj/rbm.py\n- RBM http://imonad.com/rbm/restricted-boltzmann-machine/\n2. ~~Add other method -- nesterov_momentum, relu, tanh~~\n3. debug auto adjust since we just linearly divide by max_epoch for every epoch\n4. [Add keyboardinterrupt exception then save the model](http://stackoverflow.com/questions/21120947/catching-keyboardinterrupt-in-python-during-program-shutdown)\n5. Add unsupervised learning method http://www.csri.utoronto.ca/~hinton/absps/googlerectified.pdf\n6. Change data to filter-bank\n7. ~~fix X.dot(w)+b~~\n8. ~~Check dropout method problem~~\n9. Add maxout network\n- ~~Add regularization term to every hidden layer~~\n- Add regularization to cost function\n- preprocess data with normalize and experiment the paper result       [tool](http://scikit-learn.org/stable/modules/preprocessing.html) \n> The open source Kaldi toolkit (Povey et al., 2011) was used to preprocess the data into log-\nfilter banks. A monophone system was trained to do a forced alignment and to get labels for\nspeech frames. Dropout neural networks were trained on windows of 21 consecutive frames\nto predict the label of the central frame. No speaker dependent operations were performed.\n***The inputs were mean centered and normalized to have unit variance***.\n\n- What hinton to for their project include preprocessing http://www.cs.toronto.edu/~nitish/deepnet/\n-------\n\n***Tune variable***\n1. Change hidden layer size\n2. Change dropout ratio\n3. Change activation function\n4. [Weight initializate](http://deeplearning.net/tutorial/mlp.html#weight-initialization)\n5. > We used probability of retention p = 0.8 in the input layers and 0.5 in the hidden layers.\nMax-norm constraint with c = 4 was used in all the layers. A momentum of 0.95 with a\nhigh learning rate of 0.1 was used. The learning rate was decayed as $\\epsilon_0 \\frac{1}{(1 + t/T)}$, from [Dropout: A Simple Way to Prevent Neural Networks from\nOverfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n\n***Knowledge need to fill the gap***\n1. Numpy broadcast http://wiki.scipy.org/EricsBroadcastingDoc\n2. Rewrite deep equation\n\n***Theano need to read and understand***\n- Scan (for loop in gpu) function http://deeplearning.net/software/theano/library/scan.html\n- Plot weight http://deeplearning.net/tutorial/utilities.html#how-to-plot\n\n***Question***\n1. Why dont need a bias term b in the equation or even initialize?\n2. ~~Once apply dropout, the result is strange~~ [quora](http://www.quora.com/How-would-you-implement-drop-out-in-a-deep-neural-network) [github](https://github.com/mdenil/dropout/blob/master/mlp.py#L261)\n3. Softmax weight divide by 2 ??\n\n***Github***\n- https://gist.github.com/SnippyHolloW/8a0f820261926e2f41cc\n- https://github.com/dnouri/nolearn/blob/master/nolearn/lasagne.py\n- https://github.com/benanne/Lasagne/blob/master/lasagne/nonlinearities.py\n- https://github.com/nitishsrivastava/deepnet/tree/master/deepnet\n\n***Paper***\n1. [Improving neural networks by preventing\nco-adaptation of feature detectors](http://arxiv.org/pdf/1207.0580.pdf)\n> Visible biases were initialized to zero and weights to random numbers sampled from a zeromean\nnormal distribution with standard deviation 0.01. The variance of each visible unit was\nset to 1.0 and not learned. Learning was done by minimizing Contrastive Divergence. Momentum\nwas used to speed up learning. Momentum started at 0.5 and was increased linearly to 0.9\nover 20 epochs. A learning rate of 0.001 on the average gradient was used (which was then\nmultiplied by 1-momentum). An L2 weight decay of 0.001 was used. The model was trained\nfor 100 epochs.\n\n> The pretrained RBMs were used to initialize the weights in a neural network. The network\nwas then finetuned with dropout-backpropagation. Momentum was increased from 0.5 to 0.9\nlinearly over 10 epochs. A small constant learning rate of 1.0 was used (applied to the average\ngradient on a minibatch). All other hyperparameters are the same as for MNIST dropout finetuning.\nThe model needs to be run for about 200 epochs to converge. The same network was\nalso finetuned with standard backpropagation using a smaller learning rate of 0.1, keeping all\nother hyperparameters\n\n2. [Dropout: A Simple Way to Prevent Neural Networks from\nOverfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n3. [Maxout Network](http://arxiv.org/pdf/1302.4389v4.pdf)\n\n***References***\n0. [Ipython Markdown](http://nbviewer.ipython.org/github/twistedhardware/mltutorial/blob/master/notebooks/IPython-Tutorial/2%20-%20Markdown%20%26%20LATEX.ipynb)\n- Dropout, Hinton http://arxiv.org/pdf/1207.0580.pdf\n- Dropout, Pratical guide http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf\n- Dropout, MaxOutnetwork http://arxiv.org/pdf/1302.4389v4.pdf\n- [Several Deep](http://snippyhollow.github.io/blog/2014/08/09/so-you-wanna-try-deep-learning/) -- Dropout, you will see improvement only on large-enough networks (> 1000 units / layer, > 3-4 layers **[example](https://gist.github.com/SnippyHolloW/8a0f820261926e2f41cc)**\n- Using cudamat implementation deep: expert github https://github.com/nitishsrivastava/deepnet\n2. Tutorial http://neuralnetworksanddeeplearning.com/chap3.html\n3. http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6296526&searchWithin%3Dp_Authors%3A.QT.Hinton%2C+G..QT.%26searchWithin%3Dp_Author_Ids%3A37270925500\n4. http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5704567&searchWithin%3Dp_Authors%3A.QT.Hinton%2C+G..QT.%26searchWithin%3Dp_Author_Ids%3A37270925500%26sortType%3Ddesc_p_Publication_Year\n5. Kaldi, acostic preprocess http://kaldi.sourceforge.net/", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "import theano\nfrom theano import tensor as T\nfrom theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\nimport numpy as np\n\nfrom sklearn import cross_validation as cv\nfrom sklearn import metrics\nfrom sklearn import grid_search as gs\nfrom sklearn.metrics import accuracy_score\n# from sklearn.base import BaseEstimator\nfrom itertools import repeat\n\nfrom time import time", "outputs": [{"output_type": "stream", "name": "stderr", "text": "Using gpu device 0: GeForce GTX 980\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "## Data preprocess", "cell_type": "markdown", "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": "### DataProcessing \n\n'''\n[ INPUT  ] : model { wav, NFCC, ... }\n[ OUTPUT ] : (Training_data, Validation_data, Testing_Data)\n\n- Training_data : x, y { .astype(np.float32)\n- Validation_data : x, y { .astype(np.float32)\n- Testing_Data : ( x, id )\n\n-- x format : \n---- NFCC : [ 39-vector ]\n---- FBank : [ 69-vector ]\n---- quadmfcc : [ 39*(1+1+1) - vector ] !!! Wrong implementation\n---- hexamfcc : [ 39*(4+1+4) - vector ]\n---- hexaFbank : [ 69*(4+1+4) - vector ]\n\n-- y format :\n\n---- Now we use 39-phonemes for all : [ 0 0 0 ... 1 .... 0 0 0 ]  as a number of 39\n---- But answer we get from ./label is [ 48-vector]\n'''\n\ndef Dataset ( model, dratio ) :\n    trains = {}\n    tests_data = {}\n    phones_mapping = {} # {48} to realNumber\n    result_mapping = {} # {48} to {39}\n    \n    training_inputs = []\n    training_result = []\n    \n    # MAPPING 48 to number ( we map 48 to 39 later )\n    with open('./MLDS_HW1_RELEASE_v1/phones/48_39.map') as f:    \n        i = 0\n        for lines in f :\n            phones = lines.split('\\t')\n            phones_mapping[phones[0]] = i\n            i += 1\n    \n    with open('./MLDS_HW1_RELEASE_v1/phones/48_39.map') as f:    \n        for lines in f :\n            phones = lines.split('\\t')\n            result_mapping[ phones_mapping[phones[0]] ] = phones[1]\n            \n# MFCC -------------------------------------------------------------------------------------------------------\n    if model == \"mfcc\":\n        # TRAINING X(INPUT)\n        with open('./MLDS_HW1_RELEASE_v1/mfcc/train.ark') as f:\n            for lines in f :\n                frames = lines.split(' ')\n                frame2float = [ float(x) for x in frames[1:] ]\n                trains[frames[0]] = frame2float\n        \n        \n        # TRAINING Y(OUTPUT)\n        with open('./MLDS_HW1_RELEASE_v1/label/train.lab') as f:\n            for lines in f :\n                labels = lines.split(',')\n                training_inputs.append( np.reshape(trains.get(labels[0]), 39 ) )\n                training_result.append( vectorized_result(phones_mapping[labels[1].rstrip('\\n')] ) )\n        \n        # 10% for validation\n        X_train, X_test, y_train, y_test = cv.train_test_split(training_inputs, training_result, test_size=0.1)        \n        y_train = (np.array(y_train)).astype(np.float32)        \n        X_train = (np.array(X_train)).astype(np.float32)\n        y_test = (np.array(y_test)).astype(np.float32)\n        X_test = (np.array(X_test)).astype(np.float32)\n          \n        # Testing data\n        print \"Now we process test data\"\n        with open('./MLDS_HW1_RELEASE_v1/mfcc/test.ark') as f:\n            for lines in f :\n                frames = lines.split(' ')\n                tests_data[frames[0]] = np.reshape([ float(x) for x in frames[1:] ], (39, 1) )        \n                \n# FBANL -------------------------------------------------------------------------------------------------------\n    elif model == \"fbank\":\n        # TRAINING X(INPUT)\n        with open('./MLDS_HW1_RELEASE_v1/fbank/train.ark') as f:\n            for lines in f :\n                frames = lines.split(' ')\n                frame2float = [ float(x) for x in frames[1:] ]\n                trains[frames[0]] = frame2float\n        \n        # TRAINING Y(OUTPUT)\n        with open('./MLDS_HW1_RELEASE_v1/label/train.lab') as f:\n            for lines in f :\n                labels = lines.split(',')\n                training_inputs.append( np.reshape(trains.get(labels[0]), 69 ) )\n                training_result.append( vectorized_result(phones_mapping[labels[1].rstrip('\\n')] ) )\n        \n    \n        # 10% for validation\n        X_train, X_test, y_train, y_test = cv.train_test_split(training_inputs, training_result, test_size=0.1)\n        y_train = (np.array(y_train)).astype(np.float32)\n        X_train = (np.array(X_train)).astype(np.float32)\n        y_test = (np.array(y_test)).astype(np.float32)\n        X_test = (np.array(X_test)).astype(np.float32)\n            \n        print \"Now we process test data\"\n        with open('./MLDS_HW1_RELEASE_v1/fbank/test.ark') as f:\n            for lines in f :\n                frames = lines.split(' ')\n                tests_data[frames[0]] = np.reshape([ float(x) for x in frames[1:] ], (69, 1) )        \n\n# quadmfcc -------------------------------------------------------------------------------------------------------\n    elif model == \"quadmfcc\":\n        # TRAINING X(INPUT)\n        monoTrains = {}\n        with open('./MLDS_HW1_RELEASE_v1/mfcc/train.ark') as f:\n            for lines in f :\n                frames = lines.split(' ')\n                frame2float = [ float(x) for x in frames[1:] ]\n                monoTrains[frames[0]] = frame2float\n\n        zeroFrame = list(repeat(0, 39))\n        prevframeL1 = [ float(x) for x in zeroFrame[:] ]\n        prevframeL2 = [ float(x) for x in zeroFrame[:] ]\n        prevframeR1 = [ float(x) for x in zeroFrame[:] ]\n        prevframeR2 = [ float(x) for x in zeroFrame[:] ]\n        blankframe  = [ float(x) for x in zeroFrame[:] ]\n        # L2 L1 S R1 R2\n        frameNameL1 = \"\"\n        frameNameL2 = \"\"\n        frameNameR1 = \"\"\n        frameNameR2 = \"\"\n        \n        for sample in monoTrains:\n            frames_info = sample.split('_')\n            frameNameL2 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])-2))\n            frameNameL1 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])-1))\n            frameNameR1 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])+1))\n            frameNameR2 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])+2))           \n            if   frames_info[2] == \"1\":\n                trains[sample] = blankframe + blankframe + monoTrains[sample] + monoTrains[frameNameR1] + monoTrains[frameNameR2]\n            elif frames_info[2] == \"2\":\n                trains[sample] = blankframe + monoTrains[frameNameL1] + monoTrains[sample] + monoTrains[frameNameR1] + monoTrains[frameNameR2]\n            elif monoTrains.get(frameNameR1) is None:\n                trains[sample] = monoTrains[frameNameL2] + monoTrains[frameNameL1] + monoTrains[sample] + blankframe + blankframe\n            elif monoTrains.get(frameNameR2) is None:\n                trains[sample] = monoTrains[frameNameL2] + monoTrains[frameNameL1] + monoTrains[sample] + monoTrains[frameNameR1] + blankframe    \n            else:\n                trains[sample] = monoTrains[frameNameL2] + monoTrains[frameNameL1] + monoTrains[sample] + monoTrains[frameNameR1] + monoTrains[frameNameR2]    \n        monoTrains.clear()\n        \n        # TRAINING Y(OUTPUT)\n        with open('./MLDS_HW1_RELEASE_v1/label/train.lab') as f:\n            for lines in f :\n                labels = lines.split(',')\n                # trains[labels[0]].append(labels[1])\n                training_inputs.append( np.reshape(trains.get(labels[0]), (117, 1) ) )\n                training_result.append( vectorized_result(phones_mapping[labels[1].rstrip('\\n')] ) )\n        \n        # 10% for validation\n        X_train, X_test, y_train, y_test = cv.train_test_split(training_inputs, training_result, test_size=0.1)\n        y_train = (np.array(y_train)).astype(np.float32)\n        X_train = (np.array(X_train)).astype(np.float32)\n        y_test = (np.array(y_test)).astype(np.float32)\n        X_test = (np.array(X_test)).astype(np.float32)\n            \n        # Testing data\n        print \"Now we process test data\"\n        with open('./MLDS_HW1_RELEASE_v1/mfcc/test.ark') as f:\n            for lines in f :\n                frames = lines.split(' ')\n                tests_data[frames[0]] = np.reshape([ float(x) for x in frames[1:] ], (39, 1) )   \n    \n\n# haxamfcc -------------------------------------------------------------------------------------------------------\n    elif model == \"hexamfcc\": \n        # TRAINING X(INPUT)\n        monoTrains = {}\n        with open('./MLDS_HW1_RELEASE_v1/mfcc/train.ark') as f:\n            for lines in f :\n                frames = lines.split(' ')\n                frame2float = [ float(x) for x in frames[1:] ]\n                # trains[frames[0]] = frames[1:]\n                monoTrains[frames[0]] = frame2float\n\n        zeroFrame = list(repeat(0, 39))\n        prevframeL1 = [ float(x) for x in zeroFrame[:] ]\n        prevframeL2 = [ float(x) for x in zeroFrame[:] ]\n        prevframeL3 = [ float(x) for x in zeroFrame[:] ]\n        prevframeL4 = [ float(x) for x in zeroFrame[:] ]\n        prevframeR1 = [ float(x) for x in zeroFrame[:] ]\n        prevframeR2 = [ float(x) for x in zeroFrame[:] ]\n        prevframeR3 = [ float(x) for x in zeroFrame[:] ]\n        prevframeR4 = [ float(x) for x in zeroFrame[:] ]\n        blankframe  = [ float(x) for x in zeroFrame[:] ]\n        # L4 L3 L2 L1 S R1 R2 R3 R4\n        frameNameL1 = \"\"\n        frameNameL2 = \"\"\n        frameNameL3 = \"\"\n        frameNameL4 = \"\"\n        frameNameR1 = \"\"\n        frameNameR2 = \"\"\n        frameNameR3 = \"\"\n        frameNameR4 = \"\"\n        \n        for sample in monoTrains:\n            frames_info = sample.split('_')\n            frameNameL4 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])-4))\n            frameNameL3 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])-3))\n            frameNameL2 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])-2))\n            frameNameL1 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])-1))\n            frameNameR1 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])+1))\n            frameNameR2 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])+2))  \n            frameNameR3 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])+3))\n            frameNameR4 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])+4))           \n            if   frames_info[2] == \"1\":\n                trains[sample] = blankframe + blankframe + blankframe + blankframe + monoTrains[sample] + monoTrains[frameNameR1] + monoTrains[frameNameR2] + monoTrains[frameNameR3] + monoTrains[frameNameR4]\n            elif frames_info[2] == \"2\":\n                trains[sample] = blankframe + blankframe + blankframe + monoTrains[frameNameL1] + monoTrains[sample] + monoTrains[frameNameR1] + monoTrains[frameNameR2] + monoTrains[frameNameR3] + monoTrains[frameNameR4]\n            elif frames_info[2] == \"3\":\n                trains[sample] = blankframe + blankframe + monoTrains[frameNameL2] + monoTrains[frameNameL1] + monoTrains[sample] + monoTrains[frameNameR1] + monoTrains[frameNameR2] + monoTrains[frameNameR3] + monoTrains[frameNameR4]\n            elif frames_info[2] == \"4\":\n                trains[sample] = blankframe + monoTrains[frameNameL3] + monoTrains[frameNameL2] + monoTrains[frameNameL1] + monoTrains[sample] + monoTrains[frameNameR1] + monoTrains[frameNameR2] + monoTrains[frameNameR3] + monoTrains[frameNameR4]\n            elif monoTrains.get(frameNameR1) is None:\n                trains[sample] = monoTrains[frameNameL4] + monoTrains[frameNameL3] + monoTrains[frameNameL2] + monoTrains[frameNameL1] + monoTrains[sample] + blankframe + blankframe + blankframe + blankframe\n            elif monoTrains.get(frameNameR2) is None:\n                trains[sample] = monoTrains[frameNameL4] + monoTrains[frameNameL3] + monoTrains[frameNameL2] + monoTrains[frameNameL1] + monoTrains[sample] + monoTrains[frameNameR1] + blankframe + blankframe + blankframe\n            elif monoTrains.get(frameNameR3) is None:\n                trains[sample] = monoTrains[frameNameL4] + monoTrains[frameNameL3] + monoTrains[frameNameL2] + monoTrains[frameNameL1] + monoTrains[sample] + monoTrains[frameNameR1] + monoTrains[frameNameR2] + blankframe + blankframe\n            elif monoTrains.get(frameNameR4) is None:\n                trains[sample] = monoTrains[frameNameL4] + monoTrains[frameNameL3] + monoTrains[frameNameL2] + monoTrains[frameNameL1] + monoTrains[sample] + monoTrains[frameNameR1] + monoTrains[frameNameR2] + monoTrains[frameNameR3] + blankframe\n            else:\n                trains[sample] = monoTrains[frameNameL4] + monoTrains[frameNameL3] + monoTrains[frameNameL2] + monoTrains[frameNameL1] + monoTrains[sample] + monoTrains[frameNameR1] + monoTrains[frameNameR2] + monoTrains[frameNameR3] + monoTrains[frameNameR4] \n        monoTrains.clear()\n        \n        # TRAINING Y(OUTPUT)\n        with open('./MLDS_HW1_RELEASE_v1/label/train.lab') as f:\n            for lines in f :\n                labels = lines.split(',')\n                # trains[labels[0]].append(labels[1])\n                training_inputs.append( np.reshape(trains.get(labels[0]), (351) ) )\n                training_result.append( vectorized_result(phones_mapping[labels[1].rstrip('\\n')] ) )\n        trains.clear()\n        \n        # 10% for validation\n        X_train, X_test, y_train, y_test = cv.train_test_split(training_inputs, training_result, test_size=0.1)\n        y_train = (np.array(y_train)).astype(np.float32)\n        X_train = (np.array(X_train)).astype(np.float32)\n        y_test = (np.array(y_test)).astype(np.float32)\n        X_test = (np.array(X_test)).astype(np.float32)\n        \n        # Testing data X\n        monoTest = {}\n        combineTest = {}\n        \n        print \"Now we process test data\"\n        with open('./MLDS_HW1_RELEASE_v1/mfcc/test.ark') as f:\n            for lines in f :\n                frames = lines.split(' ')\n                # monoTrains[frames[0]] = np.reshape([ float(x) for x in frames[1:] ], (39, 1) )\n                frame2float = [ float(x) for x in frames[1:] ]\n                monoTest[frames[0]] = frame2float\n        \n        zeroFrame = list(repeat(0, 39))\n        prevframeL1 = [ float(x) for x in zeroFrame[:] ]\n        prevframeL2 = [ float(x) for x in zeroFrame[:] ]\n        prevframeL3 = [ float(x) for x in zeroFrame[:] ]\n        prevframeL4 = [ float(x) for x in zeroFrame[:] ]\n        prevframeR1 = [ float(x) for x in zeroFrame[:] ]\n        prevframeR2 = [ float(x) for x in zeroFrame[:] ]\n        prevframeR3 = [ float(x) for x in zeroFrame[:] ]\n        prevframeR4 = [ float(x) for x in zeroFrame[:] ]\n        blankframe  = [ float(x) for x in zeroFrame[:] ]\n        # L4 L3 L2 L1 S R1 R2 R3 R4\n        frameNameL1 = \"\"\n        frameNameL2 = \"\"\n        frameNameR1 = \"\"\n        frameNameR2 = \"\"\n        frameNameL3 = \"\"\n        frameNameL4 = \"\"\n        frameNameR3 = \"\"\n        frameNameR4 = \"\"\n        \n        for sample in monoTest:\n            frames_info = sample.split('_')\n            frameNameL4 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])-4))\n            frameNameL3 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])-3))\n            frameNameL2 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])-2))\n            frameNameL1 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])-1))\n            frameNameR1 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])+1))\n            frameNameR2 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])+2))  \n            frameNameR3 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])+3))\n            frameNameR4 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])+4))           \n            if   frames_info[2] == \"1\":\n                combineTest[sample] = blankframe + blankframe + blankframe + blankframe + monoTest[sample] + monoTest[frameNameR1] + monoTest[frameNameR2] + monoTest[frameNameR3] + monoTest[frameNameR4]\n            elif frames_info[2] == \"2\":\n                combineTest[sample] = blankframe + blankframe + blankframe + monoTest[frameNameL1] + monoTest[sample] + monoTest[frameNameR1] + monoTest[frameNameR2] + monoTest[frameNameR3] + monoTest[frameNameR4]\n            elif frames_info[2] == \"3\":\n                combineTest[sample] = blankframe + blankframe + monoTest[frameNameL2] + monoTest[frameNameL1] + monoTest[sample] + monoTest[frameNameR1] + monoTest[frameNameR2] + monoTest[frameNameR3] + monoTest[frameNameR4]\n            elif frames_info[2] == \"4\":\n                combineTest[sample] = blankframe + monoTest[frameNameL3] + monoTest[frameNameL2] + monoTest[frameNameL1] + monoTest[sample] + monoTest[frameNameR1] + monoTest[frameNameR2] + monoTest[frameNameR3] + monoTest[frameNameR4]\n            elif monoTest.get(frameNameR1) is None:\n                combineTest[sample] = monoTest[frameNameL4] + monoTest[frameNameL3] + monoTest[frameNameL2] + monoTest[frameNameL1] + monoTest[sample] + blankframe + blankframe + blankframe + blankframe\n            elif monoTest.get(frameNameR2) is None:\n                combineTest[sample] = monoTest[frameNameL4] + monoTest[frameNameL3] + monoTest[frameNameL2] + monoTest[frameNameL1] + monoTest[sample] + monoTest[frameNameR1] + blankframe + blankframe + blankframe\n            elif monoTest.get(frameNameR3) is None:\n                combineTest[sample] = monoTest[frameNameL4] + monoTest[frameNameL3] + monoTest[frameNameL2] + monoTest[frameNameL1] + monoTest[sample] + monoTest[frameNameR1] + monoTest[frameNameR2] + blankframe + blankframe\n            elif monoTest.get(frameNameR4) is None:\n                combineTest[sample] = monoTest[frameNameL4] + monoTest[frameNameL3] + monoTest[frameNameL2] + monoTest[frameNameL1] + monoTest[sample] + monoTest[frameNameR1] + monoTest[frameNameR2] + monoTest[frameNameR3] + blankframe\n            else:\n                combineTest[sample] = monoTest[frameNameL4] + monoTest[frameNameL3] + monoTest[frameNameL2] + monoTest[frameNameL1] + monoTest[sample] + monoTest[frameNameR1] + monoTest[frameNameR2] + monoTest[frameNameR3] + monoTest[frameNameR4] \n        monoTest.clear()\n        \n        for keys in combineTest:\n            tests_data[keys] = np.reshape(combineTest[keys], (351, 1) )\n    \n# hexafbank -------------------------------------------------------------------------------------------------------\n    elif model == \"hexafbank\": \n        # TRAINING X(INPUT)\n        monoTrains = {}\n        with open('./MLDS_HW1_RELEASE_v1/fbank/train.ark') as f:\n            for lines in f :\n                frames = lines.split(' ')\n                frame2float = [ float(x) for x in frames[1:] ]\n                monoTrains[frames[0]] = frame2float\n\n        zeroFrame = list(repeat(0, 69))\n        prevframeL1 = [ float(x) for x in zeroFrame[:] ]\n        prevframeL2 = [ float(x) for x in zeroFrame[:] ]\n        prevframeL3 = [ float(x) for x in zeroFrame[:] ]\n        prevframeL4 = [ float(x) for x in zeroFrame[:] ]\n        prevframeR1 = [ float(x) for x in zeroFrame[:] ]\n        prevframeR2 = [ float(x) for x in zeroFrame[:] ]\n        prevframeR3 = [ float(x) for x in zeroFrame[:] ]\n        prevframeR4 = [ float(x) for x in zeroFrame[:] ]\n        blankframe  = [ float(x) for x in zeroFrame[:] ]\n        # L4 L3 L2 L1 S R1 R2 R3 R4\n        frameNameL1 = \"\"\n        frameNameL2 = \"\"\n        frameNameL3 = \"\"\n        frameNameL4 = \"\"\n        frameNameR1 = \"\"\n        frameNameR2 = \"\"\n        frameNameR3 = \"\"\n        frameNameR4 = \"\"\n        \n        for sample in monoTrains:\n            frames_info = sample.split('_')\n            frameNameL4 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])-4))\n            frameNameL3 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])-3))\n            frameNameL2 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])-2))\n            frameNameL1 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])-1))\n            frameNameR1 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])+1))\n            frameNameR2 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])+2))  \n            frameNameR3 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])+3))\n            frameNameR4 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])+4))           \n            if   frames_info[2] == \"1\":\n                trains[sample] = blankframe + blankframe + blankframe + blankframe + monoTrains[sample] + monoTrains[frameNameR1] + monoTrains[frameNameR2] + monoTrains[frameNameR3] + monoTrains[frameNameR4]\n            elif frames_info[2] == \"2\":\n                trains[sample] = blankframe + blankframe + blankframe + monoTrains[frameNameL1] + monoTrains[sample] + monoTrains[frameNameR1] + monoTrains[frameNameR2] + monoTrains[frameNameR3] + monoTrains[frameNameR4]\n            elif frames_info[2] == \"3\":\n                trains[sample] = blankframe + blankframe + monoTrains[frameNameL2] + monoTrains[frameNameL1] + monoTrains[sample] + monoTrains[frameNameR1] + monoTrains[frameNameR2] + monoTrains[frameNameR3] + monoTrains[frameNameR4]\n            elif frames_info[2] == \"4\":\n                trains[sample] = blankframe + monoTrains[frameNameL3] + monoTrains[frameNameL2] + monoTrains[frameNameL1] + monoTrains[sample] + monoTrains[frameNameR1] + monoTrains[frameNameR2] + monoTrains[frameNameR3] + monoTrains[frameNameR4]\n            elif monoTrains.get(frameNameR1) is None:\n                trains[sample] = monoTrains[frameNameL4] + monoTrains[frameNameL3] + monoTrains[frameNameL2] + monoTrains[frameNameL1] + monoTrains[sample] + blankframe + blankframe + blankframe + blankframe\n            elif monoTrains.get(frameNameR2) is None:\n                trains[sample] = monoTrains[frameNameL4] + monoTrains[frameNameL3] + monoTrains[frameNameL2] + monoTrains[frameNameL1] + monoTrains[sample] + monoTrains[frameNameR1] + blankframe + blankframe + blankframe\n            elif monoTrains.get(frameNameR3) is None:\n                trains[sample] = monoTrains[frameNameL4] + monoTrains[frameNameL3] + monoTrains[frameNameL2] + monoTrains[frameNameL1] + monoTrains[sample] + monoTrains[frameNameR1] + monoTrains[frameNameR2] + blankframe + blankframe\n            elif monoTrains.get(frameNameR4) is None:\n                trains[sample] = monoTrains[frameNameL4] + monoTrains[frameNameL3] + monoTrains[frameNameL2] + monoTrains[frameNameL1] + monoTrains[sample] + monoTrains[frameNameR1] + monoTrains[frameNameR2] + monoTrains[frameNameR3] + blankframe\n            else:\n                trains[sample] = monoTrains[frameNameL4] + monoTrains[frameNameL3] + monoTrains[frameNameL2] + monoTrains[frameNameL1] + monoTrains[sample] + monoTrains[frameNameR1] + monoTrains[frameNameR2] + monoTrains[frameNameR3] + monoTrains[frameNameR4] \n        monoTrains.clear()\n        \n        # TRAINING Y(OUTPUT)\n        with open('./MLDS_HW1_RELEASE_v1/label/train.lab') as f:\n            for lines in f :\n                labels = lines.split(',')\n                training_inputs.append( np.reshape(trains.get(labels[0]), 621 ) )\n                training_result.append( vectorized_result(phones_mapping[labels[1].rstrip('\\n')] ) )\n        trains.clear()\n        \n        # 10% for validation\n        X_train, X_test, y_train, y_test = cv.train_test_split(training_inputs, training_result, test_size=0.1)\n        y_train = (np.array(y_train)).astype(np.float32)\n        X_train = (np.array(X_train)).astype(np.float32)\n        y_test = (np.array(y_test)).astype(np.float32)\n        X_test = (np.array(X_test)).astype(np.float32)\n        \n        # Testing data X\n        monoTest = {}\n        combineTest = {}\n        \n        print \"Now we process test data\"\n        with open('./MLDS_HW1_RELEASE_v1/fbank/test.ark') as f:\n            for lines in f :\n                frames = lines.split(' ')\n                # monoTrains[frames[0]] = np.reshape([ float(x) for x in frames[1:] ], (39, 1) )\n                frame2float = [ float(x) for x in frames[1:] ]\n                monoTest[frames[0]] = frame2float\n        \n        zeroFrame = list(repeat(0, 69))\n        prevframeL1 = [ float(x) for x in zeroFrame[:] ]\n        prevframeL2 = [ float(x) for x in zeroFrame[:] ]\n        prevframeL3 = [ float(x) for x in zeroFrame[:] ]\n        prevframeL4 = [ float(x) for x in zeroFrame[:] ]\n        prevframeR1 = [ float(x) for x in zeroFrame[:] ]\n        prevframeR2 = [ float(x) for x in zeroFrame[:] ]\n        prevframeR3 = [ float(x) for x in zeroFrame[:] ]\n        prevframeR4 = [ float(x) for x in zeroFrame[:] ]\n        blankframe  = [ float(x) for x in zeroFrame[:] ]\n        # L4 L3 L2 L1 S R1 R2 R3 R4\n        frameNameL1 = \"\"\n        frameNameL2 = \"\"\n        frameNameR1 = \"\"\n        frameNameR2 = \"\"\n        frameNameL3 = \"\"\n        frameNameL4 = \"\"\n        frameNameR3 = \"\"\n        frameNameR4 = \"\"\n        \n        for sample in monoTest:\n            frames_info = sample.split('_')\n            frameNameL4 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])-4))\n            frameNameL3 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])-3))\n            frameNameL2 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])-2))\n            frameNameL1 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])-1))\n            frameNameR1 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])+1))\n            frameNameR2 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])+2))  \n            frameNameR3 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])+3))\n            frameNameR4 = frames_info[0] + \"_\" + frames_info[1] + \"_\" + str( (int(frames_info[2])+4))           \n            if   frames_info[2] == \"1\":\n                combineTest[sample] = blankframe + blankframe + blankframe + blankframe + monoTest[sample] + monoTest[frameNameR1] + monoTest[frameNameR2] + monoTest[frameNameR3] + monoTest[frameNameR4]\n            elif frames_info[2] == \"2\":\n                combineTest[sample] = blankframe + blankframe + blankframe + monoTest[frameNameL1] + monoTest[sample] + monoTest[frameNameR1] + monoTest[frameNameR2] + monoTest[frameNameR3] + monoTest[frameNameR4]\n            elif frames_info[2] == \"3\":\n                combineTest[sample] = blankframe + blankframe + monoTest[frameNameL2] + monoTest[frameNameL1] + monoTest[sample] + monoTest[frameNameR1] + monoTest[frameNameR2] + monoTest[frameNameR3] + monoTest[frameNameR4]\n            elif frames_info[2] == \"4\":\n                combineTest[sample] = blankframe + monoTest[frameNameL3] + monoTest[frameNameL2] + monoTest[frameNameL1] + monoTest[sample] + monoTest[frameNameR1] + monoTest[frameNameR2] + monoTest[frameNameR3] + monoTest[frameNameR4]\n            elif monoTest.get(frameNameR1) is None:\n                combineTest[sample] = monoTest[frameNameL4] + monoTest[frameNameL3] + monoTest[frameNameL2] + monoTest[frameNameL1] + monoTest[sample] + blankframe + blankframe + blankframe + blankframe\n            elif monoTest.get(frameNameR2) is None:\n                combineTest[sample] = monoTest[frameNameL4] + monoTest[frameNameL3] + monoTest[frameNameL2] + monoTest[frameNameL1] + monoTest[sample] + monoTest[frameNameR1] + blankframe + blankframe + blankframe\n            elif monoTest.get(frameNameR3) is None:\n                combineTest[sample] = monoTest[frameNameL4] + monoTest[frameNameL3] + monoTest[frameNameL2] + monoTest[frameNameL1] + monoTest[sample] + monoTest[frameNameR1] + monoTest[frameNameR2] + blankframe + blankframe\n            elif monoTest.get(frameNameR4) is None:\n                combineTest[sample] = monoTest[frameNameL4] + monoTest[frameNameL3] + monoTest[frameNameL2] + monoTest[frameNameL1] + monoTest[sample] + monoTest[frameNameR1] + monoTest[frameNameR2] + monoTest[frameNameR3] + blankframe\n            else:\n                combineTest[sample] = monoTest[frameNameL4] + monoTest[frameNameL3] + monoTest[frameNameL2] + monoTest[frameNameL1] + monoTest[sample] + monoTest[frameNameR1] + monoTest[frameNameR2] + monoTest[frameNameR3] + monoTest[frameNameR4] \n        monoTest.clear()\n        for keys in combineTest:\n            tests_data[keys] = np.reshape(combineTest[keys], (621, 1) )\n                \n    else:\n        print \"Not implement yet\"\n    \n    return (X_train, X_test, y_train, y_test, tests_data, result_mapping)\n                                       \ndef vectorized_result ( j ) :\n    e = np.zeros((48, 1))\n    e[j] = 1.0\n    return np.reshape( e, 48)\n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "## Model part\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "srng = RandomStreams(seed=2**30)\n\n# translate data to theano data type\ndef floatX(X):\n    return np.asarray(X, dtype=theano.config.floatX)\n\n# initialize weight by random\ndef init_weights(shape):\n    return theano.shared(floatX(np.random.randn(*shape) * 0.01))\n\ndef tanh(X):\n    return (1 + T.tanh(X / 2)) / 2\n\n# rectified linear unit\ndef relu(X):\n    # return T.maximum(X, 0.)\n    return (X + abs(X)) / 2.\n\ndef sigmoid(X):\n    return 1.0/(1.0+ T.exp(-X))\n\n# softmax\ndef softmax(X):\n    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n\ndef build_shared_zeros(shape):\n    \"\"\" Builds a theano shared variable filled with a zeros numpy array \"\"\"\n    return theano.shared(value=np.zeros(shape, dtype=theano.config.floatX),\n             borrow=True)\n\n# method provided by Hinton\ndef RMSprop(cost, params, lr=0.001, rho=0.9, epsilon=1e-6):\n    grads = T.grad(cost=cost, wrt=params)\n    updates = []\n    for p, g in zip(params, grads):\n        acc = theano.shared(p.get_value() * 0.)\n        acc_new = rho * acc + (1 - rho) * g ** 2\n        gradient_scaling = T.sqrt(acc_new + epsilon)\n        g = g / gradient_scaling\n        updates.append((acc, acc_new))\n        updates.append((p, p - lr * g))\n    return updates\n\n# momentum method\ndef momentum(loss, all_params, update_momentum, update_learning_rate, max_norm):\n    all_grads = theano.grad(loss, all_params)\n    updates = []\n\n    for param_i, grad_i in zip(all_params, all_grads):\n        mparam_i = theano.shared(np.zeros(param_i.get_value().shape,\n                                          dtype=theano.config.floatX),\n                                 broadcastable=param_i.broadcastable)\n        v = update_momentum * mparam_i - update_learning_rate * grad_i\n        updates.append((mparam_i, v))\n        \n        if max_norm != None:\n            W = param_i + v\n            col_norms = W.norm(2, axis=0)\n            desired_norms = T.clip(col_norms, 0, max_norm)\n            updates.append( ( param_i , (W * (desired_norms / (1e-6 + col_norms))) ) )\n        else:\n            updates.append((param_i, param_i + v))\n\n    return updates\n\ndef multinominal_cross_entropy(z, X):\n    \n    L = - T.sum(X * T.log(z) + (1 - X) * T.log(1 - z), axis=1)\n    loss = T.sum(L) / X.shape[0]\n    \n    return loss\n\ndef negative_log_likelihood_mean(z, X):\n    return -T.mean(T.log(z)[T.arange(X.shape[0]), X])\n\ndef negative_log_likelihood_sum(z, X):\n    return -T.sum(T.log(z)[T.arange(X.shape[0]), X])\n\n\n# dropout\n# https://github.com/mdenil/dropout/issues/6\n# def dropout(X, p=0.):\n#     if p > 0. and p < 1.:\n#         retain_prob = 1 - p\n#         X *= srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n#     return X\n\ndef dropout( layer, p=0.):\n# http://www.quora.com/How-would-you-implement-drop-out-in-a-deep-neural-network\n#     srng = theano.tensor.shared_randomstreams.RandomStreams(\n#             rng.randint(999999))\n    \n    if p > 0. and p < 1.:\n        # p=1-p because 1's indicate keep and p is prob of dropping\n        mask = srng.binomial(n=1, p=1-p, size=layer.shape)\n        # The cast is important because\n        # int * float32 = float64 which pulls things off the gpu\n        output = layer * T.cast(mask, theano.config.floatX)\n        return output\n    else:\n        return layer\n\ndef model(X, para, p_drop_input, p_drop_hidden, relu_bool, train):\n    ## assume 6 hidden  layers only\n    ## Softmax weight divide by 2??\n    for i in range (len(para)/2):\n        \n        if i is 0:\n            ######## Input layer\n            if train:\n                X = dropout(X, p_drop_input)\n            else:\n                para[i*2].set_value(para[i*2].get_value()*(1-p_drop_hidden[i-1]))\n                \n            if relu_bool:\n                h = relu(T.dot(X, para[i*2]) +para[i+1])\n            else:\n                h = sigmoid(T.dot(X, para[i*2]) + para[i+1])\n\n        elif i is 1:\n            ######## layer 1\n            if train:\n                h = dropout(h, p_drop_hidden[i-1])\n            else:\n                # h *= (1-p_drop_hidden[i-1])\n                para[i*2].set_value(para[i*2].get_value()*(1-p_drop_hidden[i-1]))\n            \n            if (i*2+2) is len(para): ## weight at least have 4, w_h_1, b_1, w_o, b_o\n                py_xx = softmax(T.dot(h, para[i*2])+ para[i*2+1])\n                break\n            \n            if relu_bool:\n                h2 = relu(T.dot(h, para[i*2] )+ para[i*2+1])\n            else: \n                h2 = sigmoid(T.dot(h, para[i*2] ) +para[i*2+1])\n        \n        elif i is 2:\n            ######## layer 2\n            if train:\n                h2 = dropout(h2, p_drop_hidden[i-1])\n            else:\n                # h2 *= (1-p_drop_hidden[i-1])\n                para[i*2].set_value(para[i*2].get_value()*(1-p_drop_hidden[i-1]))\n            \n            if (i*2+2) is len(para): ## weight at least have 6, w_h_1, b_1, w_o, b_o\n                py_xx = softmax(T.dot(h2, para[i*2])+ para[i*2+1])\n                break\n            \n            if relu_bool:\n                h3 = relu(T.dot(h2, para[i*2] )+ para[i*2+1])\n            else:\n                h3 = sigmoid(T.dot(h2, para[i*2] ) +para[i*2+1])\n                \n            \n        elif i is 3:\n            ######## layer 3\n            if train:\n                h3 = dropout(h3, p_drop_hidden[i-1])\n            else:\n                # h3 *= (1-p_drop_hidden[i-1])\n                para[i*2].set_value(para[i*2].get_value()*(1-p_drop_hidden[i-1]))\n            \n            if (i*2+2) is len(para): ## weight at least have 8, w_h_1, b_1, w_o, b_o\n                py_xx = softmax(T.dot(h3, para[i*2])+ para[i*2+1])\n                break\n            \n            if relu_bool:\n                h4 = relu(T.dot(h3, para[i*2] )+ para[i*2+1])\n            else:\n                h4 = sigmoid(T.dot(h3, para[i*2] ) +para[i*2+1])\n                \n                \n        elif i is 4:\n            ######## layer 4\n            if train:\n                h4 = dropout(h4, p_drop_hidden[i-1])\n            else:\n                # h4 *= (1-p_drop_hidden[i-1])\n                para[i*2].set_value(para[i*2].get_value()*(1-p_drop_hidden[i-1]))\n            \n            if (i*2+2) is len(para): ## weight at least have 10, w_h_1, b_1, w_o, b_o\n                py_xx = softmax(T.dot(h4, para[i*2])+ para[i*2+1])\n                break\n            \n            if relu_bool:\n                h5 = relu(T.dot(h4, para[i*2] )+ para[i*2+1])\n            else:\n                h5 = sigmoid(T.dot(h4, para[i*2] ) +para[i*2+1])\n                \n                \n        elif i is 5:\n            ######## layer 5\n            if train:\n                h5 = dropout(h5, p_drop_hidden[i-1])\n            else:\n                # h5 *= (1-p_drop_hidden[i-1])\n                para[i*2].set_value(para[i*2].get_value()*(1-p_drop_hidden[i-1]))\n            \n            if (i*2+2) is len(para): ## weight at least have 12, w_h_1, b_1, w_o, b_o\n                py_xx = softmax(T.dot(h5, para[i*2])+ para[i*2+1])\n                break\n            \n            if relu_bool:\n                h6 = relu(T.dot(h5, para[i*2] )+ para[i*2+1])\n            else:\n                h6 = sigmoid(T.dot(h5, para[i*2] ) +para[i*2+1])\n                \n        elif i is 6:\n            ######## layer 6\n            if train:\n                h6 = dropout(h6, p_drop_hidden[i-1])\n            else:\n                # h6 *= (1-p_drop_hidden[i-1])\n                para[i*2].set_value(para[i*2].get_value()*(1-p_drop_hidden[i-1]))\n            \n            if (i*2+2) is len(para): ## weight at least have 12, w_h_1, b_1, w_o, b_o\n                py_xx = softmax(T.dot(h6, para[i*2])+ para[i*2+1])\n                break\n            \n            if relu_bool:\n                h7 = relu(T.dot(h6, para[i*2] )+ para[i*2+1])\n            else:\n                h7 = sigmoid(T.dot(h6, para[i*2] ) +para[i*2+1])\n                \n    return py_xx\n    \n    \n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 12, "cell_type": "code", "source": "class DNN():    \n    \n    def __init__(self, input_shape, activation, hidden_layer,\n                 batch, max_epochs, eval_size, output_num_units,\n                 drop_input, drop_hidden,\n                 patience, up_learning_rate, up_momentum, max_norm):\n        \n        self.input_shape = input_shape\n        self.hidden_layer = hidden_layer\n#         self.hidden_layer_1 = hidden_layer_1\n#         self.hidden_layer_2 = hidden_layer_2\n#         self.hidden_layer_3 = hidden_layer_3\n        \n        if activation == \"relu\":\n            self.activation = True\n        else:\n            self.activation = False\n        \n        self.batch = batch\n        self.max_epochs = max_epochs\n        self.eval_size = eval_size\n        self.output_num_units = output_num_units\n        \n        self.up_learning_rate = up_learning_rate\n        self.up_momentum = up_momentum\n        \n        self.drop_input = drop_input\n        self.drop_hidden = drop_hidden\n#         self.drop_hidden_1 = drop_hidden_1\n#         self.drop_hidden_2 = drop_hidden_2\n#         self.drop_hidden_3 = drop_hidden_3\n        \n        self.patience = patience\n        self.best_valid = -np.inf\n        self.best_valid_epoch = 0\n        self.best_params = None\n        \n        self.max_norm = max_norm\n        \n        self.train_history_ = []\n\n        self.params = []\n        for i in range(len(hidden_layer)):\n            if i is 0:\n                w_h_1 = init_weights((self.input_shape[1], self.hidden_layer[i]))\n                b_1 = build_shared_zeros(self.hidden_layer[i])\n                self.params.append(w_h_1)\n                self.params.append(b_1)\n            elif i is 1:\n                w_h_2 = init_weights((self.hidden_layer[i-1], self.hidden_layer[i]))\n                b_2 = build_shared_zeros(self.hidden_layer[i])\n                self.params.append(w_h_2)\n                self.params.append(b_2)\n            elif i is 2:\n                w_h_3 = init_weights((self.hidden_layer[i-1], self.hidden_layer[i]))\n                b_3 = build_shared_zeros(self.hidden_layer[i])\n                self.params.append(w_h_3)\n                self.params.append(b_3)\n            elif i is 3:\n                w_h_4 = init_weights((self.hidden_layer[i-1], self.hidden_layer[i]))\n                b_4 = build_shared_zeros(self.hidden_layer[i])\n                self.params.append(w_h_4)\n                self.params.append(b_4)\n            elif i is 4:\n                w_h_5 = init_weights((self.hidden_layer[i-1], self.hidden_layer[i]))\n                b_5 = build_shared_zeros(self.hidden_layer[i])\n                self.params.append(w_h_5)\n                self.params.append(b_5)\n            elif i is 5:\n                w_h_6 = init_weights((self.hidden_layer[i-1], self.hidden_layer[i]))\n                b_6 = build_shared_zeros(self.hidden_layer[i])\n                self.params.append(w_h_6)\n                self.params.append(b_6)\n            elif i is 6:\n                w_h_7 = init_weights((self.hidden_layer[i-1], self.hidden_layer[i]))\n                b_7 = build_shared_zeros(self.hidden_layer[i])\n                self.params.append(w_h_7)\n                self.params.append(b_7)\n                \n            if (i+1) is len(hidden_layer):\n                w_o = init_weights((self.hidden_layer[i], self.output_num_units))\n                b_o = build_shared_zeros(self.output_num_units)\n                self.params.append(w_o)\n                self.params.append(b_o)\n                break\n\n            \n#         w_h_1 = init_weights((self.input_shape[1], self.hidden_layer_1))\n#         w_h_2 = init_weights((self.hidden_layer_1, self.hidden_layer_2))\n#         w_h_3 = init_weights((self.hidden_layer_2, self.hidden_layer_3))\n#         w_h_4 = init_weights((self.hidden_layer_3, self.hidden_layer_4))\n#         w_h_5 = init_weights((self.hidden_layer_4, self.hidden_layer_5))\n#         w_h_6 = init_weights((self.hidden_layer_5, self.hidden_layer_6))\n#         w_o = init_weights((self.hidden_layer_6, self.output_num_units))\n        \n#         b_1 = build_shared_zeros(self.hidden_layer_1)\n#         b_2 = build_shared_zeros(self.hidden_layer_2)\n#         b_3 = build_shared_zeros(self.hidden_layer_3)\n#         b_4 = build_shared_zeros(self.hidden_layer_4)\n#         b_5 = build_shared_zeros(self.hidden_layer_5)\n#         b_6 = build_shared_zeros(self.hidden_layer_6)\n#         b_o = build_shared_zeros(self.output_num_units)\n        \n        self.update_learning_rate= theano.shared(floatX( up_learning_rate['start'] ))\n        \n        ###################### \n        ## need to modify if using dropout\n        ######################\n        self.update_learning_rate.set_value(0.1)\n        ######################\n        \n        self.lr = np.linspace(up_learning_rate['start'], up_learning_rate['stop'], self.max_epochs)\n        # self.lr = np.linspace(up_learning_rate['start'], up_learning_rate['stop'], 20)\n        \n        self.update_momentum= theano.shared(floatX( up_momentum['start'] ))\n        # self.mm = np.linspace(up_momentum['start'], up_momentum['stop'], self.max_epochs)\n        self.mm = np.linspace(up_momentum['start'], up_momentum['stop'], up_momentum['epoch'])\n    \n        X = T.dmatrix()\n        Y = T.dmatrix()\n\n        # Construct Theano expression graph\n#         py_x_drop = model(X, \n#                      w_h_1,b_1,\n#                      w_h_2,b_2,\n#                      w_h_3,b_3, \n#                      w_h_4,b_4, \n#                      w_h_5,b_5, \n#                      w_h_6,b_6, \n#                      w_o, b_o,\n#                      self.drop_input, \n#                      self.drop_hidden_1, self.drop_hidden_2, self.drop_hidden_3, \n#                      self.drop_hidden_4, self.drop_hidden_5, self.drop_hidden_6,\n#                      self.activation, True)\n        \n#         py_x = model(X, \n#                      w_h_1,b_1,\n#                      w_h_2,b_2,\n#                      w_h_3,b_3, \n#                      w_h_4,b_4, \n#                      w_h_5,b_5, \n#                      w_h_6,b_6, \n#                      w_o, b_o,\n#                      self.drop_input, \n#                      self.drop_hidden_1, self.drop_hidden_2, self.drop_hidden_3, \n#                      self.drop_hidden_4, self.drop_hidden_5, self.drop_hidden_6,\n#                      self.activation, False)\n        \n        py_x_drop = model(X, self.params, \n                     self.drop_input, self.drop_hidden, self.activation, True)\n    \n        py_x = model(X, self.params, \n                     self.drop_input, self.drop_hidden, self.activation, False)\n\n        y_x = T.argmax(py_x, axis=1)\n\n        # cost = T.mean(T.nnet.categorical_crossentropy(py_x, Y))\n        cost = multinominal_cross_entropy(py_x_drop, Y) \n        \n#         self.params = [ w_h_1, b_1, \n#                        w_h_2, b_2, \n#                        w_h_3, b_3, \n#                        w_h_4, b_4,\n#                        w_h_5, b_5,\n#                        w_h_6, b_6,\n#                        w_o, b_o]\n        \n        updates = momentum(cost, self.params, self.update_momentum, self.update_learning_rate, self.max_norm)\n\n        # Compile expressions to functions\n        self.train = theano.function(inputs=[X, Y], outputs=[cost], \n                                updates=updates, allow_input_downcast=True, name = \"train\")\n        self.predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True, name = \"predict\")\n\n        \n    def fit(self, x, y):\n        X_train, X_test, y_train, y_test = cv.train_test_split(x, y, test_size= self.eval_size)\n        yy = np.array(map((lambda x: np.argmax(x)), y_test))\n        \n#         if any([x.op.__class__.__name__ in ['Gemv', 'CGemv', 'Gemm', 'CGemm'] for x in\n#         self.train.maker.fgraph.toposort()]):\n#             print 'Used the cpu'\n#         elif any([x.op.__class__.__name__ in ['GpuGemm', 'GpuGemv'] for x in\n#                   self.train.maker.fgraph.toposort()]):\n#             print 'Used the gpu'\n#         else:\n#             print 'ERROR, not able to tell if theano used the cpu or the gpu'\n#             print self.train.maker.fgraph.toposort()\n            \n        print \" \"\n        print \"start training!!!!\"\n        print \" \"\n            \n        epochs = 0\n        for i in range(self.max_epochs):\n            epochs +=1\n            t0 = time()\n            \n            # To do: modify a function of mini batch and using random\n            for start, end in zip(range(0, len(X_train), self.batch), range(self.batch, len(X_train), self.batch)):\n                err = self.train(X_train[start:end], y_train[start:end])\n            \n            score = accuracy_score(yy, self.predict(X_test))\n            self.train_history_.append({\"epoch\":epochs, \"err\": err, \"score\":score})\n            \n            print 'epoch {0} : err = {1}, score = {2}, time ={3} s'.format(epochs, err, score, time() - t0)\n            \n            if np.isnan(err):\n                for qq in range (len(self.params)):\n                    self.params[qq].set_value( self.best_params[qq] )\n                break\n            \n            if epochs != 1:\n#                 new_lr = floatX(self.lr[epochs - 1])\n#                 self.update_learning_rate.set_value(new_lr)\n                \n                ###################### \n                ## need to modify if using dropout\n                ######################\n                if epochs >= 3:  # <---- \n                    self.update_learning_rate.set_value(0.01)\n                    if epochs > 8:\n                        self.update_learning_rate.set_value(0.001)\n                        if epochs > 13:\n                            self.update_learning_rate.set_value(0.0001)\n                    \n                if epochs <= self.up_momentum['epoch']:\n                    new_mm = floatX(self.mm[epochs - 1])\n                    self.update_momentum.set_value(new_mm)\n#                     print \"momentum update: \", new_mm\n                \n            current_score = self.train_history_[-1]['score']\n            current_epoch = self.train_history_[-1]['epoch']\n            if current_score > self.best_valid:\n                self.best_valid = current_score\n                self.best_valid_epoch = current_epoch\n                self.best_params = [w.get_value() for w in self.params]\n            elif self.best_valid_epoch + self.patience <= current_epoch:\n                print \"\"\n                print \"Early stopping.\"\n                print self.best_valid_epoch,self.best_valid\n                print \"Best valid score {:.6f} at epoch {}.\".format(self.best_valid, self.best_valid_epoch)\n                \n                for qq in range (len(self.params)):\n                    self.params[qq].set_value( self.best_params[qq] )\n                break\n\n\n    def prediction(self, x):\n        return self.predict(x)\n\n    def outputCSV(self, wfilename, test_data, mapd): # read dictionary for id \n        test_results = []\n        \n        for xid, xdata in test_data.iteritems():\n            test_results.append( (xid, self.predict(xdata.T)) ) \n            \n        f = open(wfilename, 'w+')\n        f.write(\"Id,Prediction\\n\")\n        for xid, y in test_results:\n            f.write(\"{0},{1}\".format(xid, mapd[y[0]]))\n        f.close()\n        print \"MISSION COMPLETE\"\n    \n    ", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "# Start training!", "cell_type": "markdown", "metadata": {}}, {"source": " ## Data reading", "cell_type": "markdown", "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "### Data Processing \n\ndata_ratio = 1 # the input we use ( to put more efford on improve parameter)\n\n# X_train, X_test, y_train, y_test, test_data, result_mapping = Dataset(\"mfcc\", data_ratio)\n# X_train, X_test, y_train, y_test, test_data, result_mapping = Dataset(\"fbank\", data_ratio)\n# trimfcc filled wit buggy :P\n# X_train, X_test, y_train, y_test, test_data, result_mapping = Dataset(\"trimfcc\", data_ratio)\n# X_train, X_test, y_train, y_test, test_data, result_mapping = Dataset(\"quadmfcc\", data_ratio)\nX_train, X_test, y_train, y_test, test_data, result_mapping = Dataset(\"hexamfcc\", data_ratio)\n# X_train, X_test, y_train, y_test, test_data, result_mapping = Dataset(\"hexafbank\", data_ratio)\n\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Now we process test data\n"}], "metadata": {"scrolled": false, "collapsed": false, "trusted": true}}, {"execution_count": 5, "cell_type": "code", "source": "(np.array(y_train)).shape\n# y_train = np.array(y_train)\n# ((y_train.flatten())).reshape(len(y_train.flatten())/48,48).astype(np.float32)\n# y_train[0].flatten()", "outputs": [{"execution_count": 5, "output_type": "execute_result", "data": {"text/plain": "(1012340, 48)"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 11, "cell_type": "code", "source": "(np.array(X_train)).shape\n\n# print X_train[0]\n# X_train = np.array(X_train)\n# X_train[0].shape\n# (X_train.flatten()).reshape(1012340,39)[0]", "outputs": [{"execution_count": 11, "output_type": "execute_result", "data": {"text/plain": "(1012340, 351)"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 9, "cell_type": "code", "source": "(np.array(test_data)).shape\nprint len(test_data)\n# len(np.array(test_data.values()[0]))\n# np.array(test_data.values()).flatten()\n\n# test_data = np.array(test_data.values())\n# np.array(((test_data.flatten()))).reshape(len(test_data.flatten())/39,39).shape\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "180406\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "## Train model!", "cell_type": "markdown", "metadata": {}}, {"execution_count": 13, "cell_type": "code", "source": "# Test Hexa FBank with no dropout, don't ask what is Hexa it means 4 + 1 + 4\nnet5_hexaMFCC_dropout_2 = DNN(\n    input_shape=(128,351), \n    hidden_layer=[1500,1500,1500,1500 ], # maximum size to 6 layer only\n    drop_input=0.2, # usually is 0.2\n    drop_hidden=[0.5,0.5,0.5,0.5], # maximum size to 6 layer only, usually use 0.5 \n    activation = \"relu\", # relu, sigmoid\n    batch=128, \n    max_epochs=300, \n    eval_size=0.1, \n    output_num_units=48, \n    up_learning_rate = {'start':0.1, 'stop':0.0001}, # make it dynamic increase\n    up_momentum = {'start':0.5, 'stop':0.9, 'epoch':20}, # only 40 epochs, and after 40 epochs, use 0.9\n    patience=20,\n    max_norm = None # need to tune, maybe start from 3 or 4\n)\n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "%time net5_hexaMFCC_dropout_2.fit(X_train, y_train)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 13, "cell_type": "code", "source": "# Test Hexa FBank with no dropout, don't ask what is Hexa it means 4 + 1 + 4\nnet5_hexaMFCC_no_dropout_1 = DNN(\n    input_shape=(128,351), \n    hidden_layer=[1500,1500,1500,1500 ], # maximum size to 6 layer only\n    drop_input=0, # usually is 0.2\n    drop_hidden=[0,0,0,0], # maximum size to 6 layer only, usually use 0.5 \n    activation = \"relu\", # relu, sigmoid\n    batch=128, \n    max_epochs=300, \n    eval_size=0.1, \n    output_num_units=48, \n    up_learning_rate = {'start':0.001, 'stop':0.0001}, # make it dynamic increase\n    up_momentum = {'start':0.9, 'stop':0.9, 'epoch':20}, # only 40 epochs, and after 40 epochs, use 0.9\n    patience=20,\n    max_norm = None # need to tune, maybe start from 3 or 4\n)\n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "%time net5_hexaMFCC_no_dropout_1.fit(X_train, y_train)", "outputs": [{"output_type": "stream", "name": "stdout", "text": " \nstart training!!!!\n \nepoch 1 : err = [array(2.029616119147112)], score = 0.572890530849, time =2445.32905817 s\nepoch 2 : err = [array(1.7391193694849911)], score = 0.627467056523, time =2451.38642287 s\nepoch 3 : err = [array(1.6416497354307606)], score = 0.655678922101, time =2458.45371008 s\nepoch 4 : err = [array(1.5524880763519038)], score = 0.67512890926, time =2454.52987194 s\nepoch 5 : err = [array(1.4878230589142227)], score = 0.690370824031, time =2444.1128099 s\nepoch 6 : err = [array(1.4148098141776795)], score = 0.702106011814, time =2446.80837417 s\nepoch 7 : err = [array(1.3519400345133634)], score = 0.71073947488, time =2438.3882761 s\nepoch 8 : err = [array(1.2945080705872032)], score = 0.718790129798, time =2442.37712598 s\nepoch 9 : err = [array(1.2464762325886007)], score = 0.725487484442, time =2438.101964 s\nepoch 10 : err = [array(1.1962067289179408)], score = 0.731868739751, time =2443.24386907 s\nepoch 11 : err = [array(1.1819344918102077)], score = 0.736679376494, time =2441.33836484 s\nepoch 12 : err = [array(1.1592932717024944)], score = 0.741420866507, time =2444.9400599 s"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 19, "cell_type": "code", "source": "net5_hexaMFCC_no_dropout_1.train_history_", "outputs": [{"execution_count": 19, "output_type": "execute_result", "data": {"text/plain": "[{'epoch': 1, 'err': [array(2.029616119147112)], 'score': 0.5728905308493194},\n {'epoch': 2,\n  'err': [array(1.7391193694849911)],\n  'score': 0.62746705652251222},\n {'epoch': 3,\n  'err': [array(1.6416497354307606)],\n  'score': 0.65567892210127032},\n {'epoch': 4, 'err': [array(1.5524880763519038)], 'score': 0.6751289092597349},\n {'epoch': 5,\n  'err': [array(1.4878230589142227)],\n  'score': 0.69037082403145189},\n {'epoch': 6,\n  'err': [array(1.4148098141776795)],\n  'score': 0.70210601181421262},\n {'epoch': 7,\n  'err': [array(1.3519400345133634)],\n  'score': 0.71073947487998101},\n {'epoch': 8,\n  'err': [array(1.2945080705872032)],\n  'score': 0.71879012979828916},\n {'epoch': 9,\n  'err': [array(1.2464762325886007)],\n  'score': 0.72548748444198585},\n {'epoch': 10,\n  'err': [array(1.1962067289179408)],\n  'score': 0.73186873975146693},\n {'epoch': 11,\n  'err': [array(1.1819344918102077)],\n  'score': 0.73667937649406323},\n {'epoch': 12,\n  'err': [array(1.1592932717024944)],\n  'score': 0.74142086650729988},\n {'epoch': 13,\n  'err': [array(1.1035631877245522)],\n  'score': 0.74802931821324847},\n {'epoch': 14,\n  'err': [array(1.0715147550940485)],\n  'score': 0.75033091649050709},\n {'epoch': 15,\n  'err': [array(0.9570088144975465)],\n  'score': 0.75506252839954957},\n {'epoch': 16,\n  'err': [array(0.9042348851895303)],\n  'score': 0.75654424402868603},\n {'epoch': 17,\n  'err': [array(0.8578695171951888)],\n  'score': 0.76034731414346957},\n {'epoch': 18, 'err': [array(0.822309981773499)], 'score': 0.7686350435624395},\n {'epoch': 19, 'err': [array(nan)], 'score': 0.020615603453385225}]"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 19, "cell_type": "code", "source": "# Test Hexa FBank with no dropout, don't ask what is Hexa it means 4 + 1 + 4\nnet5HexaFBank_dropout_2 = DNN(\n    input_shape=(128,621), \n    hidden_layer=[1500,1500,1500,1500 ], # maximum size to 6 layer only\n    drop_input=0.2, # usually is 0.2\n    drop_hidden=[0.5,0.5,0.5,0.5], # maximum size to 6 layer only, usually use 0.5 \n    activation = \"relu\", # relu, sigmoid\n    batch=128, \n    max_epochs=300, \n    eval_size=0.1, \n    output_num_units=48, \n    up_learning_rate = {'start':0.1, 'stop':0.0001}, # hinton version of constant lr = 0.1, \n    up_momentum = {'start':0.5, 'stop':0.9, 'epoch':20}, # only 40 epochs, and after 40 epochs, use 0.9\n    patience=20,\n    max_norm = 4 # need to tune, maybe start from 3 or 4\n)\n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 20, "cell_type": "code", "source": "%time net5HexaFBank_dropout_2.fit(X_train, y_train)", "outputs": [{"output_type": "stream", "name": "stdout", "text": " \nstart training!!!!\n \nepoch 1 : err = [array(3.3154811269881197)], score = 0.245569670269, time =2582.24993706 s\nepoch 2 : err = [array(3.0614095129058008)], score = 0.316099334216, time =2595.10063887 s\nepoch 3 : err = [array(2.9409431049653723)], score = 0.365371317937, time =2563.0335331 s\nepoch 4 : err = [array(nan)], score = 0.022373906, time =2560.18848205 s\nCPU times: user 3h 41min 31s, sys: 1h 59min 6s, total: 5h 40min 37s\nWall time: 2h 51min 41s\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 11, "cell_type": "code", "source": "# Test Hexa FBank with no dropout, don't ask what is Hexa it means 4 + 1 + 4\nnet5HexaFBank_no_dropout_1 = DNN(\n    input_shape=(128,621), \n    hidden_layer=[2048,2048,2048,2048 ], # maximum size to 6 layer only\n    drop_input=0, # usually is 0.2\n    drop_hidden=[0,0,0,0], # maximum size to 6 layer only, usually use 0.5 \n    activation = \"relu\", # relu, sigmoid\n    batch=128, \n    max_epochs=300, \n    eval_size=0.1, \n    output_num_units=48, \n    up_learning_rate = {'start':0.001, 'stop':0.0001}, \n    up_momentum = {'start':0.9, 'stop':0.9, 'epoch':30}, # only 40 epochs, and after 40 epochs, use 0.9\n    patience=20,\n    max_norm = None # need to tune, maybe start from 3 or 4\n)\n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 12, "cell_type": "code", "source": "%time net5HexaFBank_no_dropout_1.fit(X_train, y_train)", "outputs": [{"output_type": "stream", "name": "stdout", "text": " \nstart training!!!!\n \nepoch 1 : err = [array(2.5324242669967876)], score = 0.485884189106, time =4534.19684601 s\nepoch 2 : err = [array(2.2429373715467134)], score = 0.546031965545, time =4524.72641611 s\n"}, {"ename": "KeyboardInterrupt", "evalue": "", "traceback": ["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)", "\u001b[1;32m<ipython-input-12-7769c9df7eb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time net5HexaFBank_no_dropout_1.fit(X_train, y_train)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[1;32m/home/pika/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[1;34m(self, arg_s)\u001b[0m\n\u001b[0;32m   2302\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2303\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2304\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2306\u001b[0m     \u001b[1;31m#-------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/home/pika/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line)\u001b[0m\n\u001b[0;32m   2223\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2224\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2225\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2226\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/home/pika/anaconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n", "\u001b[1;32m/home/pika/anaconda/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/home/pika/anaconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1160\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'eval'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1161\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1162\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1163\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1164\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n", "\u001b[1;32m<ipython-input-8-3ff9c8f81331>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[1;31m# To do: modify a function of mini batch and using random\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m                 \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/home/pika/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    595\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/home/pika/anaconda/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    766\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 768\u001b[1;33m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    769\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/home/pika/anaconda/lib/python2.7/site-packages/theano/tensor/basic.pyc\u001b[0m in \u001b[0;36mperform\u001b[1;34m(self, node, inp, out)\u001b[0m\n\u001b[0;32m   4700\u001b[0m         \u001b[1;31m# gives a numpy float object but we need to return a 0d\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4701\u001b[0m         \u001b[1;31m# ndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4702\u001b[1;33m         \u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4704\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;31mKeyboardInterrupt\u001b[0m: "], "output_type": "error"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "## Save model!!", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import sys\nsys.setrecursionlimit(10000)\n\n# import cPickle as pickle\n# with open('net5_hexaMFCC_no_dropout_1.pickle', 'wb') as f:\n#     pickle.dump(net5_hexaMFCC_no_dropout_1, f, -1)y_test\n\nimport cPickle as pickle\nwith open('net5_hexaMFCC_no_dropout_1_Amazing.pickle', 'wb') as f:\n    pickle.dump(net5_hexaMFCC_no_dropout_1, f, -1)\n", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"source": "## Load model", "cell_type": "markdown", "metadata": {}}, {"execution_count": 26, "cell_type": "code", "source": "import cPickle as pickle\n\nwith open('net5-HAXAKILL2458748.pickle', 'rb') as f:\n    net5 = pickle.load(f)\n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"source": "## validation result", "cell_type": "markdown", "metadata": {}}, {"execution_count": 18, "cell_type": "code", "source": "y_pred = net5_hexaMFCC_no_dropout_1.prediction(X_test)\n\nyyy = np.array(map((lambda x: np.argmax(x)), y_test))\n\nprint metrics.classification_report((yyy), (y_pred))\nprint accuracy_score(yyy, y_pred)\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "             precision    recall  f1-score   support\n\n          0       0.82      0.62      0.70      2447\n          1       0.71      0.83      0.77      2999\n          2       0.62      0.59      0.60      1622\n          3       0.79      0.72      0.75      2270\n          4       0.79      0.64      0.71      1126\n          5       0.56      0.58      0.57      1794\n          6       0.79      0.78      0.79      2746\n          7       0.78      0.75      0.76      1089\n          8       0.69      0.68      0.68       761\n          9       0.83      0.73      0.78      6607\n         10       0.73      0.61      0.67      1207\n         11       0.71      0.69      0.70      1463\n         12       0.77      0.74      0.75       791\n         13       0.61      0.67      0.64      2590\n         14       0.74      0.61      0.67      1002\n         15       0.61      0.54      0.58       561\n         16       0.58      0.42      0.49       386\n         17       0.83      0.79      0.81      4086\n         18       0.70      0.84      0.76      2698\n         19       0.81      0.89      0.85      2376\n         20       0.75      0.72      0.73       662\n         21       0.80      0.78      0.79      1389\n         22       0.62      0.57      0.60      2911\n         23       0.63      0.52      0.57      3577\n         24       0.79      0.80      0.80      4413\n         25       0.65      0.74      0.69       761\n         26       0.78      0.91      0.84      2506\n         27       0.72      0.84      0.78      3575\n         28       0.79      0.84      0.81      2825\n         29       0.70      0.76      0.73      1033\n         30       0.75      0.79      0.77      4381\n         31       0.73      0.78      0.76      1974\n         32       0.71      0.67      0.69       448\n         33       0.79      0.84      0.81      1822\n         34       0.74      0.81      0.78      2972\n         35       0.83      0.86      0.84      1665\n         36       0.94      0.93      0.94     11300\n         37       0.77      0.91      0.83      6935\n         38       0.76      0.31      0.44       769\n         39       0.78      0.76      0.77      2419\n         40       0.51      0.30      0.38       277\n         41       0.76      0.77      0.76      1827\n         42       0.66      0.77      0.71      3639\n         43       0.79      0.65      0.71      1337\n         44       0.81      0.81      0.81      1844\n         45       0.71      0.71      0.71       835\n         46       0.71      0.54      0.61       143\n         47       0.82      0.60      0.69      3623\n\navg / total       0.77      0.77      0.76    112483\n\n0.765644586293\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "# write predict file", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "net5_hexaMFCC_no_dropout_1.outputCSV(\"net5_hexaMFCC_no_dropout_1.csv\", test_data, result_mapping)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 18, "cell_type": "code", "source": "type(test_data)\nlen(test_data)", "outputs": [{"execution_count": 18, "output_type": "execute_result", "data": {"text/plain": "180406"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.9", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}